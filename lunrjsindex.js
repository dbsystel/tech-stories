var documents = [

{
    "id": 0,
    "uri": "search.html",
    "menu": "-",
    "title": "Search",
    "text": " lokale Suche https://codersblock.com/blog/mini-previews-for-links/ div#searchresults h3 { font-size: larger; margin-top: 0 !important; } div#searchresults h4 { font-size: large; } div#searchresults span { font-size: small; } div#searchresults span.menu { font-size: medium; margin-top: 1em; } function dosearch(element) { if (element.value.length>=3) { var searchresults = document.querySelector('#searchresults'); out = \"\"; var results = idx.search(element.value); if (results.length == 0) { results = idx.search(element.value+\"*\"); } if (results.length == 0) { results = idx.search(\"*\"+element.value+\"*\"); } if (results.length == 0) { results = idx.search(element.value+\"~1\"); } var lastMenu = \"\" var lastTitle = \"\" results.forEach(function (item) { var doc = documents[item.ref]; out += \" \"+doc.menu+\" \"; if (doc.menu != lastMenu) { lastMenu = doc.menu; } out += \" \" + doc.title + \" \"; for(var field in item.matchData.metadata) { console.log(field); var matches = item.matchData.metadata[field] if (matches['text']) { matches['text']['position'].forEach(function (pos) { var subtext = doc.text.substring(pos[0]-50,pos[0]+pos[1]+50); if (pos[0]>0) { subtext = subtext.replace(new RegExp(/^[^ ]*/,\"i\"),\"...\"); } subtext = subtext.replace(new RegExp(/[^ ]*$/,\"i\"),\"...\"); var re = new RegExp(field,\"gi\"); subtext = subtext.replace(re,\" $& \"); out += \" \" + subtext + \" \"; }) } } searchresults.innerHTML = out; }) } } var input = document.querySelector(\"#lunrsrc\"); input.focus(); var params = new URLSearchParams(window.location.search); input.value = params.get('q'); dosearch(input); "
},

{
    "id": 1,
    "uri": "blog/2024/2024-01-16-Accessibility-in-Angular.html",
    "menu": "Blog",
    "title": "Accessibility in Angular",
    "text": " Table of Contents Accessibility in Angular – Angulars features for a better and more inclusive web Slides Demos Accessibility in Angular – Angulars features for a better and more inclusive web The Angular Framework brings us some built-in features to help in creating accessible components and applications by wrapping common best practices and techniques. In this talk at the Angular Berlin Meetup, Danny presented these concepts and features. The Agular Meetup Berlin, hosted at Angular Berlin Meetup takes place regularly at DB Systel in Berlin. On January 16th 2024, Danny delved deeper into the Angular Framework and examining its features that aid in enhancing overall accessibility. Furthermore, he guided attendees through the use of the Angular CDK which provides additional tools helping us to improve the accessibility of our application. Slides Demos Angular a11y Demo Repository with source code related to the slides "
},

{
    "id": 2,
    "uri": "blog/2023/2023-12-21-vollautomatisch-konstruierter-Fahrplan.html",
    "menu": "Blog",
    "title": "Fahrplankonstruktion",
    "text": " Table of Contents Von der Vision eines vollautomatisch konstruierten Fahrplans Slides Von der Vision eines vollautomatisch konstruierten Fahrplans Wie mit Digitalisierung der Prozesse und agiler Software-Entwicklung der Fahrplan unter dem \"rollenden Rad\" sukzessive auf die Digitale Schiene gesetzt und gleichzeitig eine Kapazitätssteigerung erreicht wird. Jede Zugfahrt in Deutschland braucht einen Fahrplan. Das sind bis zu 50.000 Zugfahrten mit Fahrplänen pro Tag. Doch wo kommen diese her? Die steigende Verkehrsnachfrage führt auf der einen Seite zu höheren Kapazitätsbedarfen und Auslastung der Schieneninfrastruktur. Auf der anderen Seite wird die verfügbare Kapazität während der Bau- und Modernisierungsaktivitäten der Schieneninfrastruktur und der Leit- und Sicherungstechnik während der Maßnahmen reduziert. Zur frühzeitigen Identifikation, Vermeidung und Handhabung von möglichen Kapazitätsengpässen sind die IT und die weitere Digitalisierung der Prozesse gefordert. Durch die Einführung digitaler Prozesse entwickeln wir uns weg von der einer manuellen Fahrplankonstruktion hin zu einer weitestgehend vollständig automatisierten Fahrplankonstruktion und Kapazitätsmanagement, so dass die zukünftig steigenden Verkehrsbedarfe auf der Schiene weiterhin bedient werden können. Hierzu sind wir vor 3 Jahren gestartet, die monolithischen Bestandssysteme sukzessive durch modulare Services mit einer flexiblen, integrierten Plattform abzulösen und zu automatisieren. Damit verbunden ist die Neugestaltung und Automatisierung von Abläufen und Prozessschritten, sowie eine Konsolidierung der Datenhaltung und Informationsflüsse. Die agile Software-Entwicklung bietet eine höhere Flexibilität und Reaktionsschnelligkeit auf Kundenwünsche und neue Anforderungen. Wir arbeiten iterativ und sind im ständigen Austausch zu unseren Stakeholdern. Dadurch werden Teillieferungen in regelmäßigen Abständen gewährleistet, um so der großen Vision stückweise näher zu kommen. Die Herausforderung ist, mit über 600 Experten in crossfunktionalen, agilen Teams synchronisiert neue Abläufen, automatisierte Prozessschritte und flexibel kombinierbaren und modularen Services zu konzipieren, entwickeln und unter dem \"rollenden Rad\" sukzessive in Produktion zu bringen. Als DB Netz sind wir in Deutschland eines der größten Unternehmen, die einen signifikanten Anteil der IT-Entwicklung nach SAFe® 6.0 gestalten. Unsere Teams arbeiten mit modernsten Standards in der Cloud, setzen auf eine sehr hohe Automatisierung im Lebenszyklus der Softwareentwicklung und übernehmen eine Ende-zu-Ende Verantwortung für ihre entwickelten Artefakte. Slides download "
},

{
    "id": 3,
    "uri": "blog/2023/2023-12-21-postgreSQL.html",
    "menu": "Blog",
    "title": "PostgreSQL",
    "text": " Table of Contents Ein Jahr PostgreSQL – Das Leben danach Slides Ein Jahr PostgreSQL – Das Leben danach Dieser Vortrag ist ein Jahr nach dem Entschluss entstanden, einen neuen Job anzunehmen und mich raus aus meiner Oracle-Blase und rein in die \"neue\" Welt der PostgreSQL-Datenbankentwicklung zu begeben. Der Gedanke damals war: \"SQL ist SQL und ich werde mich schon schnell zurechtfinden. Es bleibt ja eine Relationale Datenbank\". Dieser Vortrag soll eine kleine Zusammenfassung sein über: • Einige WTF-Momente: Der Teufel liegt im Detail und welche Unterschiede da wirklich auf einen Oracle-Datenbankentwickler warten, war mir vorher nicht bewusst. • Das schöne aus beiden Welten: Sowohl Oracle als auch PostgreSQL haben schöne Seiten. Es ist von Vorteil, beide Systeme ein bisschen genauer zu kennen. • Sonstige Erfahrungen und Eindrücke, die ich nach einem Jahr im PostgreSQL-Ökosystem so beobachtet habe. Ich konzentriere mich dabei auf den Bereich SQL (z. B. analytic functions oder die Behandlung von NULL values) und habe aber auch ein paar \"admin-nahe\" bzw. theoretische Unterschiede, die ich aufzeigen möchte (z. B. Tuning-Werkzeuge und MVCC). Nach dem Vortrag sollte klarer sein, mit welchen Fallstricken man beim Umstieg von Oracle auf PostgreSQL rechnen sollte. Slides download "
},

{
    "id": 4,
    "uri": "blog/2023/2023-11-29-AI-in-Software-Design.html",
    "menu": "Blog",
    "title": "AI in Software Design",
    "text": " Table of Contents Using AI in Software Design: How ChatGPT Can Help With Creating a Solution Architecture References Slides Using AI in Software Design: How ChatGPT Can Help With Creating a Solution Architecture Artificial intelligence, or AI, is becoming a big part of our lives. One type of AI is Large Language Models (LLMs), like chatGPT. At the iSAQB Software Architecture Gathering , I showed how chatGPT can help with complex tasks in software design, such as the tasks in the iSAQB Advanced Exam. We delved into the intricacies of conversing with chatGPT, illustrating the strategies needed to generate productive prompts and effectively utilize the model as a sparring partner. The talk wexemplified these concepts by taking the audience on a step-by-step journey through the process of tackling the iSAQB Advanced Example Exam, utilizing chatGPT. We demonstrated how to prepare for a chat session with chatGPT, how to generate robust prompts, and how to manage the chat for optimal outcomes. We aimed to highlight the model’s capabilities as an interactive tool that can provide valuable insights and streamline the process of developing software architecture. Whether you are a seasoned architect or a novice in the field, this presentation will equip you with novel techniques to navigate the challenging landscape of software architecture with the assistance of AI. Discuss this topic with us on Github! References Full HTML Chat Protocol and generated Solution as PDF ChatGPT Prompt Engineering for Developers - DeepLearning.AI Softwarearchitektur: \"KI wird unsere Fähigkeiten ergänzen&#44; nicht ersetzen\" KI in der Softwareentwicklung: Überschätzt | heise online Stefan Toth: Architektur in Zeiten von KI und LLMs Slides "
},

{
    "id": 5,
    "uri": "blog/2023/2023-11-20-einfuehrung-barrierefreiheit-web.html",
    "menu": "Blog",
    "title": "A11y: EAA, BFSG, WCAG, WAI, ARIA, WTF? – it's for the people stupid!",
    "text": " Table of Contents A11y: EAA, BFSG, WCAG, WAI, ARIA, WTF? – it&#8217;s for the people stupid! Slides Referenzen A11y: EAA, BFSG, WCAG, WAI, ARIA, WTF? – it&#8217;s for the people stupid! Barrierefreiheit ist ein verstaubtes und unwichtiges Thema für Behördensoftware? Ganz im Gegenteil: Accessibility betrifft uns täglich und immer, wenn wir Software verwenden. Es ist an uns, diese umzusetzen. In unserem Talk von der W-JAX am 07.11.2023 zeigen wir euch, wie ihr eure Webanwendungen von Beginn an mit einfachen Mitteln zu einem hohen Grad barrierefrei gestaltet und entwickelt. Wir stellen euch effektive Methoden und Tools vor, die euch bei der Umsetzung und Prüfung in hohem Maße unterstützen. Wir gehen darauf ein, welchen Einfluss Barrierefreiheit auf eure Frontend-Architekturen hat und welche Aspekte hier zu beachten sind. Dabei beten wir nicht die einzelnen WCAG-Kriterien herunter – vielmehr geben wir euch eine praktische Einführung in das Thema Barrierefreiheit im Web und welche Kniffe und Fallstricke zu beachten sind. Slides Referenzen Bild von Bill Clinton W3C Logo Grafik \"human\" "
},

{
    "id": 6,
    "uri": "blog/2023/2023-11-20-conways-law.html",
    "menu": "Blog",
    "title": "Conway’s Law",
    "text": " Table of Contents Conway’s Law in real life - why organizational development and software engineering have to go hand in hand. Slides Conway’s Law in real life - why organizational development and software engineering have to go hand in hand. Conway’s Law inevitably connects the architecture of an software system with the responsible delivery organization. Unfortunately, persons responsible for organizational development and architects of software systems often work in separation – especially in large enterprises. What are the negative effects of this separation and what can we do about it? In this talk Dr. Martin Strunk presents “real world examples” of the impact that arising from the ignorance of Conway`s law by decision makers. He will also propose ways, in which companies might overcome and mitigate these problems. Teams as first-class citizens Design comes first, then comes the participation Don’t rely on emergence to solve structural problems Slides "
},

{
    "id": 7,
    "uri": "blog/2023/2023-11-08-prompt-engineering.html",
    "menu": "Blog",
    "title": "Prompt-Engineering",
    "text": " Table of Contents Die faszinierende Welt des Prompt-Engineerings Slides Referenzen Die faszinierende Welt des Prompt-Engineerings Im Zentrum meines jüngsten Vortrags stand das Prompt-Engineering, ein Schlüsselelement im Umgang mit fortschrittlichen Sprachmodellen wie GPT-3 und GPT-4. Das Prompt-Engineering ermöglicht es Entwicklern, genaue und nuancierte Antworten von KI-Modellen zu erhalten und das Potenzial der KI voll auszuschöpfen. Wir tauchten tief in die Unterschiede zwischen GPT-3 und GPT-4 ein, erkundeten die Komplexität neuronaler Netze und die Bedeutung der Multi-Modalität in der heutigen KI-Landschaft. Ein besonderes Augenmerk legten wir auf die Kunst des Primings und der Kontextualisierung von Prompts, die die Effektivität und Präzision der KI-Interaktionen erheblich steigern können. Neben theoretischen Überlegungen werden reale Anwendungsfälle diskutiert, die das Potenzial von Prompt-Engineering veranschaulichen. Slides Referenzen Golem.de: BSI sieht KI als Sicherhitsrisiko Twitter: The Hired AI Medium: ChatGPT Architecture Explained by Sreedev R Scientific Reports: Comparative performance of humans versus GPT-4.0 and GPT-3.5 GPT-4: Wikipedia FAZ.net: ChatGPT: Welche Einstellungen bei der Nutzung helfen OpenAI Chat: Beispiel OpenAI: Verbesserung der docToolchain Dokumentation DeepLearning.AI: ChatGPT Prompt Engineering for Developers Heise Online: Softwarearchitektur: \"KI wird unsere Fähigkeiten ergänzen, nicht ersetzen\" Heise Online: KI in der Softwareentwicklung: Überschätzt OpenAI Chat: Beispiel 2 Stefan Toth: Architektur in Zeiten von KI und LLMs "
},

{
    "id": 8,
    "uri": "blog/2023/2023-08-21-vue2-vue3-migration.html",
    "menu": "Blog",
    "title": "Migrate Vue 2 to Vue 3",
    "text": " Table of Contents How we migrated our Vue 2 enterprise project to Vue 3 About me and my Team Where we started Preparations Migrations before the migration Finally: Migrate to Vue 3 Post-Migration-Steps Conclusion How we migrated our Vue 2 enterprise project to Vue 3 It&#8217;s been a while: Since the 2nd February 2022, Vue 3 became the new default for Vue.js apps. It&#8217;s done! Vue 3 is now the default version and the brand new http://vuejs.org is live! More details in the blog post in case you missed it: https://blog.vuejs.org/posts/vue-3-as-the-new-default.html Tweet from @vuejs on Twitter It was a long journey to the final default release of Vue 3 since the first version published on 18th September 2020. But: even if Vue 3 isn&#8217;t a new thing anymore, there are still a lot of Vue 2 apps which haven&#8217;t been migrated yet. The migration can be quite heavy since in practice it&#8217;s much more than only following the migration guide. Projects usually rely also on 3rd-party dependencies which are maybe not available for Vue 3 or not maintained anymore. In this blog post I will give you an insight into how my team mastered the migration and what pitfalls we faced. I will describe how we planned and migrated our whole Vue 2 codebase to Vue 3 using Pinia as Store-Solution, Vite for our build environment and Vitest for fast unit test executions. The focus of this article is not to provide a very detailed step-by-step migration guide. I will focus about what things you should keep in mind, what you can already do before starting the migration and about some pitfalls we pointed out. However, I will provide you links to more detailed blog posts about specific topics. Please keep in mind, that the way we solved the migration won&#8217;t probably fit to your very specific setup for 100%, but you can check what parts seem to be good for you and your team. Vue 2 EOL: Please note, that the Vue 2 support will end on December 31st, 2023. This means there will be no fixes and features provided anymore (unless you are actively extending the support ) About me and my Team To give you a high level overview about our context, I would just like to say a few short words about myself and my team. I am working at DB Systel GmbH , in a DevOps Team building a Business-to-Government (B2G) solution together with our partner Deutsche Bahn Connect GmbH named DB Curbside Management . Our product focuses on helping cities and councils to effectively manage shared mobility offerings and their jurisdictions dynamically. They will be able to get insights about statistics, violations of agreements with the mobility providers to regulate a fair and steady distribution of all the different shared mobility vehicles across managed area. Where we started My team added the migration to Vue 3 with the new default setup and tools using Vite , Pinia and Vitest to our backlog many months ago, but the switch to Vue 3 as default gave us another push for facing the migration. We realized pretty fast, that a big-bang migration wouldn&#8217;t be possible for us, since it will block us releasing new features for quite a long time. Our codebase contained already ~200 Vue 2 components using the old-fashioned Options API as well as a huge Vuex Store and some libraries that aren&#8217;t compatible with Vue 3. Preparations Let&#8217;s start with the preparation of your team and questions you should answer yourself before starting the migration. The first thing you should do is to get comfy with Vue 3, and you should learn about the differences compared to Vue 2 and the options you have. You can set up a Vue 3 playground app locally and explore yourself the new setup and components. To know what things will change when starting the migration, I would recommend you the read the following articles in advance: Official Vue 2 to Vue 3 Migration Guide Blogpost: \"Vue.js: How to Migrate a large project from Vue 2 to Vue 3\" from Baptiste Jamin Official Vue 3 notes about the Composition API and the relationship / differences compared to the Options API Vue Master Blog-Series by Andy Li Part 1 | Part 2 Free Vue Mastery Video Course \"From Vue 2 to Vue 3\" Update to the latest minor Vue 2 version My first advice is to keep your current app as up-to-date as possible. Especially when your Vue version is below 2.7.x , I would recommend you to update it. With Vue 2.7.x \"Naruto\" release, the Vue team aimed to backport lots of features from Vue 3 to Vue 2 without introducing a breaking change. This will help you to migrate some things in preparation for a smooth Vue 3 switch. Check out the official announcement and start migrating to the Vue 3 flavour in your Vue 2 app: TypeScript or not? Are you using TypeScript right now or do you plan to migrate to TypeScript? In that case you should read the TypeScript Notes for Vue 3 . Generally I would highly recommend to use TypeScript as the Vue 2 and Vue 3 TypeScript integration is great. It will help you a lot to reduce runtime errors as hard debugging nights by analyzing bugs in production. Be prepared, that switching to Typescript might require quite an effort, but it&#8217;s still worth it. Check your dependencies A big thing you definitely have to check before is: dependencies. You should check if you are using packages that will rely on Vue 2 and won&#8217;t be available for Vue 3. Such dependencies will require your attention as they may block you from updating to Vue 3. In my previous project we weren&#8217;t able to update to Vue 3 a long time since we had a dependency to BootstrapVue which wasn&#8217;t working with Vue 3 and isn&#8217;t still. In such case where a package isn&#8217;t compatible you have the following options: Check if there is an equivalent package or a fork of your dependency that will support Vue 3. If there is one: be sure it&#8217;s still maintained and alive. If the package is just a Vue-wrapper for a common library, you may need to use the library directly Find a similar package that supports Vue 3. In this case you have to make sure the new dependency supports all your use-cases, and you have to plan how to migrate this dependency. You can contribute to the dependency and help to make it Vue 3 compatible. You may have to check VueDemi which is a great developing utility to create or update Universal Vue Libraries for Vue 2 &amp; 3. Worst Case: Write the features you need by yourself, but be sure to open-source it afterwards ;-) In all these cases you should make yourself a list of the relevant development tasks with a very rough estimate about how complex the migration will be. For example write a + if the dependency migration is straight forward (already Vue 3 compatible). Write + for very hard-to-migrate dependencies, where you may need another solution or lib or implement stuff by yourself. Add Notes about things you shouldn&#8217;t forget when starting the migration. You should also include development dependencies for example for webpack plugins. It could look like the following example: Vue 2 dependency Vue 3 dependency notes estimate @dsb-norge/vue-keycloak-js @baloise/vue-keycloak similar API, similar features ++ v-tooltip floating-vue same lib under the hood with more features + vue2-datepicker vue-datepicker-next same lib with Vue 3 support + vue2-leaflet @vue-leaflet/vue-leaflet same API, but lots of relying components ++ vue2-leaflet-draw-toolbar - no Vue 3 equivalent + webpack-license-plugin rollup-plugin-license different plugin for rollup, with a different API, we need to check / adjust the output format + Try out things in a playground For dependency update / migrations you can&#8217;t estimate, it&#8217;s a good idea to set them up / try them out in an isolated new Vue 3 playground environment. After playing around, you should be able to estimate the effort. A good example when having a look at the migration list above would be to try out the rollup-plugin-license package. Check your current Webpack environment When coming from webpack and planning to migrate to Vite, you should check your webpack config for any special behaviors. You can use the playground to reflect / try out the setup in Vite. Here are some points that were interesting for us: We don&#8217;t need a specific SCSS/SASS/LESS configuration anymore as Vite brings support for this out-of-the-box We needed to migrate the webpack-license-plugin to rollup-plugin-license (see above) Vite comes with its own approach of reading and passing environment variables and build modes which is quite easy and handy Static Asset Handling by Vite is something you should probably know before Split your Store on paper When currently using Vuex, you may be lucky, and you have already some modules splitting your store into logical parts. In our case we had just one big store without any modules as the codebase has evolved over time, and we haven&#8217;t made the step to split the store. The migration to Pinia can be a good chance to face this now as Pinia lets you easily compose multiple small stores. You should check your current store configuration and write down the modules that are loosely coupled or even completely independent (e.g. a user or an auth store). Make the migration transparent and estimable The last thing we have done was to create a new epic for the whole migration and to create small estimable tasks. This was very important as we were now able to identify things we can prepare and do even before we started the migration itself and also tasks we can do in advance. On the other hand it helped us for the communication with the product owner and to make things transparent. Please keep in mind to add some time buffer for unexpected things occurring during the migration where you may need some extra time. For example: the migration from Vuex to Pinia took a lot more time than we thought before. But: it was definitely worth it. The TypeScript support is way better and the unification of actions and mutations reduces the Boilerplate code a lot. We also underestimated the time we needed to migrate the tests. This was hard by definition but quite time-consuming as I wrote in the introduction: We had a huge Vuex store. Migrations before the migration Before starting the migration itself you should migrate everything you can, which is not related to Vue 3 / vite. Here is what we have done in my team before the migration itself. Convert Filters to functions Vue 3 kicked out the concept of using filters in the template using the pipe ( | ) syntax ( {{ expression | myFilter }} ). Filters are simply functions that can be imported and used directly. You can already import the functions, use them as a method and then pass through the expression in the template before starting the Vu3 migration: {{ myFilter(expression) }} . Update and migrate dependencies Update all possible dependencies to their latest versions to make migrations for other libs in advance. At this step: double-check if vue-specific libs are ready for using with Vue 3 or if there are other libs you have to use. If you have to change to other libs and this one supports Vue 3, make the migration now. In our team we had already lots of our dependencies updated, since we are using Mend (formerly Whitesource) Renovate for housekeeping and continuous dependency version updates. When you decide to migrate a dependency to a new one that supports Vue 2 and Vue 3 or which should be replaced with a self-implementation: Do it in advance before the actual Vue migration. Isolate hard-to-migrate components It may happens, you realize, for some of your dependencies a migration won&#8217;t be straight-forward. In our case we decided some years ago, we want to use Leaflet.js as our map library to display and interact with features on a map. Therefore we also used a wrapper for Vue 2 applications called Vue2Leaflet which made us use Leaflet in a declarative manner. However, this architectural decision was now a problem for us, as not only this dependency is not supposed to use it with Vue 3 but also extensions for this library such as Leaflet.heat needed to be migrated. To face this issue we&#8217;ve gone one step back and rethink our architectural decision to use Leaflet. At this time there was already a Vue 3 wrapper for leaflet available but not as feature-rich as we needed it. So we created a new Architectural Decision Record (ADR) to evaluate and choose our future map library as it is a central component of our app and can&#8217;t be easily replaced. After doing a Proof-of-Concept (PoC), we decided to switch to OpenLayers and make use of the vue3-openlayers wrapper too, where we were also able to contribute missing features back into the project. This whole story is probably quite special to my team and our app, but the essential thing here was, that we prepared the central components in parallel to our productive app in a separate repository in isolation. Therefore, we created the components and defined their props and events with the help of Storybook . Of course, we also created tests for these components, so that we were prepared to copy over all this into the productive app and replace the existing components later, when we were ready to actually migrate to Vue 3. A drawback with this approach is of course: It probably blocks you with releasing new features or you have to implement them twice during the preparation time (one time for the productive app based on Vue 2, one time for the isolated components based on Vue 3). Update your NPM Scripts When checking your Vue 3 default setup you will notice that some NPM script names have changed by default. For example the default command to run the development build and server is now npm run dev instead of npm run serve . You can either change the names back since you are used to the \"old\" commands, or you can already name your commands in the Vue 2 setup to the new ones to get comfy with it. Please note that you may have to change the commands in you CI/CD Pipeline too. Switch to Vite You can switch to Vite before updating to Vue 3 this makes the \"big bang\" migration a bit smaller. For that, you should install Vite and use the official plugin @vitejs/plugin-vue2 . You also need to migrate all the webpack plugins and configs. When the setup is finished, cleanup all the webpack stuff including the config and the dependencies. During the migration we noticed, that we haven&#8217;t used Type-Only Imports in all our typescript and .vue files. The default Vite setup is configured in such way, Type-Only Imports will be forced when needed, otherwise you&#8217;ll receive errors during the build. We had the option to either deactivate this strict behavior by setting the typescript config option importsNotUsedAsValues to either preserve or remove (not recommended) or to migrate. Luckily, there is a community project called ts-import-types-cli that will automate a part of this step. So we just had to run the following command to migrate to Type-Only Imports at places needed: # remove the `--dry-run` flag to migrate actually and not only list the changes npx ts-import-types-cli --no-organise-imports -p tsconfig.json --dry-run The bad news: The tool didn&#8217;t find all occurrences of the Type-Only Imports, so when running npm run build , we caught some more we had to fix manually. Switch to Vitest After your migration to Vite, you should make use of Vitest as your new pretty and fast unit testing framework. In comparison to Jest it comes with a stable out-of-the-box ESM support and faster test executions. Until now Jest&#8217;s support for ESM is still experimental (State: Jest Version 29.5). The API is quite similar and mostly compatible to jest . If you used Mocha before, the migration shouldn&#8217;t be hard either. Switch to Pinia The next big step you should do in advance is the migration of your Vuex store. You can also do this step after the migration itself and keep Vuex for now. However, we decided, it&#8217;s a good idea, to migrate the store before and switch to Pinia since the API is a lot simpler and better composable when slicing our big store into chunks. Furthermore, it comes with better TypeScript support. At the Pinia-Docs you will find a very detailed Guide for the Migration from Vuex Migrate Components Last but not least we decided to migrate all our components to the composition API with the &lt;script setup&gt; syntactical sugar . This is a step you can also omit or do in advance, but we recommend using this API since it&#8217;s also a bit more performant, and it reduces the boilerplate code you have to write. Finally: Migrate to Vue 3 You are now prepared to migrate to Vue 3, and you&#8217;ve done already a lot of things which made this step much easier and shorter. Now you can start the migration of Vue itself. Keep in mind, that for the actual migration you must migrate the unit tests too as the test utils for vue3 are slightly different. Migrate the source code Here we started by adding Vue 3 as well as the @vue/compat package as described in the Vue 3 Migration Build documentation . Also, we needed to update the VueRouter to version 4.x.x and adjust the configuration. As good step-by-step guides, I would recommend you again to read the following Blogposts: \"Vue.js: How to Migrate a large project from Vue 2 to Vue 3\" from Baptiste Jamin The official Vue 2 to Vue 3 Migration Guide . If you have already prepared some components in isolation to work with Vue 3 as we did: Of course you should replace the old ones and probably adjust the props or events if the API of your new components changed compared to the Vue 2 ones. After this step your whole app should work as before (fingers crossed). The migration of the components itself can be done one by one after the migration until everything is converted to Vue 3. Migrate to @vue/test-utils@v2 After you migrated everything, you need to update to @vue/test-utils@v2 . The migration should be straight-forward when following the migration guide . Nonetheless it can take quite a bit of time depending on the amount of unit tests you have. Post-Migration-Steps Remove Compatibility Package Once every component is migrated, make sure to remove the @vue/compat and it&#8217;s configuration as you don&#8217;t need it anymore. Make use of the Teleport feature Now that we are using Vue 3, we can use the \"Teleport\" feature. Think about components creating their DOM elements deeply in the DOM caused by the component hierarchy but where you would expect the elements to appear somewhere else close to the root. A good example is displaying a modal conditionally: &lt;body&gt; &lt;ComponentOne&gt; &lt;ComponentTwo&gt; &lt;ComponentThree&gt; &lt;MyModal v-if=\"myCondition\"&gt; &lt;/ComponentThree&gt; &lt;/ComponentTwo&gt; &lt;/ComponentOne&gt; &lt;/body&gt; In Vue 2, the modal would be rendered and appear inside the ComponentThree . Using teleport in MyModal can lift the element up to the body tag which makes more sense for common modal dialogs. Conclusion Migrating from Vue 2 to Vue 3 can be a huge thing and takes quite a bit of time. But good preparation and pre-migration will make the whole migration process much easier, more estimable and won&#8217;t block you for so long with releasing new features. Compared to writing the whole thing from scratch, we think this was well worth it. I hope this post gave you some inspiration of how you can face the migration of your project. Happy Migration ✌🏼 "
},

{
    "id": 9,
    "uri": "blog/2023/2023-05-15-developer-experience-platform-fuer-entwicklerinnen.html",
    "menu": "Blog",
    "title": "Developer Experience Platform",
    "text": " Table of Contents Developer Experience Platform (DXP) für Entwickler:innen Video Developer Experience Platform (DXP) für Entwickler:innen Die Developer Experience Platform (DXP) bietet Services für jede Phase der Software-Entwicklung. Konzern- und Betreibervorgaben sowie Cloud-Richtlinien sind in diesen Services bereits inkludiert. DXP ermöglicht damit DB-konzernweit, dass Entwickler:innen schnell, einfach und sicher Deployments vornehmen können und Software-Anwendungen von der Konzeption bis zum produktiven Einsatz schneller entwickelt werden. Das Erklärvideo zeigt dies, indem es Zuschauende in den Alltag von Entwickler:innen „entführt“. Video "
},

{
    "id": 10,
    "uri": "blog/2023/2023-05-05-loom-threading.html",
    "menu": "Blog",
    "title": "Projekt Loom ist da",
    "text": " Table of Contents Threading wie es sein soll: Projekt Loom ist da Virtualisierung hilft schon immer … und der Weg ins Schlamassel Threads sind die Grundlage der Nebenläufigkeit Asynchrone Programmierung als Notlösung Projekt Loom als Rettung VirtualThreads: benutzen ist (fast) einfacher als vermeiden Anpassungen im eigenen Code Angewohnheiten hinterfragen Synchron war nie schlecht Ausblick: Structured Concurrency Threading wie es sein soll: Projekt Loom ist da Es ist endlich so weit - das lang ersehnte Projekt Loom hat seinen Weg in das JDK gefunden! Seit über fünf Jahren haben wir uns danach gesehnt, all die Krücken wie NIO , asynchrone Programmierung , CompletableFutures und AsyncServlets hinter uns zu lassen und Java wieder so zu schreiben, wie wir es schon immer wollten. Virtualisierung hilft schon immer Auf jedem Rechner gibt es Ressourcen, die begrenzt sind. CPU-Zeit ist seit jeher eine knappe Ressource. Gleichzeitig müssen jedoch häufig viele kleine Aufgaben erledigt werden. Heutzutage verwenden wir meist API-Backends, die Anfragen über HTTP erhalten. Sie lesen Daten, transformieren sie und verändern sie gegebenenfalls. Anschließend wird die Antwort per Netzwerk-IO gesendet. Dabei die Ressourcen effizient zu nutzen, war von Anfang an eine Herausforderung und erforderte viel manuelle Arbeit. Zum Glück hatte Edsger W. Dijkstra bereits im Jahr 1965 die brillante Idee, den Zugriff auf wertvolle Ressourcen zu virtualisieren. So bekam das Berkeley Timesharing System die ersten Threads der Computer-Geschichte. Das Konzept war einfach: Threads sind kostengünstig und virtualisieren den Zugriff auf wertvolle Ressourcen. Figure 1. Threading wie die Urahnen - mit einer CPU Ein Scheduler sorgt dafür, dass blockierte Threads unterbrochen werden und andere Aufgaben ausgeführt werden können, bis die notwendigen Ressourcen verfügbar sind. Ein wahrhaft revolutionäres Konzept! Die Welt hat sich seit den ersten Threads des Berkeley Timesharing Systems weiterentwickelt. „Moderne“ Betriebssysteme wie AmigaOS haben das Konzept des Threading verbessert, indem sie es dem Betriebssystem erlauben, rechnende Prozesse zu unterbrechen und an anderer Stelle fortfahren zu lassen. Anders als bei User Threads in SunOS , wo der Code im Thread selbst anzeigt, wann er unterbrochen werden soll. … und der Weg ins Schlamassel Wir haben seitdem viel getan, um das Thread-Konzept kaputt zu bekommen. Wir nutzen gerade Netz-IO in modernen Anwendungen ganz intensiv. IO ist oft das, was diese Anwendungen am meisten machen. Und auf der anderen Seite ist die Hardware viel schneller als `65 . Wir haben so viele Requests zu verarbeiten und die Rechner sind schnell genug. Das geht. Wir können mal eben eine Million Sockets offenhalten und damit arbeiten. Nur: das Threading selbst kommt nur mit ein paar zehntausend Threads klar. Und deswegen sind inzwischen die Threads selbst die wertvolle Ressource. Und deswegen mussten wir anfangen, die Threads selbst zu teilen, zu poolen und sie wiederzuverwenden. Hierhin fällt der Aufstieg der Event-basierten IO-Bibliotheken . Netty fällt in diese Kategorie. Figure 2. IO-Thread und Worker-Thread bei der Arbeit IO und Worker Threads: ein speziell für IO-Operationen abgestellter Thread nimmt Daten entgegen. Dieser Thread wickelt sämtliche IO-Operationen ab. Damit entfällt auch die Notwendigkeit für Locking und Synchronisierung. Sobald Daten eingetroffen sind, werden sie in separaten Worker-Threads verarbeitet. Worker-Threads sollen selbst nie blockieren. Es wird dabei meistens nur ein Thread (manchmal einer pro CPU) mit IO beauftragt. Er arbeitet mit „non-blocking IO“ , erhält also Events, sobald eine IO-Operation abgeschlossen ist. Dadurch kann ein Thread alle offenen Sockets auf einmal bearbeiten. Sobald das IO abgeschlossen ist, wandert die Arbeit zu einem Worker-Thread weiter, der Berechnungen vornimmt. So lässt sich in unserem Beispiel bei drei gleichzeitig aktiven Requests die Thread-Zahl auf zwei reduzieren. Der Preis dafür ist, dass die Worker-Threads selbst Bescheid geben müssen, wenn sie fertig sind. Da ist dann das „alte“ kooperative Multitasking wieder. In der Praxis spielt das aber weniger eine Rolle, weil wir mehrere Worker-Threads benutzen, als Thread-Pool. Trotzdem – wir bezahlen gleich mehrere Preise dafür: Für jeden Request gibt es mindestens zwei Thread-Wechsel. Und die sind teuer. Sind Teile der Anwendung rechenintensiv, dann müssen wir selbst dafür sorgen, dass sie niemanden blockieren. Dann gibt es mehrere Thread-Pools. &#8230;&#8203; und wir brauchen ein kluges Threading-Konzept. Meistens heißt das, verschiedene Pools für Rechenlast, Netzwerk und File-IO einzuführen. Die IO-APIs sind alles andere als einfach zu bedienen. Und immer etwas anders. Netty für Netzwerk-IO. NIO für File-IO. RDBC für den Datenbankzugriff. Threads sind die Grundlage der Nebenläufigkeit Die Kernkonzepte von Java basieren auf Threads. Das gilt für den Sprachkern, die VM, fürs Debugging und das Profiling. IO-APIs waren synchron und sind in synchroner Form heute noch am übersichtlichsten zu benutzen. Das gesamte Exception-System ergibt nur innerhalb eines Threads wirklich Sinn. Speicherzugriffe innerhalb eines Threads sind geordnet und überschaubar. Wir könnten am übersichtlichsten alle Arbeit für einen Request in einem eigenen Thread erledigen. Wir könnten einfach einen Thread pro Request starten , synchrone APIs verwenden. Aber es geht nicht, weil einfach zu wenige Threads verfügbar sind. Asynchrone Programmierung als Notlösung Als Konsequenz opfern wir den Java-Sprachkern und verwenden reaktive Bibliotheken. Und müssen uns für Konstrukte wie Schleifen, If und Try-Catch komplett neue Konstrukte einfallen lassen. CompletableFuture .supplyAsync(info::getUrl, pool) .thenCompose(url -&gt; getBodyAsync( pool, HttpResponse.BodySubscribers.ofString(UTF_8))) .thenApply(info::findImage) .thenCompose(url -&gt; getBodyAsync( pool, HttpResponse.BodySubscribers.ofByteArray())) .thenApply(info::setImageData) .thenAccept(this::process) .exceptionally(t -&gt; { t.printStackTrace(); return null; }); Ohne auf den konkreten Inhalt dieses Handlers einzugehen, lässt sich die Auswirkung auf die Struktur der Programmiersprache erkennen: Das Programm wird nicht mehr in der üblichen Weise strukturiert, sondern über eine \"Fluent API\" erstellt und gestartet. Im Kern stellt das eine Monade dar, wie sie zum Beispiel aus Haskell bekannt ist. Dieses neue Sprachkonstrukt hat eine Reihe von Folgen, die interessant zu nutzen sind. Mit all den Problemen, die daraus resultieren, dass jetzt JVM, Werkzeuge, Sprache und Tools nicht mehr so recht zusammenpassen wollen: In Stack Traces steht oft kaum noch Hilfreiches . Mit dem Debugger durch ein reaktives Programm zu steppen ist eine Herausforderung. Und die Ursache für Lastprobleme zu finden, ist problematisch. Diesen Programmierstil verwenden wir definitiv nicht, weil er einfacher zu verstehen wäre. Oder weil er sonst irgendwie nützlicher zu handhaben wäre. Wir verwenden diesen Programmierstil, weil wir nicht anders skalieren können. Projekt Loom als Rettung Die Idee hinter Projekt Loom: Threads müssen wieder so billig werden wie damals. Es darf kein Problem sein, Millionen davon zu starten. Die JVM mappt dazu ihre eigene Art von Threads, die dort VirtualThreads heißen, auf Betriebssystem-Threads. Das ist ein M:N-Mapping. Also anders als damals zu Solaris-Zeiten, als „Green Threads“ eben nur auf einen einzigen OS-Thread abgebildet werden konnten. Aber ziemlich so, wie es in Erlang schon immer war. Und auch die Go-Fans lachten ja bereits über uns Java-Menschen. Die JVM kann das deswegen besser als das Betriebssystem, weil es zum einen mehr Wissen besitzt (zum Beispiel über Stack-Größen und das Speichermodell) und zum anderen, weil es Threads nicht jederzeit unterbrechen kann. Stattdessen wird nur dort unterbrochen, wo es blockierende Operationen gibt. Das sind hauptsächlich IO-Operationen, aber auch dort, wo wir in unseren Programmen manuell synchronisieren. Damit das funktioniert, gab es im Rahmen des Projekts Loom Anpassungen quer durch die JVM und die Basis-Bibliotheken. NIO wurde umgebaut. Das „alte“ IO wurde angepasst (und darf und soll damit ruhig wieder benutzt werden). Nur File-IO unter Windows ist noch ein Problem und dauert noch. VirtualThreads: benutzen ist (fast) einfacher als vermeiden Seit Java 19 können wir Threads sehr einfach als „virtual“ starten: var thread = Thread.startVirtualThread(() -&gt; { ... }); Das ist schon alles. Die JVM kümmert sich darum, dass diese VirtualThreads automatisch auf OS-Threads abgebildet werden. Normalerweise auf einen pro CPU-Kern. In diesem VirtualThread lassen sich nach Herzens Lust blockierende Aufrufe, Locks und Sleeps in synchroner Art platzieren. Wir sollen uns keine Gedanken mehr darüber machen, wie der Wettstreit um die Ressourcen läuft. Anpassungen im eigenen Code Einige Code-Konstrukte spielen nicht so gut mit VirtualThreads zusammen. Wir können sie ersetzen, damit der Code noch besser skaliert. Ganz weit vorne ist (jedenfalls derzeit) noch der „synchronized“-Block. Der hängt immer an einem OS-Thread, weil er mit Betriebssystemmitteln implementiert ist. Wir wollen ihn mit „ReentrantLock“ oder noch besser mit „StampedLock“ ersetzen. Der zweite Bereich sind JNI-Aufrufe. Die sind immer dann problematisch, wenn sie innerhalb von „synchronized“ passieren. Vor allem, wenn wir von nativem Code wieder nach Java callen, zum Beispiel bei Callbacks. Alles das muss uns aber nicht aufhalten. In den meisten Fällen machen ein paar wenige solche Stellen wenig aus. Viele Frameworks integrieren VirtualThreads bereits In Spring Boot Projekten werden wir bereits dahin geführt, dass wir Threading an zentraler Stelle implementieren. So wie Spring Boot es intern auch bereits macht. Wir können heute schon dafür sorgen, dass Spring Boot auf VirtualThreads setzt: @Configuration class ConfigureVirtualThreads { @Bean(TaskExecutionAutoConfiguration.APPLICATION_TASK_EXECUTOR_BEAN_NAME) public AsyncTaskExecutor asyncTaskExecutor() { return new TaskExecutorAdapter( Executors.newVirtualThreadPerTaskExecutor()); } @Bean public TomcatProtocolHandlerCustomizer&lt;?&gt; protocolHandlerVirtualThreadExecutorCustomizer() { return protocolHandler -&gt; { protocolHandler.setExecutor( Executors.newVirtualThreadPerTaskExecutor()); }; } } Mit der ersten Deklaration wird Spring konfiguriert. Der neue Task-Executor, den Spring an verschiedenen Stellen für asynchrone Aufrufe nutzt, erhält dafür jeweils einen neuen VirtualThread, statt wie vorher einen Thread-Pool. Die zweite Deklaration konfiguriert den eingebetteten Tomcat, mit dem Spring Boot die Web-Anfragen bearbeitet. Hier ist normalerweise ebenfalls ein Threadpool hinterlegt. Mit der Konfiguration fällt dieser Pool weg und es wird jedes Mal ein neuer VirtualThread zur Bearbeitung angelegt. Das als Configuration eingefügt und schon kommen Servlet-Requests bereits fertig als VirtualThread an. Spring Boot hat VirtualThreads auf dem Schirm, passt immer mal wieder etwas an und ist schon recht weit damit, VirtualThreads sehr effizient zu nutzen. Micronaut hat ebenfalls schon Support vorbereitet , der getestet werden kann. Und für Quarkus gibt es schon sehr weitreichenden Support . Und sogar in Wildfly 27 lässt sich VirtualThread-Support aktivieren. Angewohnheiten hinterfragen Mit Projekt Loom müssen wir fast nie neue Konzepte lernen. Stattdessen können wir alte Gewohnheiten ablegen: ThreadPools werden in den meisten Fällen keinen Mehrwert mehr bieten. Im Gegenteil fügen sie Overhead hinzu und verlangsamen den eigenen Code . Wo wir bisher Poolen, zum Beispiel um die Anzahl gleichzeitig durchgeführter Requests zu limitieren, können wir wieder (wie früher) Semaphoren beim Funktionsaufruf nutzen. Synchron war nie schlecht Und dann natürlich die Erkenntnis: für 99&#160;% aller Applikationen da draußen war asynchrone Programmierung nie nötig. Auch nicht ohne Projekt Loom. Die wenigsten haben mehr als 30.000 gleichzeitige Requests pro Service-Instanz. Moderne Hardware hat damit kein Problem, auch nicht mit 30k Betriebssystem-Threads. Und weil die Stack-Größe nur virtuellen Speicher angibt, haben wir auf 64-Bit-Systemen kein Problem damit. Ausblick: Structured Concurrency Bis mit Java 21 im Herbst 2023 das nächste LTS-Release aufschlägt, soll auch Structured Concurrency mit aufgenommen sein. Damit lassen sich dann die Stellen übersichtlich angehen, bei denen innerhalb einer Aufgabe Anfragen und Berechnungen parallel erfolgen sollen. @GetMapping(\"/trains\") fun listTrainsParallel(): TrainList&lt;TrainRepresentation&gt; { val list = StructuredTaskScope.ShutdownOnSuccess&lt;List&lt;Train&gt;&gt;().use { scope -&gt; scope.fork { serverA.listActiveSync() } scope.fork { serverB.listActiveSync() } scope.join().result().map { it.toListRepresentation() } } val count = StructuredTaskScope.ShutdownOnSuccess&lt;Int&gt;().use { scope -&gt; scope.fork { serverA.countActiveSync() } scope.fork { serverB.countActiveSync() } scope.joinUntil(Instant.now().plusSeconds(15)).result() } return TrainList(list, count) } Bei den beiden Abfragen können wir einfach (übrigens wieder als Monade) deklarieren, dass die dahinter liegenden Abfragen in separaten Threads erfolgen - im besten Fall in VirtualThreads. \"ShutdownOnSuccess\" sorgt dafür, dass das erste verfügbare Ergebnis gewinnt und alle anderen Threads beendet werden. Wir können einen Timeout mitgeben, um die Laufzeit - hier auf 15 Sekunden - zu begrenzen. Dabei ist wichtig: Es geht bei Structured Concurrency wirklich fast nur um die Lesbarkeit und Wartbarkeit. Schneller oder Ressourcen-sparender wird es dadurch nicht. Also: Es wird spannend im Java-Ökosystem. Mit Projekt Loom werden tatsächlich die Karten neu gemischt. Endlich können wir den Programmierstil wieder so aussuchen, wie er zu unseren Gehirnen passt. "
},

{
    "id": 11,
    "uri": "blog/2023/2023-04-09-vortrag-auf-der-javaland.html",
    "menu": "Blog",
    "title": "JavaLand 2023",
    "text": " Table of Contents Vortrag auf der JavaLand 2023 Slides Vortrag auf der JavaLand 2023 Vom 20. bis 23. März fand mit der JavaLand die große Java-Community-Konferenz im Phantasialand Brühl statt - und DB Systel war mit dabei. In dem Talk \"Fantastische Diagramme und wie Du sie selbst erstellst\" haben Falk Sippach (embarc) und Ralf D. Müller (DB Systel) gemeinsam Schwachstellen von Architekturdiagrammen aufgespürt und ausgemerzt. In den Slides des Vortrags (siehe Download-Link unten) finden sich als Ergebnis ein paar Tipps und Checklisten, die jeder für seine eigenen Diagramme verwenden kann. Abends gab es dann noch einen Fun-Workshop \"Hacking the RP2040\", bei dem wir uns angesehen haben, was man so alles mit dem Tufty-Badge von Pimoroni anstellen kann. Dabei handelt es sich um ein kleines 320x240 Display mit Tastern und RP2040 Microcontroller (besser bekannt als Raspberry Pi Pico) in der Form eines Badges zum Umhängen. Das eigentliche Highlight der jährlichen JavaLand war aber die Community mit ihren vielen Hallway-Tracks und dem Newcomer-Programm \"NextGen\" über das immer wieder frische Themen und Sichtweisen in das Programm aufgenommen werden. Slides Slides des Workshops "
},

{
    "id": 12,
    "uri": "blog/2023/2023-04-01-Indoor-GIS-zur-Rationalisierung-von-Wartungsarbeiten.html",
    "menu": "Blog",
    "title": "Indoor-GIS",
    "text": " Table of Contents Indoor-GIS zur Rationalisierung von Wartungsarbeiten Slides Indoor-GIS zur Rationalisierung von Wartungsarbeiten Genial kombiniert: Mit Tracking-Technologie und unserer Esri-Plattform Kosten und Durchlaufzeiten reduzieren. Durch die Kombination von Tracking-Technologie und unserer Esri-Plattform nutzen wir die modernsten Tracking-Systeme. Auf diese Weise reduzieren wir Kosten und Durchlaufzeiten. Eine Erfolgsgeschichte in der Arbeitswelt. Erfahren Sie, wie die offene Plattformarchitektur viele Anwendungsfälle ermöglicht. Beispiele finden Sie in der beigefügten Präsentation. Sie wurde im Rahmen eines Vortrags von Konrad Winkler &amp; Philippe Rieffel im Rahmen der Esri International Infrastructure Management &amp; GIS Conference am 18.04.2023 in Frankfurt gezeigt. Hinweis: Die Präsentation liegt in englischer Sprache vor. Slides download "
},

{
    "id": 13,
    "uri": "blog/2023/2023-03-28-ChatGPT-Einblicke-und-mehr-Generative-Sprachmodelle-Herausforderungen-und-Chancen.html",
    "menu": "Blog",
    "title": "ChatGPT",
    "text": " Table of Contents ChatGPT – Funktionsweise, Chancen, Risiken und Alternativen einfach erklärt Video ChatGPT – Funktionsweise, Chancen, Risiken und Alternativen einfach erklärt In diesem Video erkläre ich leichtverständlich die Funktionsweise von ChatGPT im Zusammenhang mit generativer Künstlicher Intelligenz (Generative AI) und großen Sprachmodellen (Large Language Models, kurz LLMs). Anhand von Anwendungsfällen beschreibe ich Chancen, Risiken und Alternativen und diskutiere, was das für uns und unsere Jobs zukünftig bedeuten könnte.  Video "
},

{
    "id": 14,
    "uri": "blog/2023/2023-03-08-Re-Platforming-Mainframe-Mehr-als-nur-Lift-Shift.html",
    "menu": "Blog",
    "title": "Re-Platforming Mainframe",
    "text": " Table of Contents Re-Platforming Mainframe – Mehr als nur Lift&amp;Shift Slides Re-Platforming Mainframe – Mehr als nur Lift&amp;Shift Die sinkende Bedeutung der Mainframe-Plattform führt uns zu unserem Projektziel Lift&amp;Shift. Die Migration in die Cloud soll mit möglichst wenigen Anpassungen erfolgen. Dabei wollen wir die IBM Mainframe basierten Cobol Anwendungen durch schrittweise Überführung der Anwendungen in die DB Enterprise Cloud ablösen. Die Schaffung einer zukunftsorientierten IT-Architektur und signifikante Einsparungen von Betriebskosten durch die Nutzung cloudbasierter Infrastruktur sind ebenso wichtig. Lesen Sie in unserer Präsentation, welche Projektziele wir außerdem verfolgen, wer unsere Partner sind und wie unsere Bestandsanalyse aussieht. Gezeigt wurde sie im Rahmen eines Vortrags von Tim Engeleiter am 08.03.2023 bei den Mainframe Dayz in Wiesbaden.   Slides download "
},

{
    "id": 15,
    "uri": "blog/2022/2022-15-03-good-practices-api.html",
    "menu": "Blog",
    "title": "Good Practices im API-Umfeld",
    "text": " Table of Contents Hätt' ich das früher gewusst - Good Practices bei API-Konzeption &amp; -Entwicklung Slides und Video Hätt' ich das früher gewusst - Good Practices bei API-Konzeption &amp; -Entwicklung Wenn du diese API noch einmal konzipieren könntest, würdest du alles noch mal genauso machen? Nicht ganz, ich würde von Anfang an &#8230;&#8203; ja, was eigentlich? Im Vortrag ziehe ich eine Zwischenbilanz aus über drei Jahren API-Entwicklung bei der DB Systel GmbH, indem ich unsere Vorgehensweisen bei API-Design und -Implementierung analysiere und praktische Ratschläge daraus ableite. Unser Vorhaben war, viele APIs für generische Aufgaben (wie Bezahlung oder Routing) und Daten (wie von Bahnhöfen oder Sharing-Fahrzeugen) zentral bereitzustellen. Doch wieso erwies sich das in vielen dieser Fälle als ungeeignet? Das Einhalten von Paradigmen wie API-first und REST stand anfangs im Fokus aller Produkte. Aber warum ist API-first gar nicht immer optimal? Und wieso können wir heute mit imperfekten REST-APIs ruhig schlafen? Mittlerweile nutzen wir Tools und Frameworks wie den OpenAPI-Generator, MapStruct, Lombok und OpenFeign. Was hat uns anfangs davon abgehalten? Slides und Video "
},

{
    "id": 16,
    "uri": "blog/2022/2022-11-24-the-journey-towards-K8s-at-Deutsche-Bahn.html",
    "menu": "Blog",
    "title": "K8s at Deutsche Bahn",
    "text": " Table of Contents The journey towards K8s at Deutsche Bahn Video The journey towards K8s at Deutsche Bahn This lecture was given by Gualter Barbas Baptista from DB Systel at the Containerdays from 05 to 07 September 2022. ( https://www.containerdays.io/ ) In 2016, Deutsche Bahn decided to get rid of their own data centers and to migrate the majority of applications to the cloud. The cloudification of Deutsche Bahn was supported by a comprehensive transformation of its digital partner DB Systel from traditional working and organisational structures to self-organisation and company-wide networks, including a DevOps culture. Within this context, the first Kubernetes platform services emerge. From an OpenShift-based Kubernetes-Namespace-as-a-Service into a GitOps based K8s fleet management, we describe how the cloud, the organisational transformation and the CNCF landscape are accelerating the digitalisation of Deutsche Bahn. Video "
},

{
    "id": 17,
    "uri": "blog/2022/2022-11-04-Produkt-statt-Projekmanagement.html",
    "menu": "Blog",
    "title": "Produkt- statt Projektmanagement",
    "text": " Table of Contents Produkt- statt Projektmanagement Video Produkt- statt Projektmanagement Warum das Funding von Projekten mittlerweile zu einem der größten Hindernisse auf dem Weg zu einer BizDevOps-Organisation geworden ist − ein Debattenbeitrag zum nächsten notwendigen Schritt der DevOps-Bewegung. Die DevOps-Bewegung ist in der Realität und im Herzen der digitalen Industrie angekommen. In der Systel arbeiten knapp 100 Teams nach dem Prinzip „Your build (or integrate) it, you run it“ und sind so in der Lage, regelmäßig und mit hoher Frequenz Änderungen und damit „Business Value“ zu den Nutzerinnen und Nutzern zu bringen. Nicht nur bei uns, sondern in allen vergleichbaren großen Konzernen sind Cloud-Nutzung, Automatisierung, Continuous Delivery, Platform Strategy und der dafür notwendige kulturelle Wandel auf der Tagesordnung. Ein großer Hebel für den Wandel zu einer DevOps-Organisation wird allerdings häufig übersehen: das etablierte Funding von digitalen Maßnahmen über Projekte ist mittlerweile zu einem der Haupt-Hindernisse auf dem Weg zu einer leistungsfähigen Delivery-Organisation geworden. Aus meiner Sicht müssen wir daher ganz vorne mit der Veränderung ansetzen, nämlich dort, wo Projekte entstehen, und über Projektbudgets entschieden wird. Warum ich dieser Meinung bin, habe ich auf einem Vortrag beim DevOps Enterprise Summit dargelegt, der mittlerweile auch über den YouTube Kanal der DB Systel zugänglich ist. Video "
},

{
    "id": 18,
    "uri": "blog/2022/2022-10-21-Deine-Diagramme-sind-Legende.html",
    "menu": "Blog",
    "title": "Deine Diagramme sind Legende?",
    "text": " Table of Contents Deine PlantUML-Diagramme sind Legende? pre { white-space: pre-wrap; } table.tableblock { overflow: auto; width: 100%;} td.tableblock {overflow: auto; width: 50%;} Deine PlantUML-Diagramme sind Legende? &#8230;&#8203;dann verpasse ihnen eine Legende! Ein Diagramm soll nicht nur für Insider lesbar sein. Mit einer Legende erklärst du die verwendeten Symbole und Farben. In diesem Artikel zeige ich dir, wie es geht. PlantUML verfügt über ein wenig dokumentiertes Element namens \" Legend \". Damit lässt sich eine Box im Diagramm z. B. in der rechten unteren Ecke platzieren. Wie aber der Inhalt dargestellt werden soll ist unklar. @startuml skinparam actorStyle awesome database Datenbank :User: -&gt; [Komponente] [Komponente] -&gt; Datenbank #green legend right &lt;b&gt;Legende&lt;/b&gt; ??? endlegend @enduml Google findet als Idee, dass die Legende als Tabelle in Creole-Syntax erstellt werden kann. Farben kann man damit gut erklären, aber für Symbole können nur Emojis oder spezielle Zeichen verwendet werden. @startuml skinparam actorStyle awesome database Datenbank :User: -&gt; [Komponente] [Komponente] -&gt; Datenbank #green legend right &lt;b&gt;Legende&lt;/b&gt; | &lt;#red&gt; | Benutzer-Zugriff | | &lt;#green&gt; | Datenbank-Verbindung | | &lt;:smiley:&gt; | Benutzer :-) | endlegend @enduml In einem Forum habe ich am Rande den Hinweis gefunden, dass man mit dem Map-Statement des Objektdiagramms auch eine Tabelle aufbauen kann. Nur geht das nicht direkt innerhalb der Legende. Es gibt aber den Trick, dass man mit der `{{ &#8230;&#8203; }} Syntax ein neues Diagramm innerhalb des Diagramms erstellen kann. Damit lässt sich dann auch eine Map innerhalb der Legende aufbauen. @startuml skinparam actorStyle awesome database Datenbank :User: -&gt; [Komponente] [Komponente] -&gt; Datenbank #green legend right {{ map \"&lt;b&gt;Legende&lt;/b&gt;\" as legend #white { &lt;#red&gt; =&gt; Benutzer-Zugriff &lt;#green&gt; =&gt; Datenbank-Verbindung &lt;:smiley:&gt; =&gt; Benutzer :-) } }} endlegend @enduml Und wenn wir jetzt schon dabei sind Diagramme innerhalb von Diagrammen zu nutzen, dann können wir das auch noch eine Ebene tiefer machen. Dadurch schaffen wir es in der Legende die Diagramm-Elemente zu zeichnen, die wir beschreiben wollen. Dazu bauen wir uns in einer Prozedur ein universelles Mini-Diagramm: scale $scale skinparam backgroundcolor transparent label \" \" as A label \" \" as B $type Der scale-Befehl erlaubt es die zu beschreibende Komponente kleiner darzustellen und somit die Legende kompakt zu halten. Die beiden unsichtbaren Labels sorgen dafür, dass wir einen Connector von A nach B darstellen können. Das ganze sieht dann kompakt wie folgt aus: @startuml skinparam actorStyle awesome database Datenbank :User: -&gt; [Komponente] [Komponente] -&gt; Datenbank #green legend right {{ !procedure $entry($type, $label, $scale=1) {{\\nscale $scale \\nskinparam backgroundcolor transparent\\nlabel \" \" as A\\nlabel \" \" as B\\n $type \\n}} =&gt; $label !endprocedure map \"&lt;b&gt;Legende&lt;/b&gt;\" as legend #white { $entry(\":Actor:\",\" Benutzer\", 0.5) $entry(\"[component]\",\" Benutzer\", 0.7) $entry(\"database db\",\"Datenbank\", 0.7) $entry(\"A -&gt; B\",\"Benutzer-Zugriff\") $entry(\"A -&gt; B #green\",\"Datenbank-Verbindung\") } }} endlegend @enduml Im letzten Schritt möchte ich die Legende mit ein paar Styles noch aufhübschen. Der doppelte Rahmen soll weg und etwas kleiner wäre auch nicht schlecht. @startuml skinparam actorStyle awesome skinparam legendBackgroundColor transparent skinparam legendBorderColor transparent database Datenbank :User: -&gt; [Komponente] [Komponente] -&gt; Datenbank #green legend right {{ scale 0.8 skinparam defaultFontSize 14 skinparam BackGroundColor transparent skinparam defaultBackgroundColor white !procedure $entry($type, $label, $scale=1) {{\\nscale $scale \\nskinparam backgroundcolor transparent\\nlabel \" \" as A\\nlabel \" \" as B\\n $type \\n}} =&gt; $label !endprocedure map \"&lt;b&gt;Legende&lt;/b&gt;\" as legend #white { $entry(\":Actor: #green\",\"\\nBenutzer\", 0.5) $entry(\"[component]\",\"\\nBenutzer\", 0.7) $entry(\"database db\",\"\\nDatenbank\", 0.7) $entry(\"A -&gt; B\",\"Benutzer-Zugriff\") $entry(\"A -&gt; B\",\"Datenbank-Verbindung\") } }} endlegend @enduml Bei der Nutzung fällt schnell auf, dass die Legende zu viel Platz einnimmt. Sie duldet keine anderen Diagramm-Elemente neben sich. Also haben wir weiter geforscht. Mit dem Diagramm in der Legende besteht eigentlich kein Grund mehr wirklich das Element Legend zu verwenden. Was passiert, wenn wir es durch eine rectangle ersetzen und diese entsprechend Stylen? Dazu müssen wir dem Element einen Stereotype verpassen, da wir sonst alle rectangle -Elemente stylen würden. Und siehe da, es funktioniert. Durch diesen Trick haben wir nun mehr Einfluss auf die Platzierung, denn wir können dieses rectangle -Element durch versteckte Verbindungen beeinflussen. @startuml skinparam actorStyle awesome database Datenbank :User: -&gt; [Komponente] [Komponente] -down-&gt; Datenbank #green rectangle a &lt;&lt;test&gt;&gt; Datenbank -left-&gt; a skinparam rectangle&lt;&lt;legend&gt;&gt; { backgroundColor transparent borderColor transparent shadowing false } hide &lt;&lt;legend&gt;&gt; stereotype rectangle legende &lt;&lt;legend&gt;&gt; [ {{ scale 0.8 skinparam defaultFontSize 14 skinparam BackGroundColor transparent skinparam defaultBackgroundColor white !procedure $entry($type, $label, $scale=1) {{\\nscale $scale \\nskinparam backgroundcolor transparent\\nlabel \" \" as A\\nlabel \" \" as B\\n $type \\n}} =&gt; $label !endprocedure map \"&lt;b&gt;Legende&lt;/b&gt;\" as legend #white { $entry(\":Actor:\",\"\\nBenutzer\", 0.5) $entry(\"[component]\",\"\\nBenutzer\", 0.7) $entry(\"database db\",\"\\nDatenbank\", 0.7) $entry(\"A -&gt; B\",\"Benutzer-Zugriff\") $entry(\"A -&gt; B #green\",\"Datenbank-Verbindung\") } }} ] User -[hidden]-&gt; legende legende -[hidden]down-&gt; a @enduml Übrigens: PlantUML möchte Elemente und ihre Verbindungen immer optimiert platzieren. Es kann also sein, dass die neue Legende deshalb noch mal kräftig durchmischt. Es gibt aber nicht nur die Pfeildefinition -[hidden]&#8594; , um eine Verbindung nicht anzuzeigen. Der Pfeil -[norank]&#8594; ist eine Verbindung, welche bei besagter Optimierung ignoriert wird. Beide Features kann man kombinieren: Mit einem -[norank,hidden]&#8594; ist die Legende unsichtbar mit einem anderen Element verbunden, ohne dass dies das Diagramm umstrukturiert. "
},

{
    "id": 19,
    "uri": "blog/2022/2022-03-23-Vielfalt-bei-der-Bahn-Computerlinguistinnen-treiben-Digitalisierung-voran.html",
    "menu": "Blog",
    "title": "Vielfalt bei der Bahn",
    "text": " Table of Contents Vielfalt bei der Bahn: Computerlinguist:innen treiben Digitalisierung voran Video Vielfalt bei der Bahn: Computerlinguist:innen treiben Digitalisierung voran Dieses Mal haben wir Claudia Schönfelder aus dem Team SALT zu Gast, mit der wir über das Thema “SINUS”, den Sprachassistenten für die Instandhaltung von Zügen, und den Beruf der Computerlinguist:innen, sprechen. Claudia entwickelt multidisziplinär mit mehreren Teams von Expert:innen einen der ersten Sprachassistenten für die industrielle Nutzung. Wir alle kennen Siri und Alexa, aber diese bewältigen in der Regel Aufgaben des Alltags. Im Gegensatz dazu führt SINUS die Konzernpartner:innen aus der Instandhaltung per Sprache durch digitalisierte Formulare, um die Eingabe von Schäden an Loks gänzlich über die Spracherkennung zu tätigen. Im Interview spricht sie mit uns über dieses ehrgeizige Projekt, das die Effizienz bei der Befundung von Zügen deutlich steigern kann. Wir spielen anhand eines Beispiels durch, welche Besonderheiten dieses Tool bei der Befundung besitzt. Dazu erklärt Claudia wie sich das Berufsbild der Computerlinguisten in den letzten Jahren entwickelt und etabliert hat. Information, die auch für die jüngeren unter uns, von großem Interesse sein kann.   Video "
},

{
    "id": 20,
    "uri": "blog/2021/2021-11-02-KITT-das-Kuenstliche-Intelligenz-Translation-Tool.html",
    "menu": "Blog",
    "title": "KITT Tool",
    "text": " Table of Contents KITT, das Künstliche Intelligenz Translation Tool Video KITT, das Künstliche Intelligenz Translation Tool Dieses Mal haben wir Pia Schwarz aus dem Team SALT zu Gast, mit der wir über das Thema “KITT”, das Künstliche Intelligenz Translation Tool, sprechen. Pia bringt mit ihrem Team Maschinen das Sprechen bei. Im Interview spricht sie mit uns über KITT, das als Interface gesprochene Sprache entgegennimmt. Es wird eingesetzt, um die Kommunikation im Zugfunk von Fahrdienstleiter:innen und Triebfahrzeugführer:innen im Grenzbereich von Frankreich und Deutschland zu verbessern. Wir spielen anhand eines Beispiels durch, welche Besonderheiten das Bahnjargon mit sich bringt und warum am Markt verfügbare Übersetzungstool in diesem Anwendungsfall nicht ausreichend sind. KITT hilft ebenso die Anforderungen an das Sprachniveau zu senken, in dem es sogenannte Predefined Messages zuverlässig übersetzt, die im europäischen Bahnverkehr verwendet werden. In der zweiten Hälfte des Interviews steigen wir tiefer in die Technik ein und gehen auf die Herausforderungen ein. Denn die Anforderungen an KITT sind nicht weniger, als exakte Übersetzungen zu produzieren. Zum Beispiel sind Übersetzungen bei Hintergrundgeräuschen oder Sätze mit Eigennamen problematisch. Zum Abschluss sprechen wir über die technische Umsetzung, Trainingsdatenmenge und Open Source Frameworks. Video "
},

{
    "id": 21,
    "uri": "blog/2021/2021-06-08-Bad-bots-Chancen-und-Herausforderungen-fuer-KI-und-Sprache.html",
    "menu": "Blog",
    "title": "Bad Bots",
    "text": " Table of Contents Bad Bots - Chancen und Herausforderungen für KI und Sprache Video Bad Bots - Chancen und Herausforderungen für KI und Sprache Sascha Wolter, Chief Advisor Conversational AI bei der DB Systel GmbH, hielt diesen Vortrag am 06.05.2021 im Rahmen der Digital Office Conference 2021 von Bitkom. Kaum jemand scheint seine Alexa nach mehr als dem Wetter oder schlechten Witzen zu fragen. Und vier der fünf erfolgreichsten Alexa Skills erzeugen nicht mehr als Pupsgeräusche. Dies steht in einem krassen Widerspruch zu den Berichten über künstliche Intelligenz in den Medien, denen zufolge weltbeherrschende Computer wie Skynet (Terminator) unmittelbar bevorstehen. Und selbst die, die den technische Fortschritt weniger als Bedrohung denn als Chance verstehen, haben oft eine völlig falsche Vorstellung von den Möglichkeiten von #Chatbots und #Sprachassistenten. Nicht selten wird davon ausgegangen, dass – sofern man ausreichend Daten hat – die #KI alles automatisch erledigt und sich der Erfolg mehr oder weniger von ganz allein einstellt. Dies ist offensichtlich noch nicht so, wie die vielen schlechten Bots im Markt eindrucksvoll zeigen. Doch wie man diesem Ziel zumindest möglichst nahe kommt, zeigt Sascha Wolter anhand zahlreicher praktischer Beispiele. Er behandelt nicht nur die Hintergründe, sondern zeigt auch technische und gestalterische Lösungen.  Video "
},

{
    "id": 22,
    "uri": "blog/2021/2021-04-12-Computer-Vision-Use-Cases-at-Deutsche-Bahn.html",
    "menu": "Blog",
    "title": "Computer Vision Use Cases",
    "text": " Table of Contents Computer Vision Use Cases @ Deutsche Bahn Video Computer Vision Use Cases @ Deutsche Bahn Wie können KI-Bildanalysen bei der Graffiti-Erkennung helfen und welche Potentiale birgt das für die Bahn? Im Rahmen des Data Festivals 2021 präsentierte das DB Systel Venture vsion.ai gemeinsam mit der Data Science Beratung Alexander Thamm Einsatzmöglichkeiten von KI-Analysen auf Bildern im Bahnkontext. Am Beispiel der automatischen Erkennung von Graffiti auf Zügen zeigt Peco Elenchevski die Besonderheiten des Use Cases auf sowie die technische Umsetzung eines Proof of Concept. Nico Becker knüpft dort an und beschreibt die Herausforderungen, welche sich aus dem Deployment von KI-Modellen ergeben. Dabei skizziert er einen Weg, wie sich ein Proof of Concept zu einem robusten Produktivsystem weiterentwickeln lässt. Der Vortrag wurde vor internationalem Publikum gehalten und ist daher auf Englisch. Video "
},

{
    "id": 23,
    "uri": "blog/2021/2021-03-20-Die-C4-Testpyramide-eine-architekturgetriebene-Teststrategie.html",
    "menu": "Blog",
    "title": "Die C4-Testpyramide",
    "text": " Table of Contents Die C4-Testpyramide - eine architekturgetriebene Teststrategie Video Die C4-Testpyramide - eine architekturgetriebene Teststrategie Die Präsentation zeigt, wie sich die Prinzipien der Test Pyramide mit dem C4 Modell zur Visualisierung von Software Architekturen verbinden lassen, um so auf einfache Weise zu einer produktspezifischen Teststrategie zu gelangen. Video "
},

{
    "id": 24,
    "uri": "blog/2020/2020-12-07-devops-mehr-geschwindigkeit-auf-der-schiene.html",
    "menu": "Blog",
    "title": "DevOps Geschwindigkeit",
    "text": " Table of Contents DevOps - Mehr Geschwindigkeit auf der Schiene Slides und Video DevOps - Mehr Geschwindigkeit auf der Schiene In diesem Vortrag im Rahmen der IT-Tage 2020 erklärt Carsten Hoffmann von der DB Systel, warum sich der erste Ansatz einer zentralen CI/CD-Installation im Projekt, eine Cloud-native Plattform für API-getriebene Softwareentwicklung aufzubauen, für alle Teams als problematisch erwies und durch dezentrale Pipelines ersetzt wurde. Danach werden die Hindernisse bei der Einführung einer eingekauften API-Management-Lösung erklärt und wieso sich der Einkauf von großer On-Premise-Software nur schwierig mit den agilen Prinzipien vereinbaren lässt. Außerdem wird erläutert, wie im Team mit polyglotter Softwareentwicklung und permanent gegen Wissensinseln angekämpft wurde. Zuletzt geht Casten Hoffmann darauf ein, wie das Team mit umfassender Architekturdokumentation begonnen hat und gescheitert ist. Slides und Video "
},

{
    "id": 25,
    "uri": "blog/2020/2020-05-19-5vue-js-vs-angular-was-ist-besser.html",
    "menu": "Blog",
    "title": "Vue.js vs. Angular",
    "text": " Table of Contents Vue.js vs. Angular: Was ist besser? Video Vue.js vs. Angular: Was ist besser? Heute zu Gast bei #000000 #c0ffee – Der Tech-Talk der DB Systel. Von Techies für Techies: Danny Koppenhagen Danny ist Frontend-Entwickler mit den Schwerpunkten Angular und Vue.js und einer der Autoren des Buches „Angular - Grundlagen, fortgeschrittene Themen und Best Practices“. Im dx.house Berlin berät er außerdem Kunden und Teams in den Themen User Experience von Enterprise-Lösungen. Er engagiert sich in der Web Community der DB Systel und ist Mitglied im Themen Team Web der Architekturgilde. Dort erarbeitet er Architektur-Standards für alle Themen Web. Im Interview spricht er über seine Erfahrungen mit Vue.js und Angular. Er geht darauf ein, welches Framework sich für welche Anwendungszwecke eignet. So bietet Vue.js hauptsächlich Vorteile, wenn es um die Integration in bestehende Anwendungen handelt und das Team gerne JavaScript einsetzt. Angular ist im Enterprise-Umfeld für neue Anwendungen interessant, da es auf TypeScript aufsetzt, ein umfangreiches Ökosystem mitbringt und zum Beispiel Migrationsguides und Templating-Fähigkeiten über sogenannte Schematics mitbringt. Außerdem erläutert Danny wie der aktuelle Stand der Technik für Progressive Webapps (PWA) ist. Hier kommt es darauf an, ob alle benötigten Features des Betriebssystems angesprochen werden können. Falls nicht, sollte in Erwägung gezogen werden eine native App zu entwickeln. Im dritten Teil sprechen wir über die Anbindung von APIs. Um die Orchestrierung von APIs zu vereinfachen, kann hier das Architekturmuster Backend For Frontends zum Einsatz kommen. Das vereinfacht den Zugriff aus der Anwendung, da die Anbindung der APIs nicht einzeln im Frontend implementiert werden muss. Video "
},

{
    "id": 26,
    "uri": "blog/2020/2020-03-27-DB-Systel-streitet-auf-der-OOP-fuer-guten-Code.html",
    "menu": "Blog",
    "title": "OOP: Guter Code",
    "text": " Table of Contents DB Systel streitet auf der OOP für guten Code Video DB Systel streitet auf der OOP für guten Code In diesem unterhaltsamen Pecha Kucha Vortrag spricht Carsten Thurau über seine Erfahrung aus dem Alltag als erfahrener Trainer und Coach. Er zeigt in seinem Kurzvortrag, warum schlechter Code Entwickler unglücklich macht und Code Reviews, TDD und Clean Code eine gute Idee sind. Fazit: Entwickelt qualitativ hochwertigen Code und seid stolz auf Euer Werk! Der Talk wurde auf der OOP 2020 Konferenz in München Anfang Februar 2020 aufgezeichnet. Video "
},

{
    "id": 27,
    "uri": "blog/2020/2020-03-14-API-first-mit-TypeScript.html",
    "menu": "Blog",
    "title": "API first mit TS",
    "text": " Table of Contents API first mit TypeScript API first mit TypeScript Mit API first kann man sehr schön REST APIs bauen. Verwendet man TypeScript, kann man mittels der Bibliothek express-openapi nicht nur einmalig einmalig ein Interface generieren, sondern auch bei späteren Änderungen API first beibehalten. Im Video zeige ich anhand eines praktischen Beispiels, wie man ein solches Projekt aufsetzt. In 6 Schritten setzen wir ein TypeScript Projekt mit express.js und express-openapi auf. Es bringt ein Swagger UI und Unit-Tests mit und lässt sich - natürlich ebenso API first wie am Anfang - leicht weiter entwickeln. Die Commits im Repository erklären, wie man schrittweise einen solchen REST Service aufbaut. "
},

{
    "id": 28,
    "uri": "blog/2019/2019-09-13-Spock-und-AsciiDoc.html",
    "menu": "Blog",
    "title": "Spock und AsciiDoc",
    "text": " Table of Contents Spock und AsciiDoc - vom Test zur Spezifikation und zurück Slides und Video Spock und AsciiDoc - vom Test zur Spezifikation und zurück Spock ist ein Testframework für Webanwendungen, mit dem man unter anderem den Behavior Driven Development Ansatz, kurz BDD, verfolgen kann. Der Product Owner beschreibt das Verhalten einer Applikation und der Entwickler überprüft es über einen automatischen Test. Dem Entwickler reicht die Ausgabe \"PASSED\" oder \"FAILED\", denn er kennt ja den Code seiner Tests. Wäre es nicht cool, wenn auch der Product Owner ein verständliches Dokument bekäme? Kein Problem! Wir generieren über ein Template einfach einen Test-Report in AsciiDoc und fügen weitere erklärende Texte hinzu um eine les- und ausführbare Spezifikation zu erhalten. Screenshots aller wichtigen Schritte bereichern die Spezifikation weiter. Sollte aber die Spezifikation nicht am Anfang stehen? Und warum Spezifikation, wenn wir agil sein wollen? Richtig! Stellen wir also eine iterative Feature-Beschreibung an den Anfang und verfeinern diese mit automatischen Tests um am Ende eine gut lesbare und verifizierbare Spezifikation des Verhaltens unseres Systems zu erhalten! Die Vorteile liegen auf der Hand – die Vorgehensweise verbessert die Kommunikation zwischen Product Owner und Entwicklern und am Ende bekommen wir ein Dokument welches Ihre wertvolle Software korrekt und überprüfbar beschreibt. Slides und Video "
},

{
    "id": 29,
    "uri": "blog/index.html",
    "menu": "Blog",
    "title": "Übersicht",
    "text": " Table of Contents Übersicht Übersicht Willkommen auf dem Tech Blog der DB Systel. "
},

{
    "id": 30,
    "uri": "blog/profiles/Stefan+Gr%C3%BCndling.html",
    "menu": "Autoren",
    "title": "Stefan Gründling",
    "text": " Table of Contents Stefan Gründling Stefan Gründling span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } "
},

{
    "id": 31,
    "uri": "blog/profiles/Sascha-Wolter.html",
    "menu": "Autoren",
    "title": "Sascha Wolter",
    "text": " Table of Contents Sascha Wolter Links Sascha Wolter span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } Sascha Wolter ist Experte für die Planung und Umsetzung von geräteübergreifenden Anwendungen. Als solcher begeistert er sich für das Benutzererlebnis und erkundet verbesserte multimodale Interaktionsformen zwischen Mensch und Maschine – u. a. in Form von Konversation über Text (Chatbots) und Sprache (auch als Alexa bekannt). Bereits seit 1995 arbeitet er als Berater, Dozent, Sprecher und Autor. In seiner Freizeit begeistert er sich für Bergsport von Wandern bis Ski und genießt guten italienischen Kaffee. Er ist Chief Advisor für Conversational AI bei DB Systel, TecCo Lead HMI bei Deutsche Bahn und er engagiert er sich als Vorstandsmitglied im Arbeitskreis Usability &amp; User Experience der BITKOM. Für sein Developer- und Community-Engagement wurde er mehrfach als Google Developer Expert für den Google Assistant (GDE) ausgezeichnet. Vorher war er Senior UX Consultant und Principal Technology Evangelist bei der Conversational AI Platform Company Cognigy, arbeitete er als Senior Developer Evangelist bei der Deutschen Telekom (u. a. Smart Home), als Senior Technology Evangelist für Alexa bei Amazon und als Freiberufler. Links LinkedIn Persönliche Website "
},

{
    "id": 32,
    "uri": "blog/profiles/Jonas+Gassenmeyer.html",
    "menu": "Autoren",
    "title": "Jonas Gassenmeyer",
    "text": " Table of Contents Jonas Gassenmeyer Jonas Gassenmeyer span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } "
},

{
    "id": 33,
    "uri": "blog/profiles/Johannes-Dienst.html",
    "menu": "Autoren",
    "title": "Johannes Dienst",
    "text": " Table of Contents Johannes Dienst Johannes Dienst span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } "
},

{
    "id": 34,
    "uri": "blog/profiles/Dr.-Martin-Strunk.html",
    "menu": "Autoren",
    "title": "Dr. Martin Strunk",
    "text": " Table of Contents Dr. Martin Strunk span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } Dr. Martin Strunk Dr. Martin Strunk has been working for more than 22 years in different expert and management roles in development and operations at DB Systel. In 2018 Dr. Martin Strunk initiated and lead the DevOps-Transformation Project “Two Deployments per Day (2D/d)” at DB Systel, where the technical, organizational and cultural foundations for a DevOps IT delivery model have been created. Currently, he leads as an Agility Master the Customer Experience Unit of DB Systel with more than half a dozen engineering teams that work according to the “You build it, you run it”-paradigm. LinkedIn Xing "
},

{
    "id": 35,
    "uri": "blog/profiles/Christian-Fischer.html",
    "menu": "Autoren",
    "title": "Christian Fischer",
    "text": " Table of Contents Christian Fischer Christian Fischer span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } "
},

{
    "id": 36,
    "uri": "blog/profiles/Bertram-Fey.html",
    "menu": "Autoren",
    "title": "Bertram Fey",
    "text": " Table of Contents Bertram Fey Bertram Fey span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } "
},

{
    "id": 37,
    "uri": "blog/profiles/Oliver+Hammer.html",
    "menu": "Autoren",
    "title": "Oliver Hammer",
    "text": " Table of Contents Oliver Hammer Oliver Hammer span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } "
},

{
    "id": 38,
    "uri": "blog/profiles/Maximilian-Franzke.html",
    "menu": "Autoren",
    "title": "Maximilian Franzke",
    "text": " Table of Contents Maximilian Franzke Links Maximilian Franzke span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } Maximilian ist ein erfahrener Softwarearchitekt und Development Lead des DB UX Design System Core bei der DB Systel GmbH, dem Digitalpartner der Deutschen Bahn AG. Er konzipiert und entwickelt Customer und Enterprise Web Anwendungen, und ist spezialisiert im herausfordernden Umfeld von High Performance Websites und Digitaler Barrierefreiheit. Des Weiteren ist er ein Open-Source Enthusiast und an zahlreichen Web-bezogenen Lösungen beteiligt. Links X (formerly known as Twitter) Profile GitHub Profile "
},

{
    "id": 39,
    "uri": "blog/profiles/Marcus-Suemnick.html",
    "menu": "Autoren",
    "title": "Marcus Sümnick",
    "text": " Table of Contents Marcus Sümnick Marcus Sümnick span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } "
},

{
    "id": 40,
    "uri": "blog/profiles/Gualter-Barbas-Baptista.html",
    "menu": "Autoren",
    "title": "Gualter Barbas Baptista",
    "text": " Table of Contents Gualter Barbas Baptista Gualter Barbas Baptista span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } "
},

{
    "id": 41,
    "uri": "blog/profiles/Danny-Koppenhagen.html",
    "menu": "Autoren",
    "title": "Danny Koppenhagen",
    "text": " Table of Contents Danny Koppenhagen Links Danny Koppenhagen span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } Danny is an experienced frontend architect at DB Systel GmbH which is the digital partner of the biggest German railway company Deutsche Bahn . He develops and architects’ enterprise web applications within a DevOps team facing the micro mobility market. Furthermore, he is an open-source enthusiast and one of the authors of the popular German-language Angular book . Links LinkedIn Profile Mastodon Profile X (formerly known as Twitter) Profile GitHub Profile Personal Website "
},

{
    "id": 42,
    "uri": "blog/profiles/Tim-Engeleiter.html",
    "menu": "Autoren",
    "title": "Tim Engeleiter",
    "text": " Table of Contents Tim Engeleiter Tim Engeleiter span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } image: "
},

{
    "id": 43,
    "uri": "blog/profiles/Sven-Hesse.html",
    "menu": "Autoren",
    "title": "Sven Hesse",
    "text": " Table of Contents Sven Hesse Links Sven Hesse span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } Sven arbeitet als DevOps-Engineer bei der DB Systel GmbH. Seine Schwerpunkte liegen im Bereich der Entwicklung und Betreuung von APIs zu Zug- und Wagendaten sowie Shared-Mobility. Links Persönliche Website "
},

{
    "id": 44,
    "uri": "blog/profiles/Ralf-D.-Mueller.html",
    "menu": "Autoren",
    "title": "Ralf D. Müller",
    "text": " Table of Contents Ralf D. Müller Ralf D. Müller span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } Ralf is a Software Engineering Advocate at DB Systel GmbH during the day and after sunset he loves everything with bits and bytes. The last few years of his career, he focused on the documentation of software systems with arc42 and the Docs-as-Code approach. You can follow him on mastodon rdmueller@mastodontech.de . "
},

{
    "id": 45,
    "uri": "blog/profiles/Joachim-Schirrmacher.html",
    "menu": "Autoren",
    "title": "Joachim Schirrmacher",
    "text": " Table of Contents Joachim Schirrmacher Links Joachim Schirrmacher span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } Joachim ist seit 2017 bei der DB Systel als Berater und Entwickler. Am liebsten programmiert er funktional mit TypeScript, notfalls aber auch mit Java und Spring Boot, mag REST APIs, aber auch Event Sourcing und Streams. Links LinkedIn Profile GitHub Profile Mastodon Stack Overflow "
},

{
    "id": 46,
    "uri": "blog/profiles/Carsten-Thurau.html",
    "menu": "Autoren",
    "title": "Carsten Thurau",
    "text": " Table of Contents Carsten Thurau Carsten Thurau span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } "
},

{
    "id": 47,
    "uri": "blog/profiles/Carsten-Hoffmann.html",
    "menu": "Autoren",
    "title": "Carsten Hoffmann",
    "text": " Table of Contents Carsten Hoffmann Links Carsten Hoffmann span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } Carsten is an experienced software architect at DB Systel GmbH which is the digital partner of the biggest German railway company Deutsche Bahn . As a member of a DevOps-Team he is a strong believer, that an architect should be close to the development team in order to create and evolve a software architecture, that meets the business needs. He is a conference speaker and held talks at internal and external conferences, like the DB TechCon and the IT-Tage . He is an Open-Source enthusiast and maintainer of the Trivy Vulnerability Explorer . Links LinkedIn Profile Mastodon Profile GitHub Profile "
},

{
    "id": 48,
    "uri": "blog/profiles/buildIT.html",
    "menu": "Autoren",
    "title": "BuildIT",
    "text": " Table of Contents BuildIT BuildIT span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } "
},

{
    "id": 49,
    "uri": "blog/profiles/Philippe-Rieffe.html",
    "menu": "Autoren",
    "title": "Philippe Rieffe",
    "text": " Table of Contents Philippe Rieffe Philippe Rieffe span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } "
},

{
    "id": 50,
    "uri": "blog/profiles/Konrad-Winkler.html",
    "menu": "Autoren",
    "title": "Konrad Winkler",
    "text": " Table of Contents Konrad Winkler Konrad Winkler span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } "
},

{
    "id": 51,
    "uri": "lunrjsindex.html",
    "menu": "null",
    "title": "null",
    "text": " will be replaced by the index "
},

];
