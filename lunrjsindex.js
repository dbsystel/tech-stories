var documents = [

{
    "id": 0,
    "uri": "search.html",
    "menu": "-",
    "title": "Search",
    "text": " lokale Suche https://codersblock.com/blog/mini-previews-for-links/ div#searchresults h3 { font-size: larger; margin-top: 0 !important; } div#searchresults h4 { font-size: large; } div#searchresults span { font-size: small; } div#searchresults span.menu { font-size: medium; margin-top: 1em; } function dosearch(element) { if (element.value.length>=3) { var searchresults = document.querySelector('#searchresults'); out = \"\"; var results = idx.search(element.value); if (results.length == 0) { results = idx.search(element.value+\"*\"); } if (results.length == 0) { results = idx.search(\"*\"+element.value+\"*\"); } if (results.length == 0) { results = idx.search(element.value+\"~1\"); } var lastMenu = \"\" var lastTitle = \"\" results.forEach(function (item) { var doc = documents[item.ref]; out += \" \"+doc.menu+\" \"; if (doc.menu != lastMenu) { lastMenu = doc.menu; } out += \" \" + doc.title + \" \"; for(var field in item.matchData.metadata) { console.log(field); var matches = item.matchData.metadata[field] if (matches['text']) { matches['text']['position'].forEach(function (pos) { var subtext = doc.text.substring(pos[0]-50,pos[0]+pos[1]+50); if (pos[0]>0) { subtext = subtext.replace(new RegExp(/^[^ ]*/,\"i\"),\"...\"); } subtext = subtext.replace(new RegExp(/[^ ]*$/,\"i\"),\"...\"); var re = new RegExp(field,\"gi\"); subtext = subtext.replace(re,\" $& \"); out += \" \" + subtext + \" \"; }) } } searchresults.innerHTML = out; }) } } var input = document.querySelector(\"#lunrsrc\"); input.focus(); var params = new URLSearchParams(window.location.search); input.value = params.get('q'); dosearch(input); "
},

{
    "id": 1,
    "uri": "blog/2024/2024-11-18-Gestaltung_digitaler_Loesungen.html",
    "menu": "Blog",
    "title": "Gestaltung digitaler Lösungen",
    "text": " Table of Contents Gestaltung digitaler Lösungen – Produktions- oder Designprozess? Slides Gestaltung digitaler Lösungen – Produktions- oder Designprozess? Mein beruflicher Weg begann in den frühen 90er Jahren als Softwareentwickler, parallel zur zunehmenden Verbreitung des Internets. Diese technologische Revolution veränderte nicht nur die Art und Weise, wie Software entwickelt wurde, sondern führte auch zur Entstehung agiler Methoden und zu einer tiefgreifenden Transformation von Geschäftsprozessen und Organisationsstrukturen. In diesem Vortrag wird die Diskrepanz zwischen dem traditionellen Produktionsprozessansatz und einem designorientierten Ansatz in einer DevOps-Welt beleuchtet. Was auf den ersten Blick als akademische Frage anmutet, kann in der konkreten Unternehmenspraxis einen großen Unterschied machen, da Designprozesse grundsätzlich anders strukturiert werden als Produktionsprozesse. Durch Beispiele wird verdeutlicht, wie der holistische Ansatz des Digital Solution Design, Anwenderbedürfnisse, Geschäftsanforderungen und IT-Technologie gleichermaßen berücksichtigt, um zu erfolgreichen digitalen Lösungen zu gelangen. Key Takeaways: Wie wirkt der Ansatz des Digital Solution Design auf bestehende DevOps-Prozesse und kann gleichzeitig die Qualität der digitalen Lösung erhöhen? Welche konkreten Schritte können Unternehmen angehen, um eine ganzheitliche Betrachtung von Anwenderbedürfnissen, Geschäftsanforderungen und IT-Technologien in ihren DevOps-Praktiken zu implementieren? Welche Auswirkungen hat die Einführung eines designorientierten Ansatzes auf die Organisationsstruktur, die Arbeitsweise der Teams und die Kultur innerhalb eines Unternehmens, insbesondere im Hinblick auf die Verbindung von Anwenderbusiness und IT? Slides "
},

{
    "id": 2,
    "uri": "blog/2024/2024-09-10-GenAI.html",
    "menu": "Blog",
    "title": "AI-gile Zeiten",
    "text": " Table of Contents AI-gile Zeiten: Wie generative KI die IT-Welt neu gestaltet Slides AI-gile Zeiten: Wie generative KI die IT-Welt neu gestaltet Mein beruflicher Weg begann in den frühen 90er Jahren als Softwareentwickler, parallel zur zunehmenden Verbreitung des Internets. Diese technologische Revolution veränderte nicht nur die Art und Weise, wie Software entwickelt wurde, sondern führte auch zur Entstehung agiler Methoden und zu einer tiefgreifenden Transformation von Geschäftsprozessen und Organisationsstrukturen. Heute stehen wir erneut an einem Wendepunkt, diesmal durch die Einführung generativer Künstlicher Intelligenz (KI). Genau wie das Internet die agile Bewegung befeuerte, wird auch die generative KI die Softwareentwicklung und die IT-Arbeitswelt grundlegend verändern. In meinem Vortrag werde ich aufzeigen, wie diese Technologie nicht nur die Entwicklung selbst automatisiert, sondern meines Erachtens auch die Zusammenarbeit zwischen IT und Fachbereichen, sowie die Struktur von Organisationen, maßgeblich beeinflussen wird. Basierend auf meiner persönlichen Erfahrung, die von der Softwareentwicklung bis hin zur Organisations- und Unternehmensentwicklung reicht, möchte ich aufzeigen, wie sich die Rolle der IT in Unternehmen verändern könnte. Der Vortrag zielt darauf ab, eine spannende Diskussion darüber anzuregen, wie wir uns auf diese Revolution vorbereiten können und welche Chancen und Herausforderungen uns erwarten. Slides "
},

{
    "id": 3,
    "uri": "blog/2024/2024-07-02-Angular-Push-Notifications.html",
    "menu": "Blog",
    "title": "Supercharge Your Angular PWA with Push Notifications: From Setup to Debugging",
    "text": " Table of Contents Supercharge Your Angular PWA with Push Notifications: From Setup to Debugging High Level Overview Set up a simple demo app Test notifications without a backend Create a simple Messaging Server Send Notifications via a backend Pitfalls &amp; Limitations Next Steps Resources Supercharge Your Angular PWA with Push Notifications: From Setup to Debugging In modern web applications, push notifications have become an essential feature for engaging users. Service workers are crucial in enabling this functionality by running scripts in the background, independent of a web page. This guide will walk you through setting up a service worker with push notifications in Angular, including testing, verifying, debugging, and avoiding common pitfalls. Registering a service worker and setting up push notifications can be very cumbersome. You need to create and register a service worker script in your JavaScript code using the navigator.serviceWorker.register method. This script handles events by listening for the push event and displaying notifications using the self.registration.showNotification method. Additionally, you need to manually handle requesting user permission for push notifications, typically using the Notification.requestPermission method, and manage push subscriptions with the Push API , including handling VAPID keys for authorization. This requires quite a lot of boilerplate code and direct management of the service worker lifecycle and push events. And last but not least: You probably need to find a way to communicate/connect the service worker with your Angular app, for example, to handle a direct jump-in from a push notification and passing over data. Luckily, the Angular Framework comes with built-in support for all this. But let&#8217;s start step-by-step by setting up a very simple Angular demo app. High Level Overview Sending a notification to an application sounds easy but&#8230;&#8203; We need a secure client/server communication to deliver publish messages and subscribe to notifications. The server needs to find the specific browser to deliver the notifications reliably. We must integrate with the native notifications of our operating system. To receive messages even when a browser or tab is closed, we need a background service (Service Worker) listening to incoming notifications. The following sequence diagram gives you a high-level overview of the general flow we will establish in the next sections. Set up a simple demo app We are creating a blank Angular app by using the Angular CLI. After creation, we can directly add @angular/pwa by using its provided Angular Schematic . It will turn our app into a Progressive Web App (PWA) and add all necessary features including a service worker. Let&#8217;s name our app \"Push Parrot\" since we use it to demonstrate push notifications and it should repeat every input like a parrot. ng new push-parrot cd push-parrot ng add @angular/pwa Now we have a working web application that can be installed as a Progressive Web App. Its prepared with a Web app manifest , and it registers a service worker configured with cached resources . The created file ngsw-config.json is used by Angular for this basic service worker configuration. The service worker Angular registers for us is also linked to some services we can consume now in our app to check for new versions or handle push notifications (which we will do in a sec). Alright, lets start the app: npm start As we can see, we are able to install the app (Icon in Browser URL bar). Checking out the Google Chrome Developer Tools, we can see in the \"Application\" tab under \"Service Worker\": No service worker. Wait&#8230;&#8203; No service worker? Shouldn&#8217;t we see a service worker here? Yes, you are right, but let&#8217;s have a look into our app.config.ts file. Here we can see that the enabled option is only activated in production mode by default. So in conclusion, the service worker will not be registered in dev mode. We can change this line to be always true for our demo. // ... export const appConfig: ApplicationConfig = { providers: [ // ... provideServiceWorker('ngsw-worker.js', { - enabled: !isDevMode(), + enabled: true, registrationStrategy: 'registerWhenStable:30000' })] }; A second thing we have to do, to make service workers work locally in dev mode is that we need to tell Angular about the service worker config file. Therefore, we copy the line for the config file from the production config to the development config: { // ... \"projects\": { \"push-parrot\": { // ... \"architect\": { \"build\": { // ... \"configurations\": { \"production\": { // ... \"serviceWorker\": \"ngsw-config.json\" }, \"development\": { // ... + \"serviceWorker\": \"ngsw-config.json\" } }, // ... }, //... } } } } Alright, if we have another look now, we can see the service worker is successfully registered. Test notifications without a backend Okay, let&#8217;s make our parrot talk. We want to simply output all received push messages in the UI. Create a service We start by creating a simple service which handles all the notification stuff for us. ng g s web-notification The service should have the responsibility to tell us whether push messaging is activated or not. Also, it should inform us about the latest message received. We are injecting the SwPush instance which is an abstraction for the interaction with the underlying service worker and uses the ngsw-config.json . We want to consume the messages as a signal. Therefore, we can use the toSignal function from the rxjs-interop package provided by Angular. This will update the signal messages every time the Observable swPush.messages emits a new value. The last thing we need is a simple getter isEnabled which basically passes through the information if service is enabled and supported by the browser from the private #swPush instance. import { inject, Injectable } from \"@angular/core\"; import { SwPush } from \"@angular/service-worker\"; import { toSignal } from \"@angular/core/rxjs-interop\"; @Injectable({ providedIn: 'root' }) export class WebNotificationService { #swPush = inject(SwPush) messages = toSignal(this.#swPush.messages) get isEnabled() { return this.#swPush.isEnabled; } } Use the service Great, let&#8217;s consume our service in the AppComponent . We want to create a Signal permission that represents the current state of Notifications (can be \"default\", \"denied\" or \"granted\"). Later on, we are updating the signal, but one step at a time. The last thing we need here is the JsonPipe . We need to include it, to display JSON data we received as a notification directly in the UI. import { Component, inject, signal } from \"@angular/core\"; import { WebNotificationService } from \"./web-notification.service\"; import { JsonPipe } from \"@angular/common\"; @Component({ /* ... */ imports: [JsonPipe] }) export class AppComponent { permission = signal&lt;NotificationPermission&gt;(\"default\"); notificationService = inject(WebNotificationService) constructor() { if (this.notificationService.isEnabled) { this.permission.set(Notification.permission) } } } Now we need to output our results in the template. First, we get rid of all the current demo code created by the Angular CLI. Next, we add a simple template to print the current permission state and show the received messages. You can add some styles to make it look good, but it&#8217;s out-of-scope of this article. At the end of this article, you will find a link to the demo code with all the styles I added. &lt;h1&gt;Push Parrot&lt;/h1&gt; &lt;div type=\"button\" (click)=\"subscribe()\" [class.green]=\"permission() === 'granted'\" [class.red]=\"permission() === 'denied'\" &gt;Notifications: ({{ permission() }})&lt;/div&gt; @if(notificationService.messages(); as messages) { &lt;pre&gt;{{ messages | json }}&lt;/pre&gt; } So far so good. But how can we test it if we don&#8217;t have a backend connected? Test with Chrome Developer Tools Luckily the Chrome Developer Tools are our friend. We can directly send messages to our connected service worker using the \"Push\" button. Let&#8217;s try it out and see what happens. Enter the following JSON input here and press \"Push\": {\"notification\":\"Hello little Parrot!\"} Awesome, now we should see the message in our UI. But wait? This wasn&#8217;t a push message which should appear as a native message in our operating system right? We simply display the data of the last received message here. And this totally makes sense since we haven&#8217;t granted any permissions that allow us to be notified in case of new messages. The permission signal is still in the \"default\" state. which means permissions have neither being granted nor denied. Start recording notifications and push messaging Before we start the implementation of this step, let&#8217;s use our developer tools to investigate a few more details. In the \"Application\" tab of the DevTools, we have a section for background services with the label \"Notifications\". Let&#8217;s have a look at it. This neat little feature allows us to record any received push messages for further inspection. We can simply start the recording for notifications received from now on. The cool thing is: It even records the notifications of delivered messages when the current tab is in background or even when the browser is closed but the service worker still receives messages. With this you can inspect messages you expect to have received, when your application is closed. The counterpart to the received notifications is the section \"Push Messaging\". This works quite similar and we can start recording from now on. In comparison to the \"Notifications\" view, here we can see all push messaging activity even if users haven&#8217;t granted permission for notification. This helps us to inspect scenarios where we received messages in general but did not get notified. Let&#8217;s try it out and start recording both: \"Notifications\" and \"Push Messaging\". We switch to the \"Service Workers\" section again and send our JSON input a second time. Now we should not see any new entry in the section for \"Notifications\" because we haven&#8217;t granted it permission. But checking the section \"Push Messaging\" gives us the insight, that the messaging works in general and that an event was dispatched and completed. Create a simple Messaging Server What we need to actually deliver real push messages is a little server. The server needs to register each subscriber and is responsible for sending messages to them. In the following example I am using a simple nodejs server written in JavaScript. I put this demo server right next to my angular app. Basic Server First things first, we need to install some dependencies for our server: npm i express cors body-parser web-push Let&#8217;s start by creating a new file src/server/index.js with a simple express.js skeleton. We need to enable Cross-Origin Resource Sharing (CORS) since we want to get access from another origin ( http://localhost:4200 ). import express from \"express\"; import bodyParser from \"body-parser\"; import cors from \"cors\"; const app = express(); const port = 3000; app.use(cors()); app.use(bodyParser.json()); app.listen(port, () =&gt; { console.log(`Server started at http://localhost:${port}`); }); VAPID Keys To securely send notifications to a browser, we use VAPID (Voluntary Application Server Identification for Web Push) keys. VAPID keys are a pair of public and private keys used to identify the server and ensure the authenticity of notifications. We use the web-push library to generates these keys. The public key can be used by an application when setting up notifications. It will be checked by the server to ensure our connected frontends are allowed to make use of the notifications. But let&#8217;s start by creating a simple endpoint that sends the public key to a client when requested. /* ... */ import webPush from \"web-push\"; /* ... */ const vapidKeys = webPush.generateVAPIDKeys(); app.get('/vapidPublicKey', (req, res) =&gt; { res.send(vapidKeys.publicKey); }); /* ... */ Now we can start the server and check if the endpoint sends us a public key once started. &gt; node src/server/index.mjs Server started at http://localhost:3000 &gt; curl http://localhost:3000/vapidPublicKey BALZVv0uBWpP9ttSJFCid0VB7x99e4oLkbsamrit5CzKvZQEwyQ_YsK95YEo418OBhfQqcS8XsYS6KSpuLCAdNA% Subscribe to messages Clients can now use the public key, to create subscriptions in the browser. To be able to send messages to the subscribers, the server needs to know about these subscriptions. Therefore, we create another endpoint, that receives the subscription data and stores it internally. With this, the backend now has the information about connected clients / push subscriptions and we are able to send data to this subscribers. /* ... */ const subscriptions = []; /* ... */ app.post('/notifications/subscribe', (req, res) =&gt; { const subscription = req.body; subscriptions.push(subscription); res.status(201).json({}); }); /* ... */ Send messages First we must configure web-push with the contact email, public key and private key. This function is crucial for configuring the push notifications. It ensures that notifications are trusted and can be decrypted by the user&#8217;s browser. The last step is to handle incoming messages for subscribers. We iterate over all subscribers and call sendNotification . We send the title and description we received as the POST body as JSON. Last but not least, we iterate over the returning promises using Promise.all() and send a response to the requester. /* ... */ const subscriptions = []; webPush.setVapidDetails( 'mailto:mail@example.org', vapidKeys.publicKey, vapidKeys.privateKey ); /* ... */ app.post('/notifications/send', (req, res) =&gt; { const promises = subscriptions.map(sub =&gt; webPush.sendNotification(sub, JSON.stringify({ notification: { title: req.body.title, body: req.body.description } })) ); Promise.all(promises) .then(() =&gt; res.status(200).json({ message: 'Notification sent successfully.' })) .catch(err =&gt; { console.error('Error sending notification, reason: ', err); res.sendStatus(500); }); }); /* ... */ Great, our server code should be good enough for our purpose now. Let&#8217;s continue by connecting a frontend. Send Notifications via a backend As always we start by adjusting our service. Lets add three new methods that will fetch / post data from / to our backend. The private methods #vapidPublicKey and #registerOnServer should be used in the next step for setting up the push subscription. The method #vapidPublicKey will simply retrieve the public key from our backend whereas #registerOnServer posts the subscription params to it. The third method sendMessage should be public since we want to use it in our component. This service should receive a title and a description and sends this data as JSON body to the server&#8217;s responsible endpoint. /* ... */ import { HttpClient } from \"@angular/common/http\"; @Injectable({ providedIn: 'root' }) export class WebNotificationService { /* ... */ #http = inject(HttpClient) #baseUrl = 'http://localhost:3000' /* ... */ #vapidPublicKey() { return this.#http.get( `${this.#baseUrl}/vapidPublicKey`, { responseType: 'text' } ) } #registerOnServer(params: PushSubscription) { return this.#http.post( `${this.#baseUrl}/notifications/subscribe`, params ); } sendMessage(title: string, description: string) { return this.#http.post( `${this.#baseUrl}/notifications/send`, { title, description } ); } } Now, we introduce another public method in the service called requestSubscription . This method should be called from the component to setup the subscription. It will use the Observable with the public VAPID key. We use the switchMap operator in the pipe to switch to another observable created out of a promise using the from creator function. Within which we call the swPush instance and request to subscribe using the public key. The next step in the pipe is to use concatMap for sending the subscription data via the private method #registerOnServer . Last but not least, we catch any errors, log them and return an empty Observable. /* ... */ import { catchError, concatMap, EMPTY, from, switchMap } from \"rxjs\"; @Injectable({ providedIn: 'root' }) export class WebNotificationService { /* ... */ requestSubscription() { return this.#vapidPublicKey().pipe( switchMap(key =&gt; from(this.#swPush.requestSubscription({ serverPublicKey: key })) ), concatMap(sub =&gt; this.#registerOnServer(sub)), catchError((e) =&gt; { console.log(e) return EMPTY }) ) } } Now we need to adjust our component. We want to create a small form using Angulars reactive forms. Therefore we need to import the ReactiveFormsModule to use it in the template in the next step. We initialize a simple form with two inputs: title and description . Next, we create a subscribe method, that&#8217;s triggered by a button to activate push notifications and subscribe to new messages on the server. In the subscription we set the permission signal to the current value of Notification.permission . With this, we can see in our UI, if the current permission state is \"default\", \"denied\" or \"granted\". The last method we need is the submit method to send the current form data via the service to our backend which will push the entered message to all subscribers. We also need to subscribe to this call in order to start the HTTP call as it only sends notifications if a subscriber is registered. /* ... */ import { FormControl, FormGroup, ReactiveFormsModule } from \"@angular/forms\"; @Component({ /* ... */ imports: [JsonPipe, ReactiveFormsModule], }) export class AppComponent { notificationForm = new FormGroup({ title: new FormControl(''), description: new FormControl(''), }); /* ... */ subscribe() { this.notificationService.requestSubscription().subscribe(() =&gt; { this.permission.set(Notification.permission) }); } submit() { this.notificationService.sendMessage( this.notificationForm.value.title || '', this.notificationForm.value.description || '' ).subscribe() } } Okay, the logic is ready so far, let&#8217;s adjust the template of our component to actually implement and show the subscribe button and the formular. The first button triggers the subscription and under the hood the process of asking the user for the permissions to show notifications. The form must be connected with the notificationForm form group. With the combination of the button button[type=\"submit\"] and the output handler for ngSubmit we start sending the form data to the subscribers. &lt;h1&gt;Push Parrot&lt;/h1&gt; &lt;button type=\"button\" (click)=\"subscribe()\" [class.green]=\"permission() === 'granted'\" [class.red]=\"permission() === 'denied'\" &gt;Notifications: ({{ permission() }})&lt;/button&gt; &lt;form [formGroup]=\"notificationForm\" (ngSubmit)=\"submit()\"&gt; &lt;label for=\"title\"&gt;Title&lt;/label&gt; &lt;input id=\"title\" placeholder=\"Please enter a title\" type=\"text\" formControlName=\"title\"&gt; &lt;label for=\"description\"&gt;Description&lt;/label&gt; &lt;input id=\"description\" placeholder=\"Please enter a description\" type=\"text\" formControlName=\"description\"&gt; &lt;button type=\"submit\"&gt;Send&lt;/button&gt; &lt;/form&gt; @if(notificationService.messages(); as messages) { &lt;pre&gt;{{ messages | json }}&lt;/pre&gt; } Done. Let&#8217;s try it out! Please be aware, you need to re-activate the permissions every time the backend is restarted since a new VAPID keypair is generated. Once you press the notifications button, your browser should ask you to grant permissions. Now your local app ist registered to receive push notifications from the backend and you are registered as a subscriber for new messages. We can use our UI and the formular to send a message to the servers which is echoed by our parrot in the UI as well as a push notification sent by the backend to our subscribed frontend. Once we submit the form, we also see the message in the developer tools in the \"Notifications\" section. The cool thing is: this will even work if we close the tab / browser and receive messages. So it helps us to inspect potentially missed messages. Pitfalls &amp; Limitations Push messages are a great way to notify users about important information. This helps us to keep users informed but on the other hand they can be very noisy and distracting if overused. So please always think twice if a message is really necessary and helpful. Also, there are some common pitfalls / limitations where you might wonder why you haven&#8217;t received a message. So always ensure the following things when testing / debuggung such situations: Is the messaging server up and running? Were permissions granted in the browser? Is you system in \"Don-not-disturb\" mode? Have you checked the DevTools for the Push Messaging and Notification settings and verified that you should have received the message? Does your application code filter out any messages? Is your application served over HTTPS (or on localhost)? A service workers requires a secure context. Clear your browser cache and unregister the service worker manually in DevTools if updates are not being reflecting. Make sure to handle cases where users deny notification permissions and provide a way to request permissions again. Next Steps Great, you managed to setup a very simple frontend and backend for creating and a delivering push messages to all subscribers. The setup is currently very basic and only processes some text information. Push messages can be configured in more detail which is out-of-scope of this article but stay tuned! I will publish a followup article where we will configure messages a bit more and learn how to react to message clicks etc. Resources Github Demo Project: Push Parrot "
},

{
    "id": 4,
    "uri": "blog/2024/2024-04-25-JAX-Inner-Source.html",
    "menu": "Blog",
    "title": "Inner Source@DB",
    "text": " Table of Contents Inner Source@DB: Eine Geschichte über Open-Source-Praktiken im DB Konzern Slides Links Inner Source@DB: Eine Geschichte über Open-Source-Praktiken im DB Konzern Entdecken Sie mit mir die Welt von Inner Source bei der Deutschen Bahn (DB) während meines Vortrags. Ich werde mit einem Rückblick auf die Geschichte von Inner Source bei der DB beginnen und dabei die einzigartigen Herausforderungen beleuchten, auf die wir gestoßen sind, sowie die Lösungsansätze, die wir entwickelt haben. Erfahren Sie, wie die Gründung eines Thementeams in unserer Architekturgilde dazu beigetragen hat, die Verbreitung von Inner Source zu fördern und Hindernisse zu überwinden. Ich werde den Nutzen von Inner Source für das Unternehmen und die Softwareentwicklung erläutern und den Unterschied zwischen \"Community-maintained\" und \"Team-maintained\" Inner Source erklären. Dabei zeige ich auf, warum eine zentrale Plattform für die Entstehung von Community-maintained Inner-Source-Projekten unerlässlich ist und welche Eigenschaften wir dieser Plattform gegeben haben. Abschließend werde ich enthüllen, wie Inner Source eine Brücke zu Open Source schlägt und einige konkrete Inner-Source-Projekte vorstellen. Seien Sie gespannt und lassen Sie sich inspirieren, wie Inner Source die Zukunft der Softwareentwicklung bei der Deutschen Bahn vorantreibt! Slides Links Open-Source-Manifest der Deutschen Bahn "
},

{
    "id": 5,
    "uri": "blog/2024/2024-04-04-DB-Systel-KubeCon.html",
    "menu": "Blog",
    "title": "DB Systel auf der KubeCon 2024 in Paris",
    "text": " Table of Contents DB Systel auf der KubeCon 2024 in Paris Slides &amp; Video Links DB Systel auf der KubeCon 2024 in Paris Die Tech-Expert:innen der DB Systel treiben nicht nur die Digitalisierung der DB voran, sie sind mit ihrem umfassenden Knowhow auch gefragte Speaker:innen bei internationalen Kongressen wie der KubeCon. Vom 19. bis 22. März fand die größte Anwender- und Entwicklerkonferenz für Software aus dem Kubernetes- und Cloud-Native-Universum in Paris statt. Über 12.000 Besucher:innen machten sich auf den Weg in die Hauptstadt Frankreichs, um ein buntes Programm mit über 10 Tracks, Keynote-Livestreamings, Tutorials und Networking-Events zu erleben. Unser Kollege Gualter Baptista konnte dort im Rahmen seiner Keynote „Building IT Green: A Journey of Platforms, Data, and Developer Empowerment at Deutsche Bahn” Einblicke geben, mit welchen Ansätzen die DB Systel versucht, den ökologischen Fußabdruck der DB-Cloud-Anwendungen zu überwachen und kontinuierlich zu minimieren. Die komplette Keynote zum Nachschauen gibt es hier: Slides &amp; Video KI Summary Links LinkedIn-Post Gualter Baptista How a platform focus helped Deutsche Bahn "
},

{
    "id": 6,
    "uri": "blog/2024/2024-04-02-Journey-from-TypeScript-to-Java-part-1.html",
    "menu": "Blog",
    "title": "My journey from TypeScript to Java - Part 1",
    "text": " Table of Contents My journey from TypeScript to Java - Part 1 No some objects are equal Fun with null values Why the hell do I need to specify types? Union types Combine object structures My journey from TypeScript to Java - Part 1 For several years now, I&#8217;ve been programming in Typescript and enjoyed it significantly due to its simple and yet powerful type safety system in contrast to JavaScript. However, a lot of people in DB Systel use Java as their preferred programming language. To understand what they like about Java, but also because it is easier to maintain the software in the team, I wanted to learn Java myself. My learnings on my way from TypeScript to Java might be useful for others going the same way. While I&#8217;m going this way, I like to add more of these articles. For now, I start with some beginner problems, but I already have prepared some more! Part 1 No some objects are equal Fun with null values Why the hell do I need to specify types? Part 2 (planned) Optional values and defaults Objectifying mappings part 3 (planned) Enum-erable charm Table manners for databases Spice up your functions with curry Many thanks to my friends who helped me with the formulation of my travelogue and showed me new ways: Carsten Hoffmann Caroline Rieseler Jasper Gerigk Danny Koppenhagen Ralf D. Müller No some objects are equal The first thing I stumbled upon might be a real beginner problem. I expected that I could compare strings very similar to TypeScript with the equality operator. Sure, the === operator in TypeScript is very special, but the IDE stops you from using it in Java. However, it doesn&#8217;t stops you to use the 'normal' == equality operator for strings. Since String is a class, and classes are only equal, if they are the same object, equality on two Strings normally doesn&#8217;t work. You might already know this behavior from TypeScript when, e.g. comparing `Date`s. To make things more complicated, there are situations where comparisons work nevertheless: String param = \"expected\"; if (param == \"expected\") { System.out.println(\"These strings are equal!\") } This happens because Java re-uses string literals like 'expected' in this example. So, the param object and the \"expected\" string literal are actually equal! However, if the String is created dynamically (e.g. from user input), it doesn&#8217;t: String param = \" expected \".trim(); if (param != \"expected\") { System.out.println(\"These strings are not equal, even if they have the same content!\") } In Java, comparing of objects is done with an equals() method: if (param.equals(\"expected\")) { System.out.println(\"Now the equality is recognized\") } However, on primitive types, equality is implemented just like I would expect. So a == 5 works if a is an int . And, surprisingly, if you see a variable defined as Integer a , you can use a == 5 , though Integer is a class and a therefore is an object of this class. Actually, the == operator defined here overloads the standard one, so the 'normal' rule of non-equality of objects if they are not the same is not applicable. Try it out yourself: class Test { public static void main(String[] args) { Integer a = Integer.parseInt(args[0]); if (a == 5) { System.out.println(\"a is 5\"); } else { System.out.println(\"a is not 5\"); } } } In my opinion, this is not very intuitive, but that might be due to my past in TypeScript. Regrettably, Java implements function overloading only in a few cases (like the above) and doesn&#8217;t give us the opportunity to define them ourselves. Else, I would create a MyString class and define a == method to compare strings like numbers. Fun with null values Typescript inherits the sometimes difficult distinction between undefined and null from Javascript. Some people might think, this is not really necessary, but the two actually have a slightly different meaning: while null means that I actually assigned a kind of empty value to a variable or parameter, undefined means 'not set'. null is rather empty on purpose. Java doesn&#8217;t make such a distinction, it only has null - which might be sufficient for most use cases. But how does both languages handle such values? Let&#8217;s say, you have a variable myObject which might be an object having a function value() but also may be undefined . The function might return a Date or again undefined . How can I compare this with the current time? In Typescript this can be done with the so called 'Optional Chaining' like this: if (myObject?.value()?.getTime() &gt; Date.now()) { // ... } The ? means that when we get an undefined , just stop evaluating, returning undefined as the result of the expression. TypeScript programmers know this as 'falsy', which means that it something that is similar to the implementation of false . Falsy in TypeScript are the values 0 , undefined , null , \"\" (the empty string), and, of course, false . This is a very compact and, if you understand the syntax, readable form. In Java, there is no such thing as a optional chaining ?. operator, we have to check every possible null value. Also, Java uses different types for numbers, int or long , so we need to decide, what value() should return. If we want to use null as well, both types won&#8217;t allow that. But there is an alternative: instead of using the native int or long types, use the classes Integer or Long (with capital first letter) for the return value of value() instead! They may be null (as all objects). if (myObject != null &amp;&amp; myObject.value() != null &amp;&amp; myObject.value() &gt; Long.valueOf(Instant.now().getEpochSecond())) { // ... } Why the hell do I need to specify types? As you saw in my previous examples, Java requires you to specify the data type when defining a variable. While in Typescript, a definition with a value looks like const string = functionThatReturnsAString(); in Java, it requires an additional specification String string = functionThatReturnsAString(); This gets particularly strange, if you have a value that needs to be explicitly converted to a string: String string = functionThatReturnsAnObject().toString(); When the function is already specifying a return type, specifying the type of the variable seems to be just overhead. The compiler could just infer the type automatically! But to improve the situation in Java 10 and higher, instead of the data type, one can use the var keyword (which is rather a reserved type name, to be exact), so that it looks similar to TypeScript: final var string = functionThatReturnsAnObject().toString(); The Java compiler then also automatically infers the actual type. In fact, TypeScript also has a var keyword, though I never would use it, and instead only use const or - in rare cases - let . Differentiating between variables that can change later ( let ) and those which may not be changed ( const - an immutable value) is a very useful feature to avoid unwanted changes. Union types A nice feature of TypeScript are Union Types. They allow to combine multiple types to be used in a clear way: type Fruit = \"apple\" | \"orange\" | \"banana\"; type DairyProducts = \"milk\" | \"butter\"; type Food = Fruit | DairyProducts; There is no such thing as Union Types in Java. In Java, you would instead use enum`s for defining `Fruit and DairyProducts . @AllArgsConstructor public enum Fruit { APPLE(\"apple\"), ORANGE(\"orange\"), BANANA(\"banana\"); String name; } The upper case identifiers work as constants, the string literals in the braces the values of these constants. The field name is needed to hold the value in each of the three instances of Fruit . Fruit values can be accessed like this: Fruit fruit = Fruit.BANANA; However, it is not easily possible to combine such enums, because they are compiled to constants. Instead, you would use interfaces and let the Fruit and the DairyProduct enums implement this interface. public interface Food {} @AllArgsConstructor @Getter public enum Fruit implements Food { APPLE(\"apple\"), ORANGE(\"orange\"), BANANA(\"banana\"); String name; } @AllArgsConstructor @Getter public enum DairyProduct implements Food { MILK(\"milk\"), CHEESE(\"cheese\"); String name; } Now you can use the new Food interface as the type to collect both, `Fruit`s and `DairyProduct`s together: List&lt;Food&gt; food = List.of(Fruit.BANANA, DairyProduct.CHEESE); Note that it is not possible (as far as I know) to use Food as the qualifier for BANANA and CHEESE and that it is not that easy to use the lower case equivalents for assignments. Instead, one would need to iterate over the enum values and find the requested value text. I use Lombok to not have to implement getters and constructors. They are necessary to have the lower case values at least when using the enum for writing to a database or when generating JSON. Also note that every interface and class needs a separate file in Java. All in all, `enum`s seems to be a very cumbersome feature. But this is also valid for TypeScript, where I prefer to use Union Types. Combine object structures To combine object structures in TypeScript you would use the &amp; : type Person = { name: string; email: string; }; type AuthDetails = { username: string; password: string; }; type User = Person &amp; AuthDetails; In Java, you would rather define two interfaces, and define a class implementing both. This would work in Typescript as well. However, interfaces in TypeScript can only be used to describe object strucures, not primitives. Read more about the differences of type aliases and interfaces in the official TypeScript documentation . I&#8217;m still working with Java, so stay tuned to read more experiences in the next chapters of my journey, when I will cover optional values, defaults and mapping of objects. "
},

{
    "id": 7,
    "uri": "blog/2024/2024-02-07-Ein-Jahr-PostgreSQL-statt-Oracle-Das-Leben-danach.html",
    "menu": "Blog",
    "title": "Ein Jahr PostgreSQL statt Oracle",
    "text": " Table of Contents Ein Jahr PostgreSQL statt Oracle – Das Leben danach Syntaktische Unterschiede Architekturunterschiede Performance Troubleshooting Fazit .bahn.ng code { color: var(--color-cool-gray-700); } Ein Jahr PostgreSQL statt Oracle – Das Leben danach Vor etwas über einem Jahr hatte ich die Möglichkeit tiefer in die Welt der PostgreSQL Datenbankentwicklung einzutauchen. Ein treibender Gedanke war: \"SQL ist SQL - ich werde mich schon schnell zurechtfinden, SQL bleibt schließlich eine Relationale Datenbank\". Doch der Teufel steckt bekanntlich im Detail: Die Unterschiede und Herausforderungen, welche wirklich auf einen Oracle-Datenbankentwickler warten, waren mir vorher nicht bewusst. Sowohl Oracle als auch PostgreSQL haben schöne Seiten. Es ist durchaus von Vorteil, beide Systeme ein wenig genauer zu kennen. Dieser Artikel gibt meine Erfahrungen und Eindrücke wieder, die ich nach einem Jahr im PostgreSQL Ökosystem gemacht habe. Syntaktische Unterschiede Beginnen wir einfach mit einem create table Befehl. In meinem Oracle-Modus lege ich Textspalten mit dem Datentypen varchar2 an. Wenn ich einen solchen Befehl in PostgreSQL absetze, dann erhalte ich die Fehlermeldung SQL Error [42704]: ERROR: type \"varchar2\" does not exist . Schon das weist darauf hin, dass man mit den Skripten nicht einfach in \"Copy und Paste\" Manier auf eine PostgreSQL Datenbank wechseln kann. Die korrekte Alternative ist in diesem Fall der Datentyp text (s. Listing 1). Listing 1: create table in PostgreSQL create table demo ( descr text , col text ); Entgegen der gängigen Praxis in Oracle, dass man die Zeichenketten-Länge möglichst einschränkt, ist das in PostgreSQL eher untypisch (vgl. [ 1 ] ). Möchte man anschließend die erstellte Tabelle im Data Dictionary überprüfen, stellt man den nächsten Unterschied fest: Die bewährte Struktur dba|all|user_tables gibt es so nicht. Stattdessen ist das Pendant im sogenannten information_schema Schema unter dem Namen pg_class zu finden. Objektnamen werden außerdem in Kleinbuchstaben hinterlegt (s. Listing 2). Listing 2: Data Dictionary Abfrage in PostgreSQL select * from pg_class where relname = 'demo' ; Wie man sieht, ist die analoge Spalte zu user_tables.table_name hier pg_class.relname . Mir hat es geholfen an die relationale Theorie von Edgar F. Codd zu denken, wo ebenfalls von tupels(=row), relations(=table) und co. die Rede ist. Der Sprachgebrauch ist im Data Dictionary von PostgreSQL wiederzuerkennen. Als nächstes soll die Tabelle Demo mittels Insert -Befehl befüllt werden. Zunächst werden dazu zwei Zeilen eingefügt – s. Listing 3. Listing 3: NULL vs. leere Zeichenkette insert into demo (descr, col) values ('1 insert', ''); insert into demo (descr, col) values ('2 insert', null); Anzumerken ist, dass die Spalte col im ersten Befehl mittels leerem String und im zweiten explizit mit NULL befüllt wird. In Oracle hat das dieselbe Bedeutung, aber in PostgreSQL ist mit dem leeren String etwas anderes gemeint, sodass eine Abfrage mittels where col is null nur eine Zeile zurückgibt. Das muss man sich mal auf der Zunge zergehen lassen. Ich möchte nicht wissen, wie viele Applikationen eine explizite NULL -Wert-Behandlung implementiert haben. Um beide Zeilen im select -Befehl zu erhalten, könnte man die where -Bedingung in where coalesce(col, '') = '' umschreiben. Damit ist es aber nicht getan. Man muss sich dann auch mit den möglichen Operationen (z. B. String Konkatenation – s. Listing 4) befassen. Listing 4: Operation mit Zeichenkette und NULL select 'test'||null; --ergibt NULL! In Listing 4 mag Ihnen aufgefallen sein, dass die Tabellenangabe from dual fehlt. Das ist in PostgreSQL valide Syntax – PostgreSQL kennt die Dummy Tabelle dual nicht. Doch zurück zum eigentlichen Thema: NULL -Werte. Wenn wir in Oracle einen NULL -Wert an eine Zeichenkette hängen, können wir sicher sein, dass die Zeichenkette wieder ausgegeben wird. Das ist in PostgreSQL nicht so. Der gesamte Ausdruck liefert NULL als Ergebnis. Tatsächlich bin ich mehrfach auf kleine Bugs in meinen Abfragen gestoßen, weil ich aus Gewohnheit falsche NULL -Logiken im Kopf hatte. Der Artikel von AWS \"Handle empty Strings when migrating from Oracle to PostgreSQL\" umfasst 13 Seiten und 3300 Worte. Das zeigt, wie komplex die Unterschiede zwischen beiden Datenbanken alleinig im Bereich NULL -Werte sind. Listing 5 verdeutlicht einen weiteren Unterschied (der sich in Oracle 23c in Luft auflösen wird): Der Boolean Datentyp existiert und kann in PostgreSQL ohne Einschränkungen in SQL verwendet werden. Listing 5: Booleans select '' = '' tst; -- liefert TRUE Bisher ist auf der Demo Tabelle kein Primärschlüssel definiert. Wenn dieser ergänzt und anschließend zwei identische Werte eingefügt werden (s. Listing 6), kommt es zu einer Fehlermeldung. Im Gegensatz zur generischen Fehlerbeschreibung in Oracle ( SQL Error [1] [23000]: ORA-00001: unique constraint (SYSTEM.xyz) violated ), enthält die Fehlerbeschreibung in PostgreSQL den tatsächlichen Spaltenwert, der zur Verletzung des Constraints geführt hat (im Beispiel ist das key1 ). Gerade bei Bulk-Loads macht das die Suche nach der Nadel im Heuhaufen deutlich einfacher. Ein kleiner, aber feiner Unterschied. Listing 6: Primärschlüssel-Verletzung in PostgreSQL alter table demo add primary key (descr); insert into demo values ('key1', 'i repeat'); insert into demo values ('key1', 'i repeat'); SQL Error [23505]: ERROR: duplicate key value violates SQL Detail: Key (descr)=(key1) already exists. Im Kontext von Bulk-Loads und Primärschlüssel-Verletzungen ist in Oracle der merge -Befehl hilfreich. Dieser ist seit 9i verfügbar und besitzt somit einen gewissen Reifegrad. In PostgreSQL wurde merge erst im Oktober 2022 mit Version 15 eingeführt. Damit ist der Befehl nicht ganz ausgereift und noch etwas limitiert. Genaueres ist dem Syntax Baum in der offiziellen Dokumentation zu entnehmen ( https://www.postgresql.org/docs/15/sql-merge.html ). Apropos Dokumentation. Sowohl Oracle als auch PostgreSQL weisen eine sehr gute Doku auf. PostgreSQL verzichtet dabei (absichtlich) auf Code-Beispiele, was manchmal weitere Google-Suchen erforderlich macht. Den Luxus, den Oracle-Base.com bietet, hat man für PostgreSQL nicht. Wer noch nicht PostgreSQL Version 15 verwendet, dem sei die kleine Schwester des merge -Befehls empfohlen (s. Listing 7). Hier lässt sich zumindest eine Update-Aktion (oder \"Fehler ignorieren\") bestimmen, wenn eine entsprechende Constraint-Verletzung vorliegt. Nicht ganz so mächtig wie der merge -Befehl, aber ich habe häufig darauf zurückgegriffen und fand ihn auch sehr selbsterklärend. Wirklich vermisst habe ich merge nicht. Listing 7: insert on conflict in PostgreSQL insert into demo values ('key2', 'i repeat') on conflict (descr) do nothing; insert into demo values ('key2', 'i repeat') on conflict (descr) --pk or uk do update set col = 'i''ve been updated'; Man ahnt es schon: An vielen Stellen wird man sich bei PostgreSQL an kleinere Syntax-Änderungen gewöhnen müssen, die mehr oder weniger einschränkende Funktionalität bedeuten. Sollte ich nach einem Jahr Vergleich die aktuell bedeutendste Einschränkung nennen, wären das die die Window-Functions. Diese sind zwar auch zahlreich in PostgreSQL vorhanden, allerdings hat mich Oracle in der Vergangenheit mit Details beeindruckt; z. B. wird ein select count(distinct …) over(partition by …) mit der Fehlermeldung SQL Error [0A000]: ERROR: DISTINCT is not implemented for window functions zurückgewiesen. Die Tabelle Demo war bis hierhin ein dankbarer Begleiter zum Aufzeigen der Unterschiede. An dieser Stelle möchte ich ein neues Beispiel einführen, um zu verdeutlichen, dass die Syntax-Differenzen zwischen Oracle und PostgreSQL an vielen Stellen deutlich spürbar sind. Man stelle sich folgenden Datensatz vor: Für eine Webanwendungen ist in einer Tabelle der jeweilige URL-Aufruf mit Dauer und Zeitstempel des Aufrufs getrackt. Ich habe mittels SQL zu Demonstrationszwecken einen solchen Datensatz künstlich erzeugt. Aus Platzgründen erkläre ich die Gegenüberstellung stichpunktartig und nenne die gravierendsten Unterschiede: Common Table Expression ( with Klausel in Kombination mit create table ) Generierung mehrerer Zeilen ( connect by level vs. generate_series ) Teilen von Strings ( substr vs. substring Funktion; siehe auch Parameter) Erzeugen von Zufallszahlen ( dbms Package vs. random() Funktion) Literale beim Erzeugen von Zeitintervallen ( interval '1 minute' vs. interval '1' minute . - Hochkommas beachten!) Figure 1. Erzeugen von Testdaten Oracle vs. PostgreSQL Syntax Bevor ich mich anderen Aspekten als den Syntax-Unterschieden widme, möchte ich noch erwähnen, dass auch syntaktisch gleiche Konstrukte zu unerwarteten Ergebnissen führen können: Ein select 5/2 ergibt in PostgreSQL 2 (Zwei). Das ist auch so dokumentiert (vgl. [ 2 ] ), allerdings finde ich den Default (\"for integral types, the devision truncates the result towards zero\") nach wie vor gewöhnungsbedürftig. Explizite Casts wie select 5.0/2 oder select 5::float/2 sind mögliche Lösungen, die dann zum Ergebnis 2,5 führen. Architekturunterschiede Im zweiten Teil möchte ich noch etwas ins Detail gehen und ein paar markante architektonische Entscheidungen in PostgreSQL beleuchten. Was eine Datenbank grundlegend ausmacht, ist das MVCC (Multi Version Concurrency Control) Prinzip, welches den konkurrierenden Zugriff auf Daten regelt. Wenn eine Transaktion liest, soll sie nicht von einer schreibenden Transaktion geblockt sein und umgekehrt. Hierzu ist es essenziell, dass Datenänderungen versioniert sind. Oracle ändert die Daten \"inline\" im Block. Es wird um jeden Preis vermieden, dass die physikalische Adresse einer Zeile im Block nochmal wandert. Das hat z. B. den Vorteil, dass ein Index stets unverändert den Zugriff auf den Block für diese Zeile kennt. Die eigentlichen Änderungen/Versionen sind im UNDO festgehalten. In PostgreSQL wird eine Zeile mehrfach kopiert und eine Transaktions-ID steuert, für welche Session die Zeile sichtbar bzw. unsichtbar ist. Je mehr parallele Transaktionen auf demselben Tupel (=Zeile) operieren, desto mehr Kopien derselben Zeile wird es geben. Das sorgt folglich für sogenannten Bloat: Die Tabelle plustert sich auf und verbraucht physikalisch mehr Speicherplatz. Wenn die älteren Versionen nicht mehr benötigt werden, bleiben überflüssige Versionen einer Zeile übrig. Ein Hintergrundprozess muss aktiv werden, um aufzuräumen. Dieser ist unter dem Begriff Vacuum (\"Staubsauger\") bekannt. Ich selbst musste diesen Prozess nie tunen. In OLTP-Systemen mit viel Last sollte man sich jedoch genauere Gedanken machen, wann und wie dieser Hintergrundprozess aktiv werden soll/muss. Hierzu werden in der zentralen Konfigurationsdatei postgresql.conf diverse Parameter angeboten (s. Abbildung 2). Figure 2. Mögliche Parameter zum Tunen von Vacuum Ein viel-gelesener Artikel [ 3 ] , der die Schwächen von PostgreSQL aufzeigt, behandelt diese \"Copy on Write\" Versionierung und es wird angemerkt, dass es ein Limit der zu vergebenen Transaktions-IDs gibt. Bevor ich jemals eine PostgreSQL Datenbank live und in Farbe verwendet hatte, hat mir der Artikel nicht unbedingt Mut gemacht. Allerdings hat sich das im echten \"hands-on\" OLTP-Betrieb nicht bewahrheitet. Laurenz Albe hat eine interessante Gegendarstellung zum Transaktions-ID Wraparound Problem verfasst (vgl. [ 4 ] ). Im Gegenteil – PostgreSQL hat im Bereich Transaktionen mein Herz gewonnen, weil DDL-Befehle (also z. B. create table ) mittels Rollback rückgängig gemacht werden können. So macht Skripte-Schreiben wirklich Spaß! Man muss im Fehlerfall nicht aufräumen und komplizierte Workarounds schaffen. Die Objekt-Änderungen werden einfach nicht commited. Das Auto-Commit ist in allen gängigen Clients (DBeaver, psql, pgAdmin,…) Standard. Man muss mittels begin Befehl explizit eine Transaktion aufspannen, um ein implizites Commit zu verhindern. Das wiederum macht das Ausführen von DML, z. B. einem delete -Befehl, erst einmal ungewohnt riskant. Wenn man nicht aufpasst, sind die gelöschten Daten wirklich festgeschrieben. Ein Rollback ist nicht mehr möglich. In meinem Projekt musste ich intensiv mit Zeitstempeln und Zeitzonen hantieren. Vor allem, wenn Client und Server nicht mit der gleichen Zeitzone konfiguriert sind, muss es zwangsläufig zu einer Konvertierung kommen, sodass \"hin- und zurückgerechnet\" werden kann. Wenn in Oracle ein Zeitstempel im Datentyp timestamptz gespeichert wird, gibt es Bytes, die festhalten in welcher Zeitzone diese Uhrzeitangabe zu verstehen ist. Was Bit-genau gespeichert wird, hängt von den NLS -Settings von Client und Server ab. In PostgreSQL gibt es eine simple Regel: There is no such thing as a server time zone. Der Server nimmt den Zeitstempel so an, wie es vom Client angegeben wird (entweder indirekt über set timezone in der Session oder direkt am übertragenen String durch die Formatmaske) und konvertiert in jedem Fall in UTC. Ein lesender Prozess findet anschließend immer Bits auf Platte, die den Zeitstempel in UTC repräsentieren. Zur Konvertierung nutzt PostgreSQL eine zentrale Datenbank für Zeitzonen – die IANA. Figure 3. Zeitstempel werden stets in UTC gespeichert Ein wenig anders verhalten sich in PostgreSQL Client und Server übrigens auch beim Austausch des Zeichensatzes (z. B. Unicode). Aber da die NLS-Settings und UTF-8 Encoding in Oracle einen eigenen Artikel wert wären, möchte ich es lediglich erwähnen. Zum Thema Zeiten möchte ich außerdem noch knapp erwähnen: Wer viel mit Zeitstempel-Arithmetik zu kämpfen hat, sollte sich in PostgreSQL auf jeden Fall mit dem tsrange() Datentypen vertraut machen [ 5 ] . Ich würde mir ein Pendant in der Oracle Datenbank wünschen. Performance Troubleshooting Im letzten Teil widme ich noch ein paar Zeilen einem Thema, welches mich im Oracle Kontext in den Bann gezogen hat. Performance Troubleshooting ist mit den Oracle Bordmitteln ein echter Zuckerschlecken. Die Art und Weise wie diese Software instrumentiert ist, ist einzigartig. Dabei gilt vor allem, dass das Logging per Default eingeschaltet ist; vornehmlich beziehe ich mich hier auf das Event Tracing und das AWR=Active Workload Repository, worauf dann auch die ASH=Active Session History basiert. Somit liegen die relevanten Informationen förmlich auf dem Tisch und man ist nur mit der eigentlichen \"Detektiv-Arbeit\" beschäftigt. Ich weiß nicht, bei wie viel Gigabyte eine Oracle-Installation inzwischen angekommen ist. Zwar ist eine PostgreSQL Vanilla-Version deutlich schlanker, das macht sich dann aber genau an solchen Features bemerkbar. Der Weg in der PostgreSQL-Welt führt dann über sogenannte Extensions. Diese sind nachträglich zu installieren. Somit ist zumindest eine Vanilla-Installation sorgfältiger zu planen. Ich habe sogar verstärkt wahrgenommen, dass bei Admins und Experten häufig noch der Performance-Nachteil (\"Workloads laufen 2% langsamer\") der Gesamtinstallation als Argument aufgeführt wird, weshalb man intensives Logging nicht per Default aktiviert. Hier halte ich es wie Tom Kyte [ 6 ] . In allen Umgebungen, in denen ich aktiv war, wurden hilfreiche Instrumentations-Mechanismen nicht aktiviert, sodass die Suche bei erstmalig aufgetretenen Performance-Schwierigkeiten knifflig bis unmöglich war. Die Metriken für eine detaillierte Analyse waren schlichtweg nicht vorhanden. Nach meinem ersten Jahr kann ich folgenden Ansatz empfehlen: Um proaktiv und nachträglich Langläufer im SQL zu identifizieren, sollte die Extension pg_stat_statements installiert werden. Es handelt sich dann um eine View, die mittels SQL-Abfragen analysierbar ist. Diese hat einige Schwächen, z. B. dass statt dem echten SQL-Statement eine normalisierte Form gezeigt wird. Literale werden durch Bind-Variablen ersetzt und die Belegung der Binds kann nicht mehr nachvollzogen werden. Das lässt keine Einzelfallbetrachtung zu. Deswegen sollte zusätzlich auch auto_explain geladen werden. Damit werden in den Server-Logs mehr Infos rund um ein SQL (inklusive ermitteltem Ausführungsplan) weggeschrieben. Der Schwellenwert, ab welcher Laufzeit ein SQL geloggt wird, lässt sich konfigurieren ( auto_explain.log_min_duration ). Hat man mit der breiten Analyse den Langläufer identifiziert, kann man folglich auf die Server-Logs zurückgreifen, um mehr Infos zu erhalten. Wenn das Problem reproduzierbar ist, kann man in PostgreSQL leider kein echtes Tracing der Session aktivieren. Die Extension pg_show_plans erlaubt es, zumindest aus einer zweiten Session heraus, in eine Langläufer-Session reinzuspicken, um den Ausführungsplan der laufenden Abfrage zu sehen. Im Fall, dass das Problem reproduzierbar ist, gibt es auch kein Sampling, wie man es von der ASH kennt. Ich finde jedoch, dass die Extension pg_sentinel vielversprechende ASH -ähnliche Ansätze beinhaltet. Um dann ein Statement wirklich anzupacken und dem Optimizer unter die Arme zu greifen, kann man nur auf Hints zurückgreifen, wenn die Extension pg_hint_plan installiert wird. Diese hat auch eine Funktionalität ähnlich zu Baselines. Baselines werden in Oracle auf Basis der SQL_ID erstellt, welche auch im Library Cache abgelegt wird. PostgreSQL hat keine SGA und damit auch keinen Library Cache. Deshalb bringen Bind-Variablen auch nur Performance-Vorteile in der gleichen Session. Session-übergreifend können geparste SQL-Befehle nicht mit einem Softparse wiederverwendet werden. Es gibt die Möglichkeit von Prepared Statements, die das mehrfache Ausführen desselben SQLs innerhalb einer Session beschleunigen. In der Ausgabe 01-2021 des Red Stack Magzins hat Herve Schweitzer einen eigens diesem Thema gewidmeten Artikel geschrieben [ 7 ] - sehr empfehlenswert! Fazit Der vorliegende Artikel kann nur begrenzt technische Feinheiten beschreiben, die dann für die wirklichen Unterschiede zwischen den beiden Datenbanken sorgen. Es wurden Syntax- und Architektur-Unterschiede betrachtet - das Wort Migration wurde von mir beispielsweise komplett ausgespart. Für mich ist klar, dass ich mich nach einem Jahr nicht für Oracle oder PostgreSQL entscheiden kann/will und für einen Umstieg plädiere! Für mich gilt: \"Kenne beide Seiten und bleibe in Übung\". Wenn jedoch keine Gründe dagegensprechen, starten Sie neue Projekte mit PostgreSQL und probieren Sie es aus. Ich freue mich auf Ihr Feedback. 1 . https://maximorlov.com/char-varchar-text-postgresql/ 2 . https://www.postgresql.org/docs/14/functions-math.html 3 . https://rbranson.medium.com/10-things-i-hate-about-postgresql-20dbab8c2791 4 . https://www.cybertec-postgresql.com/en/transaction-id-wraparound-a-walk-on-the-wild-side/ 5 . https://www.cybertec-postgresql.com/en/multiranges-in-postgresql-14/ 6 . https://carymillsap.blogspot.com/2009/02/on-usefulness-of-software.html 7 . https://www.doag.org/de/home/news/red-stack-012021-database-blockchain/ "
},

{
    "id": 8,
    "uri": "blog/2024/2024-02-06-ChatGPT.html",
    "menu": "Blog",
    "title": "Architektur mit ChatGPT",
    "text": " Table of Contents ChatGPT als Werkzeug für Software-Architektur ChatGPT als Werkzeug für Software-Architektur Kann ChatGPT Software-Architektur? Das war die Frage, der Eberhard Wolff und ich in drei Sessions nachgegangen sind. Zu einem Zeitpunkt, als ChatGPT noch viel weniger konnte als heute, interessierte ich mich schon sehr für die Technologie. Allerdings weniger für die Technik dahinter, als für die Anwendung. Ich fragte mich: Was kann ich als Software-Architekt mit diesem spannenden Tool anfangen? Kann ChatGPT die Beispielaufgabe der iSAQB-Advanced-Zertifizierung lösen? Mutig habe ich die Idee als Talk beim iSAQB Software Architecture Gathering eingereicht – wohl wissend, dass ChatGPT damals noch nicht so weit war. Die Zeit zwischen Einreichung und dem Talk habe ich genutzt, um meine Fähigkeiten im Prompt Engineering zu schärfen. So kam es dann auch zu einem Talk auf der W-JAX . Dort hatte ich die Chance, mich mit Eberhard Wolff nach seinem Talk Architektur für Menschen - nicht für Software auszutauschen. Auch Eberhard hatte sich mit dem Thema KI in der Softwareentwicklung auseinandergesetzt und so kamen wir auf die Idee, das Ganze mal gemeinsam anzugehen. Aus der Idee wurden recht schnell drei Videos auf Eberhards Youtube-Kanal \"Software-Architektur im Stream\". Im ersten Teil wiederholen wir nochmal die Grundlagen aus dem iSAQB Talk. Ich erzähle, wie ich das Problem angegangen bin. Wir diskutieren, auf welche Probleme ich gestossen bin und welche Prompt-Engineering-Techniken sinnvoll sind. Im zweiten Teil wirft Eberhard einen genaueren Blick auf die Ergebnisse und erläutert, auf welche Details er als Prüfer schaut. Das Ergebnis: Ja, ChatGPT hat Lösungen für die gestellten Aufgaben produziert, einer genaueren Prüfung halten sie aber nicht stand. Für die Zertifizierung heisst das, ChatGPT bekommt die Chance, nachzuarbeiten. Im dritten Teil versuchen wir, ChatGPT mit ein wenig mehr Aufwand eine bessere Lösung zu entlocken. Des Weiteren dsikutieren wir, wie die KI uns als Softwarearchitekten unterstüzen kann und was die Zukunft vielleicht noch bringt. "
},

{
    "id": 9,
    "uri": "blog/2024/2024-01-25-Evolution-der-Webentwicklung.html",
    "menu": "Blog",
    "title": "Meine Reise durch die Evolution der Webentwicklung",
    "text": " Table of Contents Vom Code zum Klick: Meine Reise durch die Evolution der Webentwicklung Zuerst ein paar Meilensteine der Webentwicklung tabellarisch zusammengefasst: Meine ersten Erfahrungen mit dem Internet Wie wurden Webseiten 2014 gebaut? Wie ist der Stand heute? Wie helfen moderne Frontends bei der Performance? Hier der Aufbau eines modernen Frontends mit BFF (Backend for Frontend): Wie baue ich Frontends? Bevor ich mit einem neuen Projekt anfange Abschließend ein paar Wünsche an das Frontend, welches ihr baut: Vom Code zum Klick: Meine Reise durch die Evolution der Webentwicklung In diesem persönlichen Rückblick tauchen wir ein in die faszinierende Welt der Webentwicklung – von den frühen Tagen des manuellen Codings bis hin zu den modernen Technologien, die heute den Standard setzen. Begleitet mich auf meiner Reise durch die Zeit, in der sich HTML, CSS und JavaScript von Grund auf verändert haben und entdeckt, wie sich die Herausforderungen und Lösungen in der Webentwicklung im Laufe der Jahre gewandelt haben. Von den ersten Schritten mit einfachen Tools bis hin zu den komplexen Frameworks und Methoden von heute – dies ist eine Geschichte von Innovation, Lernen und ständiger Weiterentwicklung. Zuerst ein paar Meilensteine der Webentwicklung tabellarisch zusammengefasst: 1991 erste Webseite mit HTML 1995 PHP und JavaScript 1996 CSS ------------- bis hierhin kaum automatisiertes Deployment möglich (z. B. über FTP) 2000 SVN 2003 Wordpress (Joomla/Typo3 etc.) 2005 GIT ------------- Versionierung 2006 jQuery und AWS 2008 Static Site Generators (SSG) 2009 Node.js 2010 NPM und AngularJS 2011 Bootstrap CSS 2012 Webpack, TypeScript 2013 Docker, React und Server Side Rendering ------------- automatisierte und idempotente Deployments möglich 2014 Vue.js , CSS-in-JS 2015 PWAs (100% seit 2022) 2016 Next.js , Angular (4+) 2020 Vite 2022 Generatoren für Frontend/Backend Kombinationen refine.new , RedwoodJS , Blitz.js 2023 Ende des Internet Explorers 2023 React Server Components Mehr Details und ausführlichere Infos gibt es unter anderem auf: designmodo.com: The Short History of Website Building Nicholas Mendez: A \"Brief\" History of the Web Part 4 Nick Telsan: What Even Are React Server Components Jesse Pence: React Server Components Meine ersten Erfahrungen mit dem Internet Wenn ich mich an die Anfänge meiner Tätigkeit erinnere, haben wir HTML, CSS, PHP und jQuery mit BBedit selbst geschrieben, gespeichert und manuell per FTP auf Server hochgeladen. Das Beste, was es damals gab, war das Programm Transmit, mit dem einzelne Ordner synchronisiert werden konnten. Alles war sehr fehleranfällig, ohne Versionierung und daher schwer bis gar nicht testbar – nur manuell. jQuery-Plugins wurden mit Abhängigkeiten zu unterschiedlichen jQuery-Versionen benutzt, die nicht unbedingt kompatibel waren. Man musste jedes Mal hoffen, dass die Webseite noch funktionierte, nachdem man Code hinzugefügt hatte. Was mich immer am meisten geärgert hat, war ein Flackern, sobald das JavaScript geladen wurde und über das PHP-gerenderte HTML noch extra DOM-Elemente legte. Die folgenden Probleme sind meiner Meinung nach die größten beim klassisch gerenderten Web: Wann ist der DOM fertig geladen? Wann ist das JS geladen und wird ausgeführt? Wer hat bei intensiven Seiten „das Sagen“ bzgl. Routing und Views – das Frontend oder das Backend? Wie wurden Webseiten 2014 gebaut? Nach meinem Studium habe ich in Hamburg bei AboutYou in einer IT-Agentur gearbeitet, die nach Scrum ihren Arbeitszyklus gestaltet und schon damals diverse Tools zum Bauen des Frontends benutzt hat. Da die Tools alle am Anfang ihres Lebenszyklus waren, mussten mindestens drei Package-Manager und Erweiterungen installiert werden, bevor die Webseite gebaut werden konnte. Gebaut? Ja, das war die Zeit, in der Grunt und Webpack eine moderne und strukturierte Frontendentwicklung ermöglicht haben. Vorbei waren die Zeiten mit unterschiedlichen Encodings und Zeilenumbrüchen. Was für ein Gefühl es war, lokal einen Befehl in der Kommandozeile aufzurufen und die zu entwickelnde Webseite auf dem eigenen Gerät zu sehen – heute Standard. Hier lernte ich zum ersten Mal Package-Manager und Versionierung kennen. Und noch ein Thema kam auf: IDEs für Webentwicklung. Bisher war der beste Freund der Inspektor im Browser, eine lokale, beim Entwickler stattfindende Überprüfung oder Unterstützung konnten bis dahin nur die „richtigen“ Programmiersprachen und die dazugehörigen teuren IDEs (z. B. IntelliJ) bieten. Mit Atom gab es eine IDE, die schnell startete, sich nur um Websprachen kümmerte und über Plugins erweiterbar war. Jetzt gab es Autovervollständigung für HTML/JS/CSS und über Webpack konnten alle Dateien optimiert, komprimiert und kombiniert werden. Wie ist der Stand heute? Sehr gut! Viele Probleme der Vergangenheit sind verschwunden (Internet Explorer existiert nicht mehr, schlechte Kompatibilität der Browser) und es gibt eine Reihe von Möglichkeiten, Frontends mit Frameworks zu bauen, die alle ihre Vor- und kleinen Nachteile haben. Z. B. hat sich Visual Studio Code zum kostenlosen Nachfolger von Atom für die Webentwicklung mit noch mehr Möglichkeiten zur generellen Verbesserung gemausert. Frontend-Frameworks gibt es viele (und es kommen immer mehr dazu) – durchgesetzt haben sich aktuell React, Angular, Vue.js und Svelte. Wie helfen moderne Frontends bei der Performance? Schauen wir uns hierzu den Ablauf einer klassischen Webseite an: Ein Endgerät ruft die URL unserer Seite auf. Der Server antwortet je nach Session/Cookie/Authentication mit einer Login-Seite oder dem gerendertem HTML eines PHP-Skripts. Evtl. wurde der Client auf eine andere URL umgeleitet. Das HTML wird vom Browser interpretiert und JS/CSS/Bilder werden nachgeladen. Das CSS sorgt für das Styling der Seite. Das ausgelieferte JavaScript wird interpretiert, verändert das DOM und lädt neue Daten nach, und das wiederholt sich, wie hier beschrieben. Alle Schritte benötigen Zeit und führen zu einer mehrsekündigen Wartezeit, bis die Webseite benutzt werden kann. Hier der Aufbau eines modernen Frontends mit BFF (Backend for Frontend): Der Client fragt den Server an und erhält HTML, JS und CSS sowie den Login-Status vom Backend, und zwar so aufbereitet, dass der Browser es direkt darstellen kann, ohne weitere Schleifen zu drehen. Weitere Inhalte werden z. B. als JSON nachgeladen und im Browser mit JavaScript gerendert. Zusätzlich gibt es für alle JS-Frameworks Bundler, die beim Bauen der auszuliefernden Dateien diese optimal aufbereiten, sodass der User innerhalb kürzester Zeit mit dem Frontend interagieren kann. Wie baue ich Frontends? Mein Lieblings-Javascript-Framework ist React. Eine gute Übersicht über andere JS-Frameworks und was sie unterscheidet, findet ihr z. B. auf themer.dev . Von dieser Seite kommt folgendes Zitat: „Reacts change detection paradigm is straightforward: the application state is maintained inside the framework (with APIs exposed to the developer for updating it) so that React knows when to re-render.“ Und was heißt das jetzt? Nicht mehr der Browser oder das Backend entscheiden, auf welcher URL was passiert, sondern das Frontend. Und wenn im Frontend eine Aktion ausgelöst wird, entscheidet auch das Frontend, was passiert. Das heißt nicht, dass es kein Backend gibt oder dass z. B. Validierung nur auf der Frontendseite passiert. Sicherheit im Web sollte der wichtigste Aspekt sein. Bevor ich mit einem neuen Projekt anfange Was sich für mich als Erfolgsrezept herausgestellt hat: Sich am Anfang eines Neu- oder Umbaus einer Seite viel Zeit nehmen, um passende Libraries/Frameworks zu finden, die einem das Leben erleichtern und diese zu benutzen (UI Components, Accessibility, Icons, etc.). Am besten auf bekannte Frameworks setzen, die regelmäßig aktualisiert/gewartet werden. Abschließend ein paar Wünsche an das Frontend, welches ihr baut: Macht die Webseite responsive (Probiert eure Webseite mit unterschiedlichen Geräten in unterschiedlichen Auflösungen aus) Räumt euren Header-Bereich auf und nutzt z. B. realfavicongenerator.net , um für alle ein schönes Favicon zu zaubern Nutzt Komponenten die Accessability mit eingebaut haben (z. B. Blueprint , Mantine oder React Suite ) Macht den Google Lighthouse Test und behebt die wichtigsten Probleme Macht nur eine PWA, wenn offline-Inhalte essenziell sind oder ihr eine richtige App ausliefert Erlaubt das Zoomen eurer Seiten, sowie das Kopieren und Einfügen von Text Wenn etwas sich wie ein link verhält, dann lasst die User Link-Aktionen machen (z. B. speichern unter) Macht Hover-Styles für Links Wenn sich die Gui geändert hat, nachdem geklickt wurde, ändert die URL Das war die Reise durch 20 Jahre Webentwicklung – ich bin sehr gespannt, was in 20 Jahren im Web alles möglich ist. "
},

{
    "id": 10,
    "uri": "blog/2024/2024-01-16-Accessibility-in-Angular.html",
    "menu": "Blog",
    "title": "Accessibility in Angular",
    "text": " Table of Contents Accessibility in Angular – Angulars features for a better and more inclusive web Slides Demos Accessibility in Angular – Angulars features for a better and more inclusive web The Angular Framework brings us some built-in features to help in creating accessible components and applications by wrapping common best practices and techniques. In this talk at the Angular Berlin Meetup, Danny presented these concepts and features. The Agular Meetup Berlin, hosted at Angular Berlin Meetup takes place regularly at DB Systel in Berlin. On January 16th 2024, Danny delved deeper into the Angular Framework and examining its features that aid in enhancing overall accessibility. Furthermore, he guided attendees through the use of the Angular CDK which provides additional tools helping us to improve the accessibility of our application. Slides Demos Angular a11y Demo Repository with source code related to the slides "
},

{
    "id": 11,
    "uri": "blog/2023/2023-12-21-vollautomatisch-konstruierter-Fahrplan.html",
    "menu": "Blog",
    "title": "Fahrplankonstruktion",
    "text": " Table of Contents Von der Vision eines vollautomatisch konstruierten Fahrplans Slides Von der Vision eines vollautomatisch konstruierten Fahrplans Wie mit Digitalisierung der Prozesse und agiler Software-Entwicklung der Fahrplan unter dem \"rollenden Rad\" sukzessive auf die Digitale Schiene gesetzt und gleichzeitig eine Kapazitätssteigerung erreicht wird. Jede Zugfahrt in Deutschland braucht einen Fahrplan. Das sind bis zu 50.000 Zugfahrten mit Fahrplänen pro Tag. Doch wo kommen diese her? Die steigende Verkehrsnachfrage führt auf der einen Seite zu höheren Kapazitätsbedarfen und Auslastung der Schieneninfrastruktur. Auf der anderen Seite wird die verfügbare Kapazität während der Bau- und Modernisierungsaktivitäten der Schieneninfrastruktur und der Leit- und Sicherungstechnik während der Maßnahmen reduziert. Zur frühzeitigen Identifikation, Vermeidung und Handhabung von möglichen Kapazitätsengpässen sind die IT und die weitere Digitalisierung der Prozesse gefordert. Durch die Einführung digitaler Prozesse entwickeln wir uns weg von der einer manuellen Fahrplankonstruktion hin zu einer weitestgehend vollständig automatisierten Fahrplankonstruktion und Kapazitätsmanagement, so dass die zukünftig steigenden Verkehrsbedarfe auf der Schiene weiterhin bedient werden können. Hierzu sind wir vor 3 Jahren gestartet, die monolithischen Bestandssysteme sukzessive durch modulare Services mit einer flexiblen, integrierten Plattform abzulösen und zu automatisieren. Damit verbunden ist die Neugestaltung und Automatisierung von Abläufen und Prozessschritten, sowie eine Konsolidierung der Datenhaltung und Informationsflüsse. Die agile Software-Entwicklung bietet eine höhere Flexibilität und Reaktionsschnelligkeit auf Kundenwünsche und neue Anforderungen. Wir arbeiten iterativ und sind im ständigen Austausch zu unseren Stakeholdern. Dadurch werden Teillieferungen in regelmäßigen Abständen gewährleistet, um so der großen Vision stückweise näher zu kommen. Die Herausforderung ist, mit über 600 Experten in crossfunktionalen, agilen Teams synchronisiert neue Abläufen, automatisierte Prozessschritte und flexibel kombinierbaren und modularen Services zu konzipieren, entwickeln und unter dem \"rollenden Rad\" sukzessive in Produktion zu bringen. Als DB Netz sind wir in Deutschland eines der größten Unternehmen, die einen signifikanten Anteil der IT-Entwicklung nach SAFe® 6.0 gestalten. Unsere Teams arbeiten mit modernsten Standards in der Cloud, setzen auf eine sehr hohe Automatisierung im Lebenszyklus der Softwareentwicklung und übernehmen eine Ende-zu-Ende Verantwortung für ihre entwickelten Artefakte. Slides download "
},

{
    "id": 12,
    "uri": "blog/2023/2023-11-29-AI-in-Software-Design.html",
    "menu": "Blog",
    "title": "AI in Software Design",
    "text": " Table of Contents Using AI in Software Design: How ChatGPT Can Help With Creating a Solution Architecture References Slides &amp; Video Using AI in Software Design: How ChatGPT Can Help With Creating a Solution Architecture Artificial intelligence, or AI, is becoming a big part of our lives. One type of AI is Large Language Models (LLMs), like chatGPT. At the iSAQB Software Architecture Gathering , I showed how chatGPT can help with complex tasks in software design, such as the tasks in the iSAQB Advanced Exam. We delved into the intricacies of conversing with ChatGPT, illustrating the strategies needed to generate productive prompts and effectively utilize the model as a sparring partner. The talk exemplified these concepts by taking the audience on a step-by-step journey through the process of tackling the iSAQB Advanced Example Exam, utilizing ChatGPT. We demonstrated how to prepare for a chat session with ChatGPT, how to generate robust prompts, and how to manage the chat for optimal outcomes. Our aim was to highlight the model’s capabilities as an interactive tool that can provide valuable insights and streamline the process of developing software architecture. Whether you are a seasoned architect or a novice in the field, this presentation will equip you with novel techniques to navigate the challenging landscape of software architecture with the assistance of AI. Discuss this topic with us on Github! References Full HTML Chat Protocol and generated Solution as PDF ChatGPT Prompt Engineering for Developers - DeepLearning.AI Softwarearchitektur: \"KI wird unsere Fähigkeiten ergänzen&#44; nicht ersetzen\" KI in der Softwareentwicklung: Überschätzt | heise online Stefan Toth: Architektur in Zeiten von KI und LLMs Slides &amp; Video "
},

{
    "id": 13,
    "uri": "blog/2023/2023-11-20-einfuehrung-barrierefreiheit-web.html",
    "menu": "Blog",
    "title": "A11y: EAA, BFSG, WCAG, WAI, ARIA, WTF? – it's for the people stupid!",
    "text": " Table of Contents A11y: EAA, BFSG, WCAG, WAI, ARIA, WTF? – it&#8217;s for the people stupid! Video Slides Referenzen A11y: EAA, BFSG, WCAG, WAI, ARIA, WTF? – it&#8217;s for the people stupid! Barrierefreiheit ist ein verstaubtes und unwichtiges Thema für Behördensoftware? Ganz im Gegenteil: Accessibility betrifft uns täglich und immer, wenn wir Software verwenden. Es ist an uns, diese umzusetzen. In unserem Talk von der W-JAX am 07.11.2023 zeigen wir euch, wie ihr eure Webanwendungen von Beginn an mit einfachen Mitteln zu einem hohen Grad barrierefrei gestaltet und entwickelt. Wir stellen euch effektive Methoden und Tools vor, die euch bei der Umsetzung und Prüfung in hohem Maße unterstützen. Wir gehen darauf ein, welchen Einfluss Barrierefreiheit auf eure Frontend-Architekturen hat und welche Aspekte hier zu beachten sind. Dabei beten wir nicht die einzelnen WCAG-Kriterien herunter – vielmehr geben wir euch eine praktische Einführung in das Thema Barrierefreiheit im Web und welche Kniffe und Fallstricke zu beachten sind. Video Slides Referenzen Bild von Bill Clinton W3C Logo Grafik \"human\" "
},

{
    "id": 14,
    "uri": "blog/2023/2023-11-20-conways-law.html",
    "menu": "Blog",
    "title": "Conway’s Law",
    "text": " Table of Contents Conway’s Law in real life - why organizational development and software engineering have to go hand in hand. Slides Conway’s Law in real life - why organizational development and software engineering have to go hand in hand. Conway’s Law inevitably connects the architecture of an software system with the responsible delivery organization. Unfortunately, persons responsible for organizational development and architects of software systems often work in separation – especially in large enterprises. What are the negative effects of this separation and what can we do about it? In this talk Dr. Martin Strunk presents “real world examples” of the impact that arising from the ignorance of Conway`s law by decision makers. He will also propose ways, in which companies might overcome and mitigate these problems. Teams as first-class citizens Design comes first, then comes the participation Don’t rely on emergence to solve structural problems Slides "
},

{
    "id": 15,
    "uri": "blog/2023/2023-11-08-prompt-engineering.html",
    "menu": "Blog",
    "title": "Prompt Engineering",
    "text": " Table of Contents Die faszinierende Welt des Prompt Engineerings Slides Referenzen Die faszinierende Welt des Prompt Engineerings Im Zentrum meines jüngsten Vortrags stand das Prompt Engineering, ein Schlüsselelement im Umgang mit fortschrittlichen Sprachmodellen wie GPT-3 und GPT-4. Das Prompt Engineering ermöglicht es Entwicklern, genaue und nuancierte Antworten von KI-Modellen zu erhalten und das Potenzial der KI voll auszuschöpfen. Wir tauchten tief in die Unterschiede zwischen GPT-3 und GPT-4 ein, erkundeten die Komplexität neuronaler Netze und die Bedeutung der Multi-Modalität in der heutigen KI-Landschaft. Ein besonderes Augenmerk legten wir auf die Kunst des Primings und der Kontextualisierung von Prompts, die die Effektivität und Präzision der KI-Interaktionen erheblich steigern können. Neben theoretischen Überlegungen wurden reale Anwendungsfälle diskutiert, die das Potenzial von Prompt Engineering veranschaulichen. Slides Referenzen Golem.de: BSI sieht KI als Sicherheitsrisiko Twitter: The Hired AI Medium: ChatGPT Architecture Explained by Sreedev R Scientific Reports: Comparative performance of humans versus GPT-4.0 and GPT-3.5 GPT-4: Wikipedia FAZ.net: ChatGPT: Welche Einstellungen bei der Nutzung helfen OpenAI Chat: Beispiel OpenAI: Verbesserung der docToolchain Dokumentation DeepLearning.AI: ChatGPT Prompt Engineering for Developers Heise Online: Softwarearchitektur: \"KI wird unsere Fähigkeiten ergänzen, nicht ersetzen\" Heise Online: KI in der Softwareentwicklung: Überschätzt OpenAI Chat: Beispiel 2 Stefan Toth: Architektur in Zeiten von KI und LLMs "
},

{
    "id": 16,
    "uri": "blog/2023/2023-08-21-vue2-vue3-migration.html",
    "menu": "Blog",
    "title": "Migrate Vue 2 to Vue 3",
    "text": " Table of Contents How we migrated our Vue 2 enterprise project to Vue 3 About me and my Team Where we started Preparations Migrations before the migration Finally: Migrate to Vue 3 Post-Migration-Steps Conclusion How we migrated our Vue 2 enterprise project to Vue 3 It&#8217;s been a while: Since the 2nd February 2022, Vue 3 became the new default for Vue.js apps. It&#8217;s done! Vue 3 is now the default version and the brand new http://vuejs.org is live! More details in the blog post in case you missed it: https://blog.vuejs.org/posts/vue-3-as-the-new-default.html Tweet from @vuejs on Twitter It was a long journey to the final default release of Vue 3 since the first version published on 18th September 2020. But: even if Vue 3 isn&#8217;t a new thing anymore, there are still a lot of Vue 2 apps which haven&#8217;t been migrated yet. The migration can be quite heavy since in practice it&#8217;s much more than only following the migration guide. Projects usually rely also on 3rd-party dependencies which are maybe not available for Vue 3 or not maintained anymore. In this blog post I will give you an insight into how my team mastered the migration and what pitfalls we faced. I will describe how we planned and migrated our whole Vue 2 codebase to Vue 3 using Pinia as Store-Solution, Vite for our build environment and Vitest for fast unit test executions. The focus of this article is not to provide a very detailed step-by-step migration guide. I will focus about what things you should keep in mind, what you can already do before starting the migration and about some pitfalls we pointed out. However, I will provide you links to more detailed blog posts about specific topics. Please keep in mind, that the way we solved the migration won&#8217;t probably fit to your very specific setup for 100%, but you can check what parts seem to be good for you and your team. Vue 2 EOL: Please note, that the Vue 2 support will end on December 31st, 2023. This means there will be no fixes and features provided anymore (unless you are actively extending the support ) About me and my Team To give you a high level overview about our context, I would just like to say a few short words about myself and my team. I am working at DB Systel GmbH , in a DevOps Team building a Business-to-Government (B2G) solution together with our partner Deutsche Bahn Connect GmbH named DB Curbside Management . Our product focuses on helping cities and councils to effectively manage shared mobility offerings and their jurisdictions dynamically. They will be able to get insights about statistics, violations of agreements with the mobility providers to regulate a fair and steady distribution of all the different shared mobility vehicles across managed area. Where we started My team added the migration to Vue 3 with the new default setup and tools using Vite , Pinia and Vitest to our backlog many months ago, but the switch to Vue 3 as default gave us another push for facing the migration. We realized pretty fast, that a big-bang migration wouldn&#8217;t be possible for us, since it will block us releasing new features for quite a long time. Our codebase contained already ~200 Vue 2 components using the old-fashioned Options API as well as a huge Vuex Store and some libraries that aren&#8217;t compatible with Vue 3. Preparations Let&#8217;s start with the preparation of your team and questions you should answer yourself before starting the migration. The first thing you should do is to get comfy with Vue 3, and you should learn about the differences compared to Vue 2 and the options you have. You can set up a Vue 3 playground app locally and explore yourself the new setup and components. To know what things will change when starting the migration, I would recommend you the read the following articles in advance: Official Vue 2 to Vue 3 Migration Guide Blogpost: \"Vue.js: How to Migrate a large project from Vue 2 to Vue 3\" from Baptiste Jamin Official Vue 3 notes about the Composition API and the relationship / differences compared to the Options API Vue Master Blog-Series by Andy Li Part 1 | Part 2 Free Vue Mastery Video Course \"From Vue 2 to Vue 3\" Update to the latest minor Vue 2 version My first advice is to keep your current app as up-to-date as possible. Especially when your Vue version is below 2.7.x , I would recommend you to update it. With Vue 2.7.x \"Naruto\" release, the Vue team aimed to backport lots of features from Vue 3 to Vue 2 without introducing a breaking change. This will help you to migrate some things in preparation for a smooth Vue 3 switch. Check out the official announcement and start migrating to the Vue 3 flavour in your Vue 2 app: TypeScript or not? Are you using TypeScript right now or do you plan to migrate to TypeScript? In that case you should read the TypeScript Notes for Vue 3 . Generally I would highly recommend to use TypeScript as the Vue 2 and Vue 3 TypeScript integration is great. It will help you a lot to reduce runtime errors as hard debugging nights by analyzing bugs in production. Be prepared, that switching to Typescript might require quite an effort, but it&#8217;s still worth it. Check your dependencies A big thing you definitely have to check before is: dependencies. You should check if you are using packages that will rely on Vue 2 and won&#8217;t be available for Vue 3. Such dependencies will require your attention as they may block you from updating to Vue 3. In my previous project we weren&#8217;t able to update to Vue 3 a long time since we had a dependency to BootstrapVue which wasn&#8217;t working with Vue 3 and isn&#8217;t still. In such case where a package isn&#8217;t compatible you have the following options: Check if there is an equivalent package or a fork of your dependency that will support Vue 3. If there is one: be sure it&#8217;s still maintained and alive. If the package is just a Vue-wrapper for a common library, you may need to use the library directly Find a similar package that supports Vue 3. In this case you have to make sure the new dependency supports all your use-cases, and you have to plan how to migrate this dependency. You can contribute to the dependency and help to make it Vue 3 compatible. You may have to check VueDemi which is a great developing utility to create or update Universal Vue Libraries for Vue 2 &amp; 3. Worst Case: Write the features you need by yourself, but be sure to open-source it afterwards ;-) In all these cases you should make yourself a list of the relevant development tasks with a very rough estimate about how complex the migration will be. For example write a + if the dependency migration is straight forward (already Vue 3 compatible). Write + for very hard-to-migrate dependencies, where you may need another solution or lib or implement stuff by yourself. Add Notes about things you shouldn&#8217;t forget when starting the migration. You should also include development dependencies for example for webpack plugins. It could look like the following example: Vue 2 dependency Vue 3 dependency notes estimate @dsb-norge/vue-keycloak-js @baloise/vue-keycloak similar API, similar features ++ v-tooltip floating-vue same lib under the hood with more features + vue2-datepicker vue-datepicker-next same lib with Vue 3 support + vue2-leaflet @vue-leaflet/vue-leaflet same API, but lots of relying components ++ vue2-leaflet-draw-toolbar - no Vue 3 equivalent + webpack-license-plugin rollup-plugin-license different plugin for rollup, with a different API, we need to check / adjust the output format + Try out things in a playground For dependency update / migrations you can&#8217;t estimate, it&#8217;s a good idea to set them up / try them out in an isolated new Vue 3 playground environment. After playing around, you should be able to estimate the effort. A good example when having a look at the migration list above would be to try out the rollup-plugin-license package. Check your current Webpack environment When coming from webpack and planning to migrate to Vite, you should check your webpack config for any special behaviors. You can use the playground to reflect / try out the setup in Vite. Here are some points that were interesting for us: We don&#8217;t need a specific SCSS/SASS/LESS configuration anymore as Vite brings support for this out-of-the-box We needed to migrate the webpack-license-plugin to rollup-plugin-license (see above) Vite comes with its own approach of reading and passing environment variables and build modes which is quite easy and handy Static Asset Handling by Vite is something you should probably know before Split your Store on paper When currently using Vuex, you may be lucky, and you have already some modules splitting your store into logical parts. In our case we had just one big store without any modules as the codebase has evolved over time, and we haven&#8217;t made the step to split the store. The migration to Pinia can be a good chance to face this now as Pinia lets you easily compose multiple small stores. You should check your current store configuration and write down the modules that are loosely coupled or even completely independent (e.g. a user or an auth store). Make the migration transparent and estimable The last thing we have done was to create a new epic for the whole migration and to create small estimable tasks. This was very important as we were now able to identify things we can prepare and do even before we started the migration itself and also tasks we can do in advance. On the other hand it helped us for the communication with the product owner and to make things transparent. Please keep in mind to add some time buffer for unexpected things occurring during the migration where you may need some extra time. For example: the migration from Vuex to Pinia took a lot more time than we thought before. But: it was definitely worth it. The TypeScript support is way better and the unification of actions and mutations reduces the Boilerplate code a lot. We also underestimated the time we needed to migrate the tests. This was hard by definition but quite time-consuming as I wrote in the introduction: We had a huge Vuex store. Migrations before the migration Before starting the migration itself you should migrate everything you can, which is not related to Vue 3 / vite. Here is what we have done in my team before the migration itself. Convert Filters to functions Vue 3 kicked out the concept of using filters in the template using the pipe ( | ) syntax ( {{ expression | myFilter }} ). Filters are simply functions that can be imported and used directly. You can already import the functions, use them as a method and then pass through the expression in the template before starting the Vu3 migration: {{ myFilter(expression) }} . Update and migrate dependencies Update all possible dependencies to their latest versions to make migrations for other libs in advance. At this step: double-check if vue-specific libs are ready for using with Vue 3 or if there are other libs you have to use. If you have to change to other libs and this one supports Vue 3, make the migration now. In our team we had already lots of our dependencies updated, since we are using Mend (formerly Whitesource) Renovate for housekeeping and continuous dependency version updates. When you decide to migrate a dependency to a new one that supports Vue 2 and Vue 3 or which should be replaced with a self-implementation: Do it in advance before the actual Vue migration. Isolate hard-to-migrate components It may happens, you realize, for some of your dependencies a migration won&#8217;t be straight-forward. In our case we decided some years ago, we want to use Leaflet.js as our map library to display and interact with features on a map. Therefore we also used a wrapper for Vue 2 applications called Vue2Leaflet which made us use Leaflet in a declarative manner. However, this architectural decision was now a problem for us, as not only this dependency is not supposed to use it with Vue 3 but also extensions for this library such as Leaflet.heat needed to be migrated. To face this issue we&#8217;ve gone one step back and rethink our architectural decision to use Leaflet. At this time there was already a Vue 3 wrapper for leaflet available but not as feature-rich as we needed it. So we created a new Architectural Decision Record (ADR) to evaluate and choose our future map library as it is a central component of our app and can&#8217;t be easily replaced. After doing a Proof-of-Concept (PoC), we decided to switch to OpenLayers and make use of the vue3-openlayers wrapper too, where we were also able to contribute missing features back into the project. This whole story is probably quite special to my team and our app, but the essential thing here was, that we prepared the central components in parallel to our productive app in a separate repository in isolation. Therefore, we created the components and defined their props and events with the help of Storybook . Of course, we also created tests for these components, so that we were prepared to copy over all this into the productive app and replace the existing components later, when we were ready to actually migrate to Vue 3. A drawback with this approach is of course: It probably blocks you with releasing new features or you have to implement them twice during the preparation time (one time for the productive app based on Vue 2, one time for the isolated components based on Vue 3). Update your NPM Scripts When checking your Vue 3 default setup you will notice that some NPM script names have changed by default. For example the default command to run the development build and server is now npm run dev instead of npm run serve . You can either change the names back since you are used to the \"old\" commands, or you can already name your commands in the Vue 2 setup to the new ones to get comfy with it. Please note that you may have to change the commands in you CI/CD Pipeline too. Switch to Vite You can switch to Vite before updating to Vue 3 this makes the \"big bang\" migration a bit smaller. For that, you should install Vite and use the official plugin @vitejs/plugin-vue2 . You also need to migrate all the webpack plugins and configs. When the setup is finished, cleanup all the webpack stuff including the config and the dependencies. During the migration we noticed, that we haven&#8217;t used Type-Only Imports in all our typescript and .vue files. The default Vite setup is configured in such way, Type-Only Imports will be forced when needed, otherwise you&#8217;ll receive errors during the build. We had the option to either deactivate this strict behavior by setting the typescript config option importsNotUsedAsValues to either preserve or remove (not recommended) or to migrate. Luckily, there is a community project called ts-import-types-cli that will automate a part of this step. So we just had to run the following command to migrate to Type-Only Imports at places needed: # remove the `--dry-run` flag to migrate actually and not only list the changes npx ts-import-types-cli --no-organise-imports -p tsconfig.json --dry-run The bad news: The tool didn&#8217;t find all occurrences of the Type-Only Imports, so when running npm run build , we caught some more we had to fix manually. Switch to Vitest After your migration to Vite, you should make use of Vitest as your new pretty and fast unit testing framework. In comparison to Jest it comes with a stable out-of-the-box ESM support and faster test executions. Until now Jest&#8217;s support for ESM is still experimental (State: Jest Version 29.5). The API is quite similar and mostly compatible to jest . If you used Mocha before, the migration shouldn&#8217;t be hard either. Switch to Pinia The next big step you should do in advance is the migration of your Vuex store. You can also do this step after the migration itself and keep Vuex for now. However, we decided, it&#8217;s a good idea, to migrate the store before and switch to Pinia since the API is a lot simpler and better composable when slicing our big store into chunks. Furthermore, it comes with better TypeScript support. At the Pinia-Docs you will find a very detailed Guide for the Migration from Vuex Migrate Components Last but not least we decided to migrate all our components to the composition API with the &lt;script setup&gt; syntactical sugar . This is a step you can also omit or do in advance, but we recommend using this API since it&#8217;s also a bit more performant, and it reduces the boilerplate code you have to write. Finally: Migrate to Vue 3 You are now prepared to migrate to Vue 3, and you&#8217;ve done already a lot of things which made this step much easier and shorter. Now you can start the migration of Vue itself. Keep in mind, that for the actual migration you must migrate the unit tests too as the test utils for vue3 are slightly different. Migrate the source code Here we started by adding Vue 3 as well as the @vue/compat package as described in the Vue 3 Migration Build documentation . Also, we needed to update the VueRouter to version 4.x.x and adjust the configuration. As good step-by-step guides, I would recommend you again to read the following Blogposts: \"Vue.js: How to Migrate a large project from Vue 2 to Vue 3\" from Baptiste Jamin The official Vue 2 to Vue 3 Migration Guide . If you have already prepared some components in isolation to work with Vue 3 as we did: Of course you should replace the old ones and probably adjust the props or events if the API of your new components changed compared to the Vue 2 ones. After this step your whole app should work as before (fingers crossed). The migration of the components itself can be done one by one after the migration until everything is converted to Vue 3. Migrate to @vue/test-utils@v2 After you migrated everything, you need to update to @vue/test-utils@v2 . The migration should be straight-forward when following the migration guide . Nonetheless it can take quite a bit of time depending on the amount of unit tests you have. Post-Migration-Steps Remove Compatibility Package Once every component is migrated, make sure to remove the @vue/compat and it&#8217;s configuration as you don&#8217;t need it anymore. Make use of the Teleport feature Now that we are using Vue 3, we can use the \"Teleport\" feature. Think about components creating their DOM elements deeply in the DOM caused by the component hierarchy but where you would expect the elements to appear somewhere else close to the root. A good example is displaying a modal conditionally: &lt;body&gt; &lt;ComponentOne&gt; &lt;ComponentTwo&gt; &lt;ComponentThree&gt; &lt;MyModal v-if=\"myCondition\"&gt; &lt;/ComponentThree&gt; &lt;/ComponentTwo&gt; &lt;/ComponentOne&gt; &lt;/body&gt; In Vue 2, the modal would be rendered and appear inside the ComponentThree . Using teleport in MyModal can lift the element up to the body tag which makes more sense for common modal dialogs. Conclusion Migrating from Vue 2 to Vue 3 can be a huge thing and takes quite a bit of time. But good preparation and pre-migration will make the whole migration process much easier, more estimable and won&#8217;t block you for so long with releasing new features. Compared to writing the whole thing from scratch, we think this was well worth it. I hope this post gave you some inspiration of how you can face the migration of your project. Happy Migration ✌🏼 "
},

{
    "id": 17,
    "uri": "blog/2023/2023-05-15-developer-experience-platform-fuer-entwicklerinnen.html",
    "menu": "Blog",
    "title": "Developer Experience Platform",
    "text": " Table of Contents Developer Experience Platform (DXP) für Entwickler:innen Video Developer Experience Platform (DXP) für Entwickler:innen Die Developer Experience Platform (DXP) bietet Services für jede Phase der Software-Entwicklung. Konzern- und Betreibervorgaben sowie Cloud-Richtlinien sind in diesen Services bereits inkludiert. DXP ermöglicht damit DB-konzernweit, dass Entwickler:innen schnell, einfach und sicher Deployments vornehmen können und Software-Anwendungen von der Konzeption bis zum produktiven Einsatz schneller entwickelt werden. Das Erklärvideo zeigt dies, indem es Zuschauende in den Alltag von Entwickler:innen „entführt“. Video "
},

{
    "id": 18,
    "uri": "blog/2023/2023-05-05-loom-threading.html",
    "menu": "Blog",
    "title": "Projekt Loom ist da",
    "text": " Table of Contents Threading wie es sein soll: Projekt Loom ist da Virtualisierung hilft schon immer … und der Weg ins Schlamassel Threads sind die Grundlage der Nebenläufigkeit Asynchrone Programmierung als Notlösung Projekt Loom als Rettung VirtualThreads: benutzen ist (fast) einfacher als vermeiden Anpassungen im eigenen Code Angewohnheiten hinterfragen Synchron war nie schlecht Ausblick: Structured Concurrency Threading wie es sein soll: Projekt Loom ist da Es ist endlich so weit - das lang ersehnte Projekt Loom hat seinen Weg in das JDK gefunden! Seit über fünf Jahren haben wir uns danach gesehnt, all die Krücken wie NIO , asynchrone Programmierung , CompletableFutures und AsyncServlets hinter uns zu lassen und Java wieder so zu schreiben, wie wir es schon immer wollten. Virtualisierung hilft schon immer Auf jedem Rechner gibt es Ressourcen, die begrenzt sind. CPU-Zeit ist seit jeher eine knappe Ressource. Gleichzeitig müssen jedoch häufig viele kleine Aufgaben erledigt werden. Heutzutage verwenden wir meist API-Backends, die Anfragen über HTTP erhalten. Sie lesen Daten, transformieren sie und verändern sie gegebenenfalls. Anschließend wird die Antwort per Netzwerk-IO gesendet. Dabei die Ressourcen effizient zu nutzen, war von Anfang an eine Herausforderung und erforderte viel manuelle Arbeit. Zum Glück hatte Edsger W. Dijkstra bereits im Jahr 1965 die brillante Idee, den Zugriff auf wertvolle Ressourcen zu virtualisieren. So bekam das Berkeley Timesharing System die ersten Threads der Computer-Geschichte. Das Konzept war einfach: Threads sind kostengünstig und virtualisieren den Zugriff auf wertvolle Ressourcen. Figure 1. Threading wie die Urahnen - mit einer CPU Ein Scheduler sorgt dafür, dass blockierte Threads unterbrochen werden und andere Aufgaben ausgeführt werden können, bis die notwendigen Ressourcen verfügbar sind. Ein wahrhaft revolutionäres Konzept! Die Welt hat sich seit den ersten Threads des Berkeley Timesharing Systems weiterentwickelt. „Moderne“ Betriebssysteme wie AmigaOS haben das Konzept des Threading verbessert, indem sie es dem Betriebssystem erlauben, rechnende Prozesse zu unterbrechen und an anderer Stelle fortfahren zu lassen. Anders als bei User Threads in SunOS , wo der Code im Thread selbst anzeigt, wann er unterbrochen werden soll. … und der Weg ins Schlamassel Wir haben seitdem viel getan, um das Thread-Konzept kaputt zu bekommen. Wir nutzen gerade Netz-IO in modernen Anwendungen ganz intensiv. IO ist oft das, was diese Anwendungen am meisten machen. Und auf der anderen Seite ist die Hardware viel schneller als `65 . Wir haben so viele Requests zu verarbeiten und die Rechner sind schnell genug. Das geht. Wir können mal eben eine Million Sockets offenhalten und damit arbeiten. Nur: das Threading selbst kommt nur mit ein paar zehntausend Threads klar. Und deswegen sind inzwischen die Threads selbst die wertvolle Ressource. Und deswegen mussten wir anfangen, die Threads selbst zu teilen, zu poolen und sie wiederzuverwenden. Hierhin fällt der Aufstieg der Event-basierten IO-Bibliotheken . Netty fällt in diese Kategorie. Figure 2. IO-Thread und Worker-Thread bei der Arbeit IO und Worker Threads: ein speziell für IO-Operationen abgestellter Thread nimmt Daten entgegen. Dieser Thread wickelt sämtliche IO-Operationen ab. Damit entfällt auch die Notwendigkeit für Locking und Synchronisierung. Sobald Daten eingetroffen sind, werden sie in separaten Worker-Threads verarbeitet. Worker-Threads sollen selbst nie blockieren. Es wird dabei meistens nur ein Thread (manchmal einer pro CPU) mit IO beauftragt. Er arbeitet mit „non-blocking IO“ , erhält also Events, sobald eine IO-Operation abgeschlossen ist. Dadurch kann ein Thread alle offenen Sockets auf einmal bearbeiten. Sobald das IO abgeschlossen ist, wandert die Arbeit zu einem Worker-Thread weiter, der Berechnungen vornimmt. So lässt sich in unserem Beispiel bei drei gleichzeitig aktiven Requests die Thread-Zahl auf zwei reduzieren. Der Preis dafür ist, dass die Worker-Threads selbst Bescheid geben müssen, wenn sie fertig sind. Da ist dann das „alte“ kooperative Multitasking wieder. In der Praxis spielt das aber weniger eine Rolle, weil wir mehrere Worker-Threads benutzen, als Thread-Pool. Trotzdem – wir bezahlen gleich mehrere Preise dafür: Für jeden Request gibt es mindestens zwei Thread-Wechsel. Und die sind teuer. Sind Teile der Anwendung rechenintensiv, dann müssen wir selbst dafür sorgen, dass sie niemanden blockieren. Dann gibt es mehrere Thread-Pools. &#8230;&#8203; und wir brauchen ein kluges Threading-Konzept. Meistens heißt das, verschiedene Pools für Rechenlast, Netzwerk und File-IO einzuführen. Die IO-APIs sind alles andere als einfach zu bedienen. Und immer etwas anders. Netty für Netzwerk-IO. NIO für File-IO. RDBC für den Datenbankzugriff. Threads sind die Grundlage der Nebenläufigkeit Die Kernkonzepte von Java basieren auf Threads. Das gilt für den Sprachkern, die VM, fürs Debugging und das Profiling. IO-APIs waren synchron und sind in synchroner Form heute noch am übersichtlichsten zu benutzen. Das gesamte Exception-System ergibt nur innerhalb eines Threads wirklich Sinn. Speicherzugriffe innerhalb eines Threads sind geordnet und überschaubar. Wir könnten am übersichtlichsten alle Arbeit für einen Request in einem eigenen Thread erledigen. Wir könnten einfach einen Thread pro Request starten , synchrone APIs verwenden. Aber es geht nicht, weil einfach zu wenige Threads verfügbar sind. Asynchrone Programmierung als Notlösung Als Konsequenz opfern wir den Java-Sprachkern und verwenden reaktive Bibliotheken. Und müssen uns für Konstrukte wie Schleifen, If und Try-Catch komplett neue Konstrukte einfallen lassen. CompletableFuture .supplyAsync(info::getUrl, pool) .thenCompose(url -&gt; getBodyAsync( pool, HttpResponse.BodySubscribers.ofString(UTF_8))) .thenApply(info::findImage) .thenCompose(url -&gt; getBodyAsync( pool, HttpResponse.BodySubscribers.ofByteArray())) .thenApply(info::setImageData) .thenAccept(this::process) .exceptionally(t -&gt; { t.printStackTrace(); return null; }); Ohne auf den konkreten Inhalt dieses Handlers einzugehen, lässt sich die Auswirkung auf die Struktur der Programmiersprache erkennen: Das Programm wird nicht mehr in der üblichen Weise strukturiert, sondern über eine \"Fluent API\" erstellt und gestartet. Im Kern stellt das eine Monade dar, wie sie zum Beispiel aus Haskell bekannt ist. Dieses neue Sprachkonstrukt hat eine Reihe von Folgen, die interessant zu nutzen sind. Mit all den Problemen, die daraus resultieren, dass jetzt JVM, Werkzeuge, Sprache und Tools nicht mehr so recht zusammenpassen wollen: In Stack Traces steht oft kaum noch Hilfreiches . Mit dem Debugger durch ein reaktives Programm zu steppen ist eine Herausforderung. Und die Ursache für Lastprobleme zu finden, ist problematisch. Diesen Programmierstil verwenden wir definitiv nicht, weil er einfacher zu verstehen wäre. Oder weil er sonst irgendwie nützlicher zu handhaben wäre. Wir verwenden diesen Programmierstil, weil wir nicht anders skalieren können. Projekt Loom als Rettung Die Idee hinter Projekt Loom: Threads müssen wieder so billig werden wie damals. Es darf kein Problem sein, Millionen davon zu starten. Die JVM mappt dazu ihre eigene Art von Threads, die dort VirtualThreads heißen, auf Betriebssystem-Threads. Das ist ein M:N-Mapping. Also anders als damals zu Solaris-Zeiten, als „Green Threads“ eben nur auf einen einzigen OS-Thread abgebildet werden konnten. Aber ziemlich so, wie es in Erlang schon immer war. Und auch die Go-Fans lachten ja bereits über uns Java-Menschen. Die JVM kann das deswegen besser als das Betriebssystem, weil es zum einen mehr Wissen besitzt (zum Beispiel über Stack-Größen und das Speichermodell) und zum anderen, weil es Threads nicht jederzeit unterbrechen kann. Stattdessen wird nur dort unterbrochen, wo es blockierende Operationen gibt. Das sind hauptsächlich IO-Operationen, aber auch dort, wo wir in unseren Programmen manuell synchronisieren. Damit das funktioniert, gab es im Rahmen des Projekts Loom Anpassungen quer durch die JVM und die Basis-Bibliotheken. NIO wurde umgebaut. Das „alte“ IO wurde angepasst (und darf und soll damit ruhig wieder benutzt werden). Nur File-IO unter Windows ist noch ein Problem und dauert noch. VirtualThreads: benutzen ist (fast) einfacher als vermeiden Seit Java 19 können wir Threads sehr einfach als „virtual“ starten: var thread = Thread.startVirtualThread(() -&gt; { ... }); Das ist schon alles. Die JVM kümmert sich darum, dass diese VirtualThreads automatisch auf OS-Threads abgebildet werden. Normalerweise auf einen pro CPU-Kern. In diesem VirtualThread lassen sich nach Herzens Lust blockierende Aufrufe, Locks und Sleeps in synchroner Art platzieren. Wir sollen uns keine Gedanken mehr darüber machen, wie der Wettstreit um die Ressourcen läuft. Anpassungen im eigenen Code Einige Code-Konstrukte spielen nicht so gut mit VirtualThreads zusammen. Wir können sie ersetzen, damit der Code noch besser skaliert. Ganz weit vorne ist (jedenfalls derzeit) noch der „synchronized“-Block. Der hängt immer an einem OS-Thread, weil er mit Betriebssystemmitteln implementiert ist. Wir wollen ihn mit „ReentrantLock“ oder noch besser mit „StampedLock“ ersetzen. Der zweite Bereich sind JNI-Aufrufe. Die sind immer dann problematisch, wenn sie innerhalb von „synchronized“ passieren. Vor allem, wenn wir von nativem Code wieder nach Java callen, zum Beispiel bei Callbacks. Alles das muss uns aber nicht aufhalten. In den meisten Fällen machen ein paar wenige solche Stellen wenig aus. Viele Frameworks integrieren VirtualThreads bereits In Spring Boot Projekten werden wir bereits dahin geführt, dass wir Threading an zentraler Stelle implementieren. So wie Spring Boot es intern auch bereits macht. Wir können heute schon dafür sorgen, dass Spring Boot auf VirtualThreads setzt: @Configuration class ConfigureVirtualThreads { @Bean(TaskExecutionAutoConfiguration.APPLICATION_TASK_EXECUTOR_BEAN_NAME) public AsyncTaskExecutor asyncTaskExecutor() { return new TaskExecutorAdapter( Executors.newVirtualThreadPerTaskExecutor()); } @Bean public TomcatProtocolHandlerCustomizer&lt;?&gt; protocolHandlerVirtualThreadExecutorCustomizer() { return protocolHandler -&gt; { protocolHandler.setExecutor( Executors.newVirtualThreadPerTaskExecutor()); }; } } Mit der ersten Deklaration wird Spring konfiguriert. Der neue Task-Executor, den Spring an verschiedenen Stellen für asynchrone Aufrufe nutzt, erhält dafür jeweils einen neuen VirtualThread, statt wie vorher einen Thread-Pool. Die zweite Deklaration konfiguriert den eingebetteten Tomcat, mit dem Spring Boot die Web-Anfragen bearbeitet. Hier ist normalerweise ebenfalls ein Threadpool hinterlegt. Mit der Konfiguration fällt dieser Pool weg und es wird jedes Mal ein neuer VirtualThread zur Bearbeitung angelegt. Das als Configuration eingefügt und schon kommen Servlet-Requests bereits fertig als VirtualThread an. Spring Boot hat VirtualThreads auf dem Schirm, passt immer mal wieder etwas an und ist schon recht weit damit, VirtualThreads sehr effizient zu nutzen. Micronaut hat ebenfalls schon Support vorbereitet , der getestet werden kann. Und für Quarkus gibt es schon sehr weitreichenden Support . Und sogar in Wildfly 27 lässt sich VirtualThread-Support aktivieren. Angewohnheiten hinterfragen Mit Projekt Loom müssen wir fast nie neue Konzepte lernen. Stattdessen können wir alte Gewohnheiten ablegen: ThreadPools werden in den meisten Fällen keinen Mehrwert mehr bieten. Im Gegenteil fügen sie Overhead hinzu und verlangsamen den eigenen Code . Wo wir bisher Poolen, zum Beispiel um die Anzahl gleichzeitig durchgeführter Requests zu limitieren, können wir wieder (wie früher) Semaphoren beim Funktionsaufruf nutzen. Synchron war nie schlecht Und dann natürlich die Erkenntnis: für 99&#160;% aller Applikationen da draußen war asynchrone Programmierung nie nötig. Auch nicht ohne Projekt Loom. Die wenigsten haben mehr als 30.000 gleichzeitige Requests pro Service-Instanz. Moderne Hardware hat damit kein Problem, auch nicht mit 30k Betriebssystem-Threads. Und weil die Stack-Größe nur virtuellen Speicher angibt, haben wir auf 64-Bit-Systemen kein Problem damit. Ausblick: Structured Concurrency Bis mit Java 21 im Herbst 2023 das nächste LTS-Release aufschlägt, soll auch Structured Concurrency mit aufgenommen sein. Damit lassen sich dann die Stellen übersichtlich angehen, bei denen innerhalb einer Aufgabe Anfragen und Berechnungen parallel erfolgen sollen. @GetMapping(\"/trains\") fun listTrainsParallel(): TrainList&lt;TrainRepresentation&gt; { val list = StructuredTaskScope.ShutdownOnSuccess&lt;List&lt;Train&gt;&gt;().use { scope -&gt; scope.fork { serverA.listActiveSync() } scope.fork { serverB.listActiveSync() } scope.join().result().map { it.toListRepresentation() } } val count = StructuredTaskScope.ShutdownOnSuccess&lt;Int&gt;().use { scope -&gt; scope.fork { serverA.countActiveSync() } scope.fork { serverB.countActiveSync() } scope.joinUntil(Instant.now().plusSeconds(15)).result() } return TrainList(list, count) } Bei den beiden Abfragen können wir einfach (übrigens wieder als Monade) deklarieren, dass die dahinter liegenden Abfragen in separaten Threads erfolgen - im besten Fall in VirtualThreads. \"ShutdownOnSuccess\" sorgt dafür, dass das erste verfügbare Ergebnis gewinnt und alle anderen Threads beendet werden. Wir können einen Timeout mitgeben, um die Laufzeit - hier auf 15 Sekunden - zu begrenzen. Dabei ist wichtig: Es geht bei Structured Concurrency wirklich fast nur um die Lesbarkeit und Wartbarkeit. Schneller oder Ressourcen-sparender wird es dadurch nicht. Also: Es wird spannend im Java-Ökosystem. Mit Projekt Loom werden tatsächlich die Karten neu gemischt. Endlich können wir den Programmierstil wieder so aussuchen, wie er zu unseren Gehirnen passt. "
},

{
    "id": 19,
    "uri": "blog/2023/2023-04-09-vortrag-auf-der-javaland.html",
    "menu": "Blog",
    "title": "JavaLand 2023",
    "text": " Table of Contents Vortrag auf der JavaLand 2023 Slides Vortrag auf der JavaLand 2023 Vom 20. bis 23. März fand mit der JavaLand die große Java-Community-Konferenz im Phantasialand Brühl statt - und DB Systel war mit dabei. In dem Talk \"Fantastische Diagramme und wie Du sie selbst erstellst\" haben Falk Sippach (embarc) und Ralf D. Müller (DB Systel) gemeinsam Schwachstellen von Architekturdiagrammen aufgespürt und ausgemerzt. In den Slides des Vortrags (siehe Download-Link unten) finden sich als Ergebnis ein paar Tipps und Checklisten, die jeder für seine eigenen Diagramme verwenden kann. Abends gab es dann noch einen Fun-Workshop \"Hacking the RP2040\", bei dem wir uns angesehen haben, was man so alles mit dem Tufty-Badge von Pimoroni anstellen kann. Dabei handelt es sich um ein kleines 320x240 Display mit Tastern und RP2040 Microcontroller (besser bekannt als Raspberry Pi Pico) in der Form eines Badges zum Umhängen. Das eigentliche Highlight der jährlichen JavaLand war aber die Community mit ihren vielen Hallway-Tracks und dem Newcomer-Programm \"NextGen\" über das immer wieder frische Themen und Sichtweisen in das Programm aufgenommen werden. Slides Slides des Workshops "
},

{
    "id": 20,
    "uri": "blog/2023/2023-04-01-Indoor-GIS-zur-Rationalisierung-von-Wartungsarbeiten.html",
    "menu": "Blog",
    "title": "Indoor-GIS",
    "text": " Table of Contents Indoor-GIS zur Rationalisierung von Wartungsarbeiten Slides Indoor-GIS zur Rationalisierung von Wartungsarbeiten Genial kombiniert: Mit Tracking-Technologie und unserer Esri-Plattform Kosten und Durchlaufzeiten reduzieren. Durch die Kombination von Tracking-Technologie und unserer Esri-Plattform nutzen wir die modernsten Tracking-Systeme. Auf diese Weise reduzieren wir Kosten und Durchlaufzeiten. Eine Erfolgsgeschichte in der Arbeitswelt. Erfahren Sie, wie die offene Plattformarchitektur viele Anwendungsfälle ermöglicht. Beispiele finden Sie in der beigefügten Präsentation. Sie wurde im Rahmen eines Vortrags von Konrad Winkler &amp; Philippe Rieffel im Rahmen der Esri International Infrastructure Management &amp; GIS Conference am 18.04.2023 in Frankfurt gezeigt. Hinweis: Die Präsentation liegt in englischer Sprache vor. Slides download "
},

{
    "id": 21,
    "uri": "blog/2023/2023-03-28-ChatGPT-Einblicke-und-mehr-Generative-Sprachmodelle-Herausforderungen-und-Chancen.html",
    "menu": "Blog",
    "title": "ChatGPT",
    "text": " Table of Contents ChatGPT – Funktionsweise, Chancen, Risiken und Alternativen einfach erklärt Video ChatGPT – Funktionsweise, Chancen, Risiken und Alternativen einfach erklärt In diesem Video erkläre ich leichtverständlich die Funktionsweise von ChatGPT im Zusammenhang mit generativer Künstlicher Intelligenz (Generative AI) und großen Sprachmodellen (Large Language Models, kurz LLMs). Anhand von Anwendungsfällen beschreibe ich Chancen, Risiken und Alternativen und diskutiere, was das für uns und unsere Jobs zukünftig bedeuten könnte.  Video "
},

{
    "id": 22,
    "uri": "blog/2023/2023-03-08-Re-Platforming-Mainframe-Mehr-als-nur-Lift-Shift.html",
    "menu": "Blog",
    "title": "Re-Platforming Mainframe",
    "text": " Table of Contents Re-Platforming Mainframe – Mehr als nur Lift&amp;Shift Slides Re-Platforming Mainframe – Mehr als nur Lift&amp;Shift Die sinkende Bedeutung der Mainframe-Plattform führt uns zu unserem Projektziel Lift&amp;Shift. Die Migration in die Cloud soll mit möglichst wenigen Anpassungen erfolgen. Dabei wollen wir die IBM Mainframe basierten Cobol Anwendungen durch schrittweise Überführung der Anwendungen in die DB Enterprise Cloud ablösen. Die Schaffung einer zukunftsorientierten IT-Architektur und signifikante Einsparungen von Betriebskosten durch die Nutzung cloudbasierter Infrastruktur sind ebenso wichtig. Lesen Sie in unserer Präsentation, welche Projektziele wir außerdem verfolgen, wer unsere Partner sind und wie unsere Bestandsanalyse aussieht. Gezeigt wurde sie im Rahmen eines Vortrags von Tim Engeleiter am 08.03.2023 bei den Mainframe Dayz in Wiesbaden.   Slides download "
},

{
    "id": 23,
    "uri": "blog/2022/2022-15-03-good-practices-api.html",
    "menu": "Blog",
    "title": "Good Practices im API-Umfeld",
    "text": " Table of Contents Hätt' ich das früher gewusst - Good Practices bei API-Konzeption &amp; -Entwicklung Slides und Video Hätt' ich das früher gewusst - Good Practices bei API-Konzeption &amp; -Entwicklung Wenn du diese API noch einmal konzipieren könntest, würdest du alles noch mal genauso machen? Nicht ganz, ich würde von Anfang an &#8230;&#8203; ja, was eigentlich? Im Vortrag ziehe ich eine Zwischenbilanz aus über drei Jahren API-Entwicklung bei der DB Systel GmbH, indem ich unsere Vorgehensweisen bei API-Design und -Implementierung analysiere und praktische Ratschläge daraus ableite. Unser Vorhaben war, viele APIs für generische Aufgaben (wie Bezahlung oder Routing) und Daten (wie von Bahnhöfen oder Sharing-Fahrzeugen) zentral bereitzustellen. Doch wieso erwies sich das in vielen dieser Fälle als ungeeignet? Das Einhalten von Paradigmen wie API-first und REST stand anfangs im Fokus aller Produkte. Aber warum ist API-first gar nicht immer optimal? Und wieso können wir heute mit imperfekten REST-APIs ruhig schlafen? Mittlerweile nutzen wir Tools und Frameworks wie den OpenAPI-Generator, MapStruct, Lombok und OpenFeign. Was hat uns anfangs davon abgehalten? Slides und Video "
},

{
    "id": 24,
    "uri": "blog/2022/2022-11-24-the-journey-towards-K8s-at-Deutsche-Bahn.html",
    "menu": "Blog",
    "title": "K8s at Deutsche Bahn",
    "text": " Table of Contents The journey towards K8s at Deutsche Bahn Video The journey towards K8s at Deutsche Bahn This lecture was given by Gualter Barbas Baptista from DB Systel at the Containerdays from 05 to 07 September 2022. ( https://www.containerdays.io/ ) In 2016, Deutsche Bahn decided to get rid of their own data centers and to migrate the majority of applications to the cloud. The cloudification of Deutsche Bahn was supported by a comprehensive transformation of its digital partner DB Systel from traditional working and organisational structures to self-organisation and company-wide networks, including a DevOps culture. Within this context, the first Kubernetes platform services emerge. From an OpenShift-based Kubernetes-Namespace-as-a-Service into a GitOps based K8s fleet management, we describe how the cloud, the organisational transformation and the CNCF landscape are accelerating the digitalisation of Deutsche Bahn. Video "
},

{
    "id": 25,
    "uri": "blog/2022/2022-11-04-Produkt-statt-Projekmanagement.html",
    "menu": "Blog",
    "title": "Produkt- statt Projektmanagement",
    "text": " Table of Contents Produkt- statt Projektmanagement Video Produkt- statt Projektmanagement Warum das Funding von Projekten mittlerweile zu einem der größten Hindernisse auf dem Weg zu einer BizDevOps-Organisation geworden ist − ein Debattenbeitrag zum nächsten notwendigen Schritt der DevOps-Bewegung. Die DevOps-Bewegung ist in der Realität und im Herzen der digitalen Industrie angekommen. In der Systel arbeiten knapp 100 Teams nach dem Prinzip „Your build (or integrate) it, you run it“ und sind so in der Lage, regelmäßig und mit hoher Frequenz Änderungen und damit „Business Value“ zu den Nutzerinnen und Nutzern zu bringen. Nicht nur bei uns, sondern in allen vergleichbaren großen Konzernen sind Cloud-Nutzung, Automatisierung, Continuous Delivery, Platform Strategy und der dafür notwendige kulturelle Wandel auf der Tagesordnung. Ein großer Hebel für den Wandel zu einer DevOps-Organisation wird allerdings häufig übersehen: das etablierte Funding von digitalen Maßnahmen über Projekte ist mittlerweile zu einem der Haupt-Hindernisse auf dem Weg zu einer leistungsfähigen Delivery-Organisation geworden. Aus meiner Sicht müssen wir daher ganz vorne mit der Veränderung ansetzen, nämlich dort, wo Projekte entstehen, und über Projektbudgets entschieden wird. Warum ich dieser Meinung bin, habe ich auf einem Vortrag beim DevOps Enterprise Summit dargelegt, der mittlerweile auch über den YouTube Kanal der DB Systel zugänglich ist. Video "
},

{
    "id": 26,
    "uri": "blog/2022/2022-10-21-Deine-Diagramme-sind-Legende.html",
    "menu": "Blog",
    "title": "Deine Diagramme sind Legende?",
    "text": " Table of Contents Deine PlantUML-Diagramme sind Legende? pre { white-space: pre-wrap; } table.tableblock { overflow: auto; width: 100%;} td.tableblock {overflow: auto; width: 50%;} Deine PlantUML-Diagramme sind Legende? &#8230;&#8203;dann verpasse ihnen eine Legende! Ein Diagramm soll nicht nur für Insider lesbar sein. Mit einer Legende erklärst du die verwendeten Symbole und Farben. In diesem Artikel zeige ich dir, wie es geht. PlantUML verfügt über ein wenig dokumentiertes Element namens \" Legend \". Damit lässt sich eine Box im Diagramm z. B. in der rechten unteren Ecke platzieren. Wie aber der Inhalt dargestellt werden soll ist unklar. @startuml skinparam actorStyle awesome database Datenbank :User: -&gt; [Komponente] [Komponente] -&gt; Datenbank #green legend right &lt;b&gt;Legende&lt;/b&gt; ??? endlegend @enduml Google findet als Idee, dass die Legende als Tabelle in Creole-Syntax erstellt werden kann. Farben kann man damit gut erklären, aber für Symbole können nur Emojis oder spezielle Zeichen verwendet werden. @startuml skinparam actorStyle awesome database Datenbank :User: -&gt; [Komponente] [Komponente] -&gt; Datenbank #green legend right &lt;b&gt;Legende&lt;/b&gt; | &lt;#red&gt; | Benutzer-Zugriff | | &lt;#green&gt; | Datenbank-Verbindung | | &lt;:smiley:&gt; | Benutzer :-) | endlegend @enduml In einem Forum habe ich am Rande den Hinweis gefunden, dass man mit dem Map-Statement des Objektdiagramms auch eine Tabelle aufbauen kann. Nur geht das nicht direkt innerhalb der Legende. Es gibt aber den Trick, dass man mit der `{{ &#8230;&#8203; }} Syntax ein neues Diagramm innerhalb des Diagramms erstellen kann. Damit lässt sich dann auch eine Map innerhalb der Legende aufbauen. @startuml skinparam actorStyle awesome database Datenbank :User: -&gt; [Komponente] [Komponente] -&gt; Datenbank #green legend right {{ map \"&lt;b&gt;Legende&lt;/b&gt;\" as legend #white { &lt;#red&gt; =&gt; Benutzer-Zugriff &lt;#green&gt; =&gt; Datenbank-Verbindung &lt;:smiley:&gt; =&gt; Benutzer :-) } }} endlegend @enduml Und wenn wir jetzt schon dabei sind Diagramme innerhalb von Diagrammen zu nutzen, dann können wir das auch noch eine Ebene tiefer machen. Dadurch schaffen wir es in der Legende die Diagramm-Elemente zu zeichnen, die wir beschreiben wollen. Dazu bauen wir uns in einer Prozedur ein universelles Mini-Diagramm: scale $scale skinparam backgroundcolor transparent label \" \" as A label \" \" as B $type Der scale-Befehl erlaubt es die zu beschreibende Komponente kleiner darzustellen und somit die Legende kompakt zu halten. Die beiden unsichtbaren Labels sorgen dafür, dass wir einen Connector von A nach B darstellen können. Das ganze sieht dann kompakt wie folgt aus: @startuml skinparam actorStyle awesome database Datenbank :User: -&gt; [Komponente] [Komponente] -&gt; Datenbank #green legend right {{ !procedure $entry($type, $label, $scale=1) {{\\nscale $scale \\nskinparam backgroundcolor transparent\\nlabel \" \" as A\\nlabel \" \" as B\\n $type \\n}} =&gt; $label !endprocedure map \"&lt;b&gt;Legende&lt;/b&gt;\" as legend #white { $entry(\":Actor:\",\" Benutzer\", 0.5) $entry(\"[component]\",\" Benutzer\", 0.7) $entry(\"database db\",\"Datenbank\", 0.7) $entry(\"A -&gt; B\",\"Benutzer-Zugriff\") $entry(\"A -&gt; B #green\",\"Datenbank-Verbindung\") } }} endlegend @enduml Im letzten Schritt möchte ich die Legende mit ein paar Styles noch aufhübschen. Der doppelte Rahmen soll weg und etwas kleiner wäre auch nicht schlecht. @startuml skinparam actorStyle awesome skinparam legendBackgroundColor transparent skinparam legendBorderColor transparent database Datenbank :User: -&gt; [Komponente] [Komponente] -&gt; Datenbank #green legend right {{ scale 0.8 skinparam defaultFontSize 14 skinparam BackGroundColor transparent skinparam defaultBackgroundColor white !procedure $entry($type, $label, $scale=1) {{\\nscale $scale \\nskinparam backgroundcolor transparent\\nlabel \" \" as A\\nlabel \" \" as B\\n $type \\n}} =&gt; $label !endprocedure map \"&lt;b&gt;Legende&lt;/b&gt;\" as legend #white { $entry(\":Actor: #green\",\"\\nBenutzer\", 0.5) $entry(\"[component]\",\"\\nBenutzer\", 0.7) $entry(\"database db\",\"\\nDatenbank\", 0.7) $entry(\"A -&gt; B\",\"Benutzer-Zugriff\") $entry(\"A -&gt; B\",\"Datenbank-Verbindung\") } }} endlegend @enduml Bei der Nutzung fällt schnell auf, dass die Legende zu viel Platz einnimmt. Sie duldet keine anderen Diagramm-Elemente neben sich. Also haben wir weiter geforscht. Mit dem Diagramm in der Legende besteht eigentlich kein Grund mehr wirklich das Element Legend zu verwenden. Was passiert, wenn wir es durch eine rectangle ersetzen und diese entsprechend Stylen? Dazu müssen wir dem Element einen Stereotype verpassen, da wir sonst alle rectangle -Elemente stylen würden. Und siehe da, es funktioniert. Durch diesen Trick haben wir nun mehr Einfluss auf die Platzierung, denn wir können dieses rectangle -Element durch versteckte Verbindungen beeinflussen. @startuml skinparam actorStyle awesome database Datenbank :User: -&gt; [Komponente] [Komponente] -down-&gt; Datenbank #green rectangle a &lt;&lt;test&gt;&gt; Datenbank -left-&gt; a skinparam rectangle&lt;&lt;legend&gt;&gt; { backgroundColor transparent borderColor transparent shadowing false } hide &lt;&lt;legend&gt;&gt; stereotype rectangle legende &lt;&lt;legend&gt;&gt; [ {{ scale 0.8 skinparam defaultFontSize 14 skinparam BackGroundColor transparent skinparam defaultBackgroundColor white !procedure $entry($type, $label, $scale=1) {{\\nscale $scale \\nskinparam backgroundcolor transparent\\nlabel \" \" as A\\nlabel \" \" as B\\n $type \\n}} =&gt; $label !endprocedure map \"&lt;b&gt;Legende&lt;/b&gt;\" as legend #white { $entry(\":Actor:\",\"\\nBenutzer\", 0.5) $entry(\"[component]\",\"\\nBenutzer\", 0.7) $entry(\"database db\",\"\\nDatenbank\", 0.7) $entry(\"A -&gt; B\",\"Benutzer-Zugriff\") $entry(\"A -&gt; B #green\",\"Datenbank-Verbindung\") } }} ] User -[hidden]-&gt; legende legende -[hidden]down-&gt; a @enduml Übrigens: PlantUML möchte Elemente und ihre Verbindungen immer optimiert platzieren. Es kann also sein, dass die neue Legende deshalb noch mal kräftig durchmischt. Es gibt aber nicht nur die Pfeildefinition -[hidden]&#8594; , um eine Verbindung nicht anzuzeigen. Der Pfeil -[norank]&#8594; ist eine Verbindung, welche bei besagter Optimierung ignoriert wird. Beide Features kann man kombinieren: Mit einem -[norank,hidden]&#8594; ist die Legende unsichtbar mit einem anderen Element verbunden, ohne dass dies das Diagramm umstrukturiert. "
},

{
    "id": 27,
    "uri": "blog/2022/2022-03-23-Vielfalt-bei-der-Bahn-Computerlinguistinnen-treiben-Digitalisierung-voran.html",
    "menu": "Blog",
    "title": "Vielfalt bei der Bahn",
    "text": " Table of Contents Vielfalt bei der Bahn: Computerlinguist:innen treiben Digitalisierung voran Video Vielfalt bei der Bahn: Computerlinguist:innen treiben Digitalisierung voran Dieses Mal haben wir Claudia Schönfelder aus dem Team SALT zu Gast, mit der wir über das Thema “SINUS”, den Sprachassistenten für die Instandhaltung von Zügen, und den Beruf der Computerlinguist:innen, sprechen. Claudia entwickelt multidisziplinär mit mehreren Teams von Expert:innen einen der ersten Sprachassistenten für die industrielle Nutzung. Wir alle kennen Siri und Alexa, aber diese bewältigen in der Regel Aufgaben des Alltags. Im Gegensatz dazu führt SINUS die Konzernpartner:innen aus der Instandhaltung per Sprache durch digitalisierte Formulare, um die Eingabe von Schäden an Loks gänzlich über die Spracherkennung zu tätigen. Im Interview spricht sie mit uns über dieses ehrgeizige Projekt, das die Effizienz bei der Befundung von Zügen deutlich steigern kann. Wir spielen anhand eines Beispiels durch, welche Besonderheiten dieses Tool bei der Befundung besitzt. Dazu erklärt Claudia wie sich das Berufsbild der Computerlinguisten in den letzten Jahren entwickelt und etabliert hat. Information, die auch für die jüngeren unter uns, von großem Interesse sein kann.   Video "
},

{
    "id": 28,
    "uri": "blog/2021/2021-11-02-KITT-das-Kuenstliche-Intelligenz-Translation-Tool.html",
    "menu": "Blog",
    "title": "KITT Tool",
    "text": " Table of Contents KITT, das Künstliche Intelligenz Translation Tool Video KITT, das Künstliche Intelligenz Translation Tool Dieses Mal haben wir Pia Schwarz aus dem Team SALT zu Gast, mit der wir über das Thema “KITT”, das Künstliche Intelligenz Translation Tool, sprechen. Pia bringt mit ihrem Team Maschinen das Sprechen bei. Im Interview spricht sie mit uns über KITT, das als Interface gesprochene Sprache entgegennimmt. Es wird eingesetzt, um die Kommunikation im Zugfunk von Fahrdienstleiter:innen und Triebfahrzeugführer:innen im Grenzbereich von Frankreich und Deutschland zu verbessern. Wir spielen anhand eines Beispiels durch, welche Besonderheiten das Bahnjargon mit sich bringt und warum am Markt verfügbare Übersetzungstool in diesem Anwendungsfall nicht ausreichend sind. KITT hilft ebenso die Anforderungen an das Sprachniveau zu senken, in dem es sogenannte Predefined Messages zuverlässig übersetzt, die im europäischen Bahnverkehr verwendet werden. In der zweiten Hälfte des Interviews steigen wir tiefer in die Technik ein und gehen auf die Herausforderungen ein. Denn die Anforderungen an KITT sind nicht weniger, als exakte Übersetzungen zu produzieren. Zum Beispiel sind Übersetzungen bei Hintergrundgeräuschen oder Sätze mit Eigennamen problematisch. Zum Abschluss sprechen wir über die technische Umsetzung, Trainingsdatenmenge und Open Source Frameworks. Video "
},

{
    "id": 29,
    "uri": "blog/2021/2021-06-08-Bad-bots-Chancen-und-Herausforderungen-fuer-KI-und-Sprache.html",
    "menu": "Blog",
    "title": "Bad Bots",
    "text": " Table of Contents Bad Bots - Chancen und Herausforderungen für KI und Sprache Video Bad Bots - Chancen und Herausforderungen für KI und Sprache Sascha Wolter, Chief Advisor Conversational AI bei der DB Systel GmbH, hielt diesen Vortrag am 06.05.2021 im Rahmen der Digital Office Conference 2021 von Bitkom. Kaum jemand scheint seine Alexa nach mehr als dem Wetter oder schlechten Witzen zu fragen. Und vier der fünf erfolgreichsten Alexa Skills erzeugen nicht mehr als Pupsgeräusche. Dies steht in einem krassen Widerspruch zu den Berichten über künstliche Intelligenz in den Medien, denen zufolge weltbeherrschende Computer wie Skynet (Terminator) unmittelbar bevorstehen. Und selbst die, die den technische Fortschritt weniger als Bedrohung denn als Chance verstehen, haben oft eine völlig falsche Vorstellung von den Möglichkeiten von #Chatbots und #Sprachassistenten. Nicht selten wird davon ausgegangen, dass – sofern man ausreichend Daten hat – die #KI alles automatisch erledigt und sich der Erfolg mehr oder weniger von ganz allein einstellt. Dies ist offensichtlich noch nicht so, wie die vielen schlechten Bots im Markt eindrucksvoll zeigen. Doch wie man diesem Ziel zumindest möglichst nahe kommt, zeigt Sascha Wolter anhand zahlreicher praktischer Beispiele. Er behandelt nicht nur die Hintergründe, sondern zeigt auch technische und gestalterische Lösungen.  Video "
},

{
    "id": 30,
    "uri": "blog/2021/2021-04-12-Computer-Vision-Use-Cases-at-Deutsche-Bahn.html",
    "menu": "Blog",
    "title": "Computer Vision Use Cases",
    "text": " Table of Contents Computer Vision Use Cases @ Deutsche Bahn Video Computer Vision Use Cases @ Deutsche Bahn Wie können KI-Bildanalysen bei der Graffiti-Erkennung helfen und welche Potentiale birgt das für die Bahn? Im Rahmen des Data Festivals 2021 präsentierte das DB Systel Venture vsion.ai gemeinsam mit der Data Science Beratung Alexander Thamm Einsatzmöglichkeiten von KI-Analysen auf Bildern im Bahnkontext. Am Beispiel der automatischen Erkennung von Graffiti auf Zügen zeigt Peco Elenchevski die Besonderheiten des Use Cases auf sowie die technische Umsetzung eines Proof of Concept. Nico Becker knüpft dort an und beschreibt die Herausforderungen, welche sich aus dem Deployment von KI-Modellen ergeben. Dabei skizziert er einen Weg, wie sich ein Proof of Concept zu einem robusten Produktivsystem weiterentwickeln lässt. Der Vortrag wurde vor internationalem Publikum gehalten und ist daher auf Englisch. Video "
},

{
    "id": 31,
    "uri": "blog/2021/2021-03-20-Die-C4-Testpyramide-eine-architekturgetriebene-Teststrategie.html",
    "menu": "Blog",
    "title": "Die C4-Testpyramide",
    "text": " Table of Contents Die C4-Testpyramide - eine architekturgetriebene Teststrategie Video Die C4-Testpyramide - eine architekturgetriebene Teststrategie Die Präsentation zeigt, wie sich die Prinzipien der Test Pyramide mit dem C4 Modell zur Visualisierung von Software Architekturen verbinden lassen, um so auf einfache Weise zu einer produktspezifischen Teststrategie zu gelangen. Video "
},

{
    "id": 32,
    "uri": "blog/2020/2020-12-07-devops-mehr-geschwindigkeit-auf-der-schiene.html",
    "menu": "Blog",
    "title": "DevOps Geschwindigkeit",
    "text": " Table of Contents DevOps - Mehr Geschwindigkeit auf der Schiene Slides und Video DevOps - Mehr Geschwindigkeit auf der Schiene In diesem Vortrag im Rahmen der IT-Tage 2020 erklärt Carsten Hoffmann von der DB Systel, warum sich der erste Ansatz einer zentralen CI/CD-Installation im Projekt, eine Cloud-native Plattform für API-getriebene Softwareentwicklung aufzubauen, für alle Teams als problematisch erwies und durch dezentrale Pipelines ersetzt wurde. Danach werden die Hindernisse bei der Einführung einer eingekauften API-Management-Lösung erklärt und wieso sich der Einkauf von großer On-Premise-Software nur schwierig mit den agilen Prinzipien vereinbaren lässt. Außerdem wird erläutert, wie im Team mit polyglotter Softwareentwicklung und permanent gegen Wissensinseln angekämpft wurde. Zuletzt geht Casten Hoffmann darauf ein, wie das Team mit umfassender Architekturdokumentation begonnen hat und gescheitert ist. Slides und Video "
},

{
    "id": 33,
    "uri": "blog/2020/2020-05-19-5vue-js-vs-angular-was-ist-besser.html",
    "menu": "Blog",
    "title": "Vue.js vs. Angular",
    "text": " Table of Contents Vue.js vs. Angular: Was ist besser? Video Vue.js vs. Angular: Was ist besser? Heute zu Gast bei #000000 #c0ffee – Der Tech-Talk der DB Systel. Von Techies für Techies: Danny Koppenhagen Danny ist Frontend-Entwickler mit den Schwerpunkten Angular und Vue.js und einer der Autoren des Buches „Angular - Grundlagen, fortgeschrittene Themen und Best Practices“. Im dx.house Berlin berät er außerdem Kunden und Teams in den Themen User Experience von Enterprise-Lösungen. Er engagiert sich in der Web Community der DB Systel und ist Mitglied im Themen Team Web der Architekturgilde. Dort erarbeitet er Architektur-Standards für alle Themen Web. Im Interview spricht er über seine Erfahrungen mit Vue.js und Angular. Er geht darauf ein, welches Framework sich für welche Anwendungszwecke eignet. So bietet Vue.js hauptsächlich Vorteile, wenn es um die Integration in bestehende Anwendungen handelt und das Team gerne JavaScript einsetzt. Angular ist im Enterprise-Umfeld für neue Anwendungen interessant, da es auf TypeScript aufsetzt, ein umfangreiches Ökosystem mitbringt und zum Beispiel Migrationsguides und Templating-Fähigkeiten über sogenannte Schematics mitbringt. Außerdem erläutert Danny wie der aktuelle Stand der Technik für Progressive Webapps (PWA) ist. Hier kommt es darauf an, ob alle benötigten Features des Betriebssystems angesprochen werden können. Falls nicht, sollte in Erwägung gezogen werden eine native App zu entwickeln. Im dritten Teil sprechen wir über die Anbindung von APIs. Um die Orchestrierung von APIs zu vereinfachen, kann hier das Architekturmuster Backend For Frontends zum Einsatz kommen. Das vereinfacht den Zugriff aus der Anwendung, da die Anbindung der APIs nicht einzeln im Frontend implementiert werden muss. Video "
},

{
    "id": 34,
    "uri": "blog/2020/2020-03-27-DB-Systel-streitet-auf-der-OOP-fuer-guten-Code.html",
    "menu": "Blog",
    "title": "OOP: Guter Code",
    "text": " Table of Contents DB Systel streitet auf der OOP für guten Code Video DB Systel streitet auf der OOP für guten Code In diesem unterhaltsamen Pecha Kucha Vortrag spricht Carsten Thurau über seine Erfahrung aus dem Alltag als erfahrener Trainer und Coach. Er zeigt in seinem Kurzvortrag, warum schlechter Code Entwickler unglücklich macht und Code Reviews, TDD und Clean Code eine gute Idee sind. Fazit: Entwickelt qualitativ hochwertigen Code und seid stolz auf Euer Werk! Der Talk wurde auf der OOP 2020 Konferenz in München Anfang Februar 2020 aufgezeichnet. Video "
},

{
    "id": 35,
    "uri": "blog/2020/2020-03-14-API-first-mit-TypeScript.html",
    "menu": "Blog",
    "title": "API first mit TS",
    "text": " Table of Contents API first mit TypeScript API first mit TypeScript Mit API first kann man sehr schön REST APIs bauen. Verwendet man TypeScript, kann man mittels der Bibliothek express-openapi nicht nur einmalig einmalig ein Interface generieren, sondern auch bei späteren Änderungen API first beibehalten. Im Video zeige ich anhand eines praktischen Beispiels, wie man ein solches Projekt aufsetzt. In 6 Schritten setzen wir ein TypeScript Projekt mit express.js und express-openapi auf. Es bringt ein Swagger UI und Unit-Tests mit und lässt sich - natürlich ebenso API first wie am Anfang - leicht weiter entwickeln. Die Commits im Repository erklären, wie man schrittweise einen solchen REST Service aufbaut. "
},

{
    "id": 36,
    "uri": "blog/2019/2019-09-13-Spock-und-AsciiDoc.html",
    "menu": "Blog",
    "title": "Spock und AsciiDoc",
    "text": " Table of Contents Spock und AsciiDoc - vom Test zur Spezifikation und zurück Slides und Video Spock und AsciiDoc - vom Test zur Spezifikation und zurück Spock ist ein Testframework für Webanwendungen, mit dem man unter anderem den Behavior Driven Development Ansatz, kurz BDD, verfolgen kann. Der Product Owner beschreibt das Verhalten einer Applikation und der Entwickler überprüft es über einen automatischen Test. Dem Entwickler reicht die Ausgabe \"PASSED\" oder \"FAILED\", denn er kennt ja den Code seiner Tests. Wäre es nicht cool, wenn auch der Product Owner ein verständliches Dokument bekäme? Kein Problem! Wir generieren über ein Template einfach einen Test-Report in AsciiDoc und fügen weitere erklärende Texte hinzu um eine les- und ausführbare Spezifikation zu erhalten. Screenshots aller wichtigen Schritte bereichern die Spezifikation weiter. Sollte aber die Spezifikation nicht am Anfang stehen? Und warum Spezifikation, wenn wir agil sein wollen? Richtig! Stellen wir also eine iterative Feature-Beschreibung an den Anfang und verfeinern diese mit automatischen Tests um am Ende eine gut lesbare und verifizierbare Spezifikation des Verhaltens unseres Systems zu erhalten! Die Vorteile liegen auf der Hand – die Vorgehensweise verbessert die Kommunikation zwischen Product Owner und Entwicklern und am Ende bekommen wir ein Dokument welches Ihre wertvolle Software korrekt und überprüfbar beschreibt. Slides und Video "
},

{
    "id": 37,
    "uri": "blog/index.html",
    "menu": "Blog",
    "title": "Übersicht",
    "text": " Table of Contents Übersicht Übersicht Willkommen auf dem Tech Blog der DB Systel. "
},

{
    "id": 38,
    "uri": "blog/profiles/buildIT.html",
    "menu": "Autoren",
    "title": "BuildIT",
    "text": " Table of Contents BuildIT BuildIT span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } "
},

{
    "id": 39,
    "uri": "blog/profiles/Tim-Engeleiter.html",
    "menu": "Autoren",
    "title": "Tim Engeleiter",
    "text": " Table of Contents Tim Engeleiter Tim Engeleiter span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } image: "
},

{
    "id": 40,
    "uri": "blog/profiles/Sascha-Wolter.html",
    "menu": "Autoren",
    "title": "Sascha Wolter",
    "text": " Table of Contents Sascha Wolter Links Sascha Wolter span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } Sascha Wolter ist Experte für die Planung und Umsetzung von geräteübergreifenden Anwendungen. Als solcher begeistert er sich für das Benutzererlebnis und erkundet verbesserte multimodale Interaktionsformen zwischen Mensch und Maschine – u. a. in Form von Konversation über Text (Chatbots) und Sprache (auch als Alexa bekannt). Bereits seit 1995 arbeitet er als Berater, Dozent, Sprecher und Autor. In seiner Freizeit begeistert er sich für Bergsport von Wandern bis Ski und genießt guten italienischen Kaffee. Er ist Chief Advisor für Conversational AI bei DB Systel, TecCo Lead HMI bei Deutsche Bahn und er engagiert er sich als Vorstandsmitglied im Arbeitskreis Usability &amp; User Experience der BITKOM. Für sein Developer- und Community-Engagement wurde er mehrfach als Google Developer Expert für den Google Assistant (GDE) ausgezeichnet. Vorher war er Senior UX Consultant und Principal Technology Evangelist bei der Conversational AI Platform Company Cognigy, arbeitete er als Senior Developer Evangelist bei der Deutschen Telekom (u. a. Smart Home), als Senior Technology Evangelist für Alexa bei Amazon und als Freiberufler. Links LinkedIn Persönliche Website "
},

{
    "id": 41,
    "uri": "blog/profiles/Marcus-Suemnick.html",
    "menu": "Autoren",
    "title": "Marcus Sümnick",
    "text": " Table of Contents Marcus Sümnick Marcus Sümnick span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } "
},

{
    "id": 42,
    "uri": "blog/profiles/Johannes-Dienst.html",
    "menu": "Autoren",
    "title": "Johannes Dienst",
    "text": " Table of Contents Johannes Dienst Johannes Dienst span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } "
},

{
    "id": 43,
    "uri": "blog/profiles/Bernd-Schimmer.html",
    "menu": "Autoren",
    "title": "Bernd Schimmer",
    "text": " Table of Contents Bernd Schimmer Bernd Schimmer span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } Bernd is an experienced frontend developer and Agility Master at DB Systel GmbH which is the digital partner of the biggest German railway company Deutsche Bahn . "
},

{
    "id": 44,
    "uri": "blog/profiles/Stefan-Gruendling.html",
    "menu": "Autoren",
    "title": "Stefan Gründling",
    "text": " Table of Contents Stefan Gründling Stefan Gründling span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } Stefan Gründling ist SAFe® Solution Architect bei der DB Netz AG, dem größten Betreiber von Schieneninfrastruktur in Deutschland. Er arbeitet seit mehr als 10 Jahren in der Beratung und Softwareentwicklung. Als Architekt war er verantwortlich für die Entwicklung der Click&amp;Ride App und die automatische Fahrplankonstruktion mit welchen die DB Netz AG als weltweit erstes Unternehmen über eine App vollständig automatisiert Fahrpläne erstellen konnte. "
},

{
    "id": 45,
    "uri": "blog/profiles/Joachim-Schirrmacher.html",
    "menu": "Autoren",
    "title": "Joachim Schirrmacher",
    "text": " Table of Contents Joachim Schirrmacher Links Joachim Schirrmacher span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } Joachim ist seit 2017 bei der DB Systel als Berater und Entwickler. Am liebsten programmiert er funktional mit TypeScript, notfalls aber auch mit Java und Spring Boot, mag REST APIs, aber auch Event Sourcing und Streams. Links LinkedIn Profile GitHub Profile Mastodon Stack Overflow "
},

{
    "id": 46,
    "uri": "blog/profiles/Philippe-Rieffe.html",
    "menu": "Autoren",
    "title": "Philippe Rieffe",
    "text": " Table of Contents Philippe Rieffe Philippe Rieffe span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } "
},

{
    "id": 47,
    "uri": "blog/profiles/Dr.-Martin-Strunk.html",
    "menu": "Autoren",
    "title": "Dr. Martin Strunk",
    "text": " Table of Contents Dr. Martin Strunk span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } Dr. Martin Strunk Dr. Martin Strunk has been working for more than 22 years in different expert and management roles in development and operations at DB Systel. In 2018 Dr. Martin Strunk initiated and lead the DevOps-Transformation Project “Two Deployments per Day (2D/d)” at DB Systel, where the technical, organizational and cultural foundations for a DevOps IT delivery model have been created. Currently, he leads as an Agility Master the Customer Experience Unit of DB Systel with more than half a dozen engineering teams that work according to the “You build it, you run it”-paradigm. LinkedIn Xing "
},

{
    "id": 48,
    "uri": "blog/profiles/Danny-Koppenhagen.html",
    "menu": "Autoren",
    "title": "Danny Koppenhagen",
    "text": " Table of Contents Danny Koppenhagen Links Danny Koppenhagen span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } Danny is an experienced frontend architect at DB Systel GmbH which is the digital partner of the biggest German railway company Deutsche Bahn . He develops and architects’ enterprise web applications within a DevOps team facing the micro mobility market. Furthermore, he is an open-source enthusiast and one of the authors of the popular German-language Angular book . Links LinkedIn Profile Mastodon Profile X (formerly known as Twitter) Profile GitHub Profile Personal Website "
},

{
    "id": 49,
    "uri": "blog/profiles/Sven-Hesse.html",
    "menu": "Autoren",
    "title": "Sven Hesse",
    "text": " Table of Contents Sven Hesse Links Sven Hesse span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } Sven arbeitet als DevOps-Engineer bei der DB Systel GmbH. Seine Schwerpunkte liegen im Bereich der Entwicklung und Betreuung von APIs zu Zug- und Wagendaten sowie Shared-Mobility. Links Persönliche Website "
},

{
    "id": 50,
    "uri": "blog/profiles/Ralf-D.-Mueller.html",
    "menu": "Autoren",
    "title": "Ralf D. Müller",
    "text": " Table of Contents Ralf D. Müller Links Ralf D. Müller span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } Ralf D. Müller arbeitet seit über 25 Jahren in der Softwarebranche, zunächst als Entwickler und später als Softwarearchitekt. Da er großen Wert auf die klare Kommunikation und Dokumentation seiner Ideen legt, hat er das Open-Source-Projekt docToolchain gestartet, das sich der effektiven Dokumentation von Softwarearchitekturen widmet und das arc42 Template als Grundlage nutzt. Zunehmend beschäftigt ihn aber der Einsatz von KI für das Softwaredesign. In diesem Zusammenhang hat Ralf bereits Erfahrungen mit ChatGPT gesammelt, etwa als Werkzeug zum Lösen spezifischer Teilprobleme in der \"iSAQB Advanced Level\"-Beispielprüfung. Links LinkedIn Profile Mastodon Profile X (formerly known as Twitter) Profile GitHub Profile Personal Website "
},

{
    "id": 51,
    "uri": "blog/profiles/Konrad-Winkler.html",
    "menu": "Autoren",
    "title": "Konrad Winkler",
    "text": " Table of Contents Konrad Winkler Konrad Winkler span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } "
},

{
    "id": 52,
    "uri": "blog/profiles/Jonas-Gassenmeyer.html",
    "menu": "Autoren",
    "title": "Jonas Gassenmeyer",
    "text": " Table of Contents Jonas Gassenmeyer Links Jonas Gassenmeyer span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } Jonas könnte den ganzen Tag über relationale Datenbanken reden. Er ist froh, dass er diese Leidenschaft auch zu einem Beruf machen konnte. Nach einigen Jahren in der Beratung als Datenbank-Entwickler arbeitet er nun für die Bahnstromer bei der DB Systel. Dort sind sowohl Oracle als auch PostgreSQL Datenbanken im Betrieb, die Telemetrie-Daten der elektrisch betriebenen Loks in Europa speichern und verarbeiten. Links Mastodon Profile X (formerly known as Twitter) Profile "
},

{
    "id": 53,
    "uri": "blog/profiles/Maximilian-Franzke.html",
    "menu": "Autoren",
    "title": "Maximilian Franzke",
    "text": " Table of Contents Maximilian Franzke Links Maximilian Franzke span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } Maximilian ist ein erfahrener Softwarearchitekt und Development Lead des DB UX Design System Core bei der DB Systel GmbH, dem Digitalpartner der Deutschen Bahn AG. Er konzipiert und entwickelt Customer und Enterprise Web Anwendungen, und ist spezialisiert im herausfordernden Umfeld von High Performance Websites und Digitaler Barrierefreiheit. Des Weiteren ist er ein Open-Source Enthusiast und an zahlreichen Web-bezogenen Lösungen beteiligt. Links X (formerly known as Twitter) Profile GitHub Profile "
},

{
    "id": 54,
    "uri": "blog/profiles/Christian-Fischer.html",
    "menu": "Autoren",
    "title": "Christian Fischer",
    "text": " Table of Contents Christian Fischer Christian Fischer span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } "
},

{
    "id": 55,
    "uri": "blog/profiles/Carsten-Hoffmann.html",
    "menu": "Autoren",
    "title": "Carsten Hoffmann",
    "text": " Table of Contents Carsten Hoffmann Links Carsten Hoffmann span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } Carsten is an experienced software architect at DB Systel GmbH which is the digital partner of the biggest German railway company Deutsche Bahn . As a member of a DevOps-Team he is a strong believer, that an architect should be close to the development team in order to create and evolve a software architecture, that meets the business needs. He is a conference speaker and held talks at internal and external conferences, like the DB TechCon and the IT-Tage . He is an Open-Source enthusiast and maintainer of the Trivy Vulnerability Explorer . Links LinkedIn Profile Mastodon Profile GitHub Profile "
},

{
    "id": 56,
    "uri": "blog/profiles/Rene-Hoffmann.html",
    "menu": "Autoren",
    "title": "René Hoffmann",
    "text": " Table of Contents René Hoffmann Links René Hoffmann span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } René Hoffmann bringt über 25 Jahre Erfahrung in der Beratung und Software-Entwicklung mit. In verschiedenen Fach- und Führungsrollen, von Software-Entwickler und -Architekt bis hin zur Unternehmens- und Organisationsentwicklung, hat er umfangreiche Erfahrungen an der Schnittstelle zwischen Business und IT in großen Organisationen gesammelt. Ein strategischer Weitblick ist ihm wichtig, um zukunftsorientierte Digitalisierungs- und Veränderungsinitiativen zu starten, die Menschen in Business und IT zusammenbringen. Aktuell widmet er sich mit großer Begeisterung den Möglichkeiten generativer KI und deren transformativem Potenzial für Geschäftsprozesse und IT. Links LinkedIn Profile "
},

{
    "id": 57,
    "uri": "blog/profiles/Gualter-Barbas-Baptista.html",
    "menu": "Autoren",
    "title": "Gualter Barbas Baptista",
    "text": " Table of Contents Gualter Barbas Baptista Links Gualter Barbas Baptista span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } Lead Consultant for Platform Strategy and Enablement Gualter is a platform strategist and enabler with over 25 years experience in Linux and FLOSS. He worked as a Product Owner within multiple platform teams. Driven by a passion for sustainability, Gualter draws from his unique academic journey in environmental engineering, complemented by a PhD in ecological economics. He is dedicated to shedding light on the ecological impact of digitalization and empowering developers to make informed decisions for sustainable practices in their daily work. Links LinkedIn Profile "
},

{
    "id": 58,
    "uri": "blog/profiles/Carsten-Thurau.html",
    "menu": "Autoren",
    "title": "Carsten Thurau",
    "text": " Table of Contents Carsten Thurau Carsten Thurau span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } "
},

{
    "id": 59,
    "uri": "blog/profiles/Bertram-Fey.html",
    "menu": "Autoren",
    "title": "Bertram Fey",
    "text": " Table of Contents Bertram Fey Lieblings-OpenSource Eigene Open Source Bertram Fey span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } Bertram ist Technologie Veteran bei der DB Systel GmbH bei den Platform Enablern und den Software Engineering Advocates. Er macht am liebsten aus komplizierten Architekturen einfache. Oder zumindest übersichtliche. Nebenher kümmert er sich um Inner Source, die DB Architektur-Katas und - falls Zeit bleibt - um API-Design. Lieblings-OpenSource PlantUML Fly By Wire Simulation Dark HTTPd Mediathek View Eigene Open Source Bertrams eigene OpenSource-Projekte sind zum Beispiel Fintenfisch - das Degen Trainingsbuch Mediatheken DLNA Bridge "
},

{
    "id": 60,
    "uri": "blog/profiles/Oliver-Hammer.html",
    "menu": "Autoren",
    "title": "Oliver Hammer",
    "text": " Table of Contents Oliver Hammer Oliver Hammer span.profile img { border: 5px solid #288ABF; border-radius: 10px; max-width: 100px; } Oliver Hammer ist Product Owner einer Einheit mit 32 Teams und 300 Mitarbeitern bei der DB Systel, dem Digitalpartner der DB AG. Er arbeitet seit über 25 Jahren in der Beratung und Software-Entwicklung. In den unterschiedlichsten Rollen vom Entwickler über Architekten, fachlichem Berater und Projektleiter hat er viel Erfahrungen an der Schnittstelle zwischen Business und Technologie in großen Projekten und der Zusammenarbeit in Teams und Organisationen und Vorgehensmodellen gesammelt. "
},

{
    "id": 61,
    "uri": "lunrjsindex.html",
    "menu": "null",
    "title": "null",
    "text": " will be replaced by the index "
},

];
